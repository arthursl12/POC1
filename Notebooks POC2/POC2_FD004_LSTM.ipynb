{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "wbuPiXILHHLC",
        "Pnwfsl-aHHLM",
        "MIxpl_deHHLT",
        "6EAk29FIHHLU",
        "y1r4pfQgHHLW",
        "jN9jMcE-HHLX",
        "ApGlTbAtHHLY",
        "nTPBH5fg_sFd",
        "f3Or3dZbB5Pr",
        "DvHTMj_9_xss",
        "9mjReYMmM08s",
        "Z7Z5u9Bu_Q4x",
        "TAC6RVwbHHLg"
      ],
      "authorship_tag": "ABX9TyNz9Q+MuBG6YhF/N7CHiEGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthursl12/POC1/blob/main/POC2_FD004_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "wbuPiXILHHLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install scikeras;\n",
        "%pip install -U tensorflow-addons;\n",
        "%pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34982714-e652-4e76-d08c-b6d907f7ec6f",
        "id": "xgrLxZBAHHLE"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in h:\\anaconda3\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in h:\\anaconda3\\lib\\site-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in h:\\anaconda3\\lib\\site-packages (from scikeras) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in h:\\anaconda3\\lib\\site-packages (from packaging>=0.21->scikeras) (2.4.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.6.2)\n",
            "Requirement already satisfied: joblib>=0.11 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow-addons in h:\\anaconda3\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in h:\\anaconda3\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: packaging in h:\\anaconda3\\lib\\site-packages (from tensorflow-addons) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in h:\\anaconda3\\lib\\site-packages (from packaging->tensorflow-addons) (2.4.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scikit-optimize in h:\\anaconda3\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in h:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in h:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in h:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.22.4)\n",
            "Requirement already satisfied: pyaml>=16.9 in h:\\anaconda3\\lib\\site-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in h:\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.2)\n",
            "Requirement already satisfied: PyYAML in h:\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import glob\n",
        "import datetime"
      ],
      "metadata": {
        "id": "WxPjchtEHHLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c763425a-d510-48ed-c481-8212a07776a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "H:\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "H:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
            "H:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "from sklearn.base import BaseEstimator,RegressorMixin"
      ],
      "metadata": {
        "id": "NGG29VypHHLH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "\n",
        "from skopt.space.space import Categorical, Integer, Real"
      ],
      "metadata": {
        "id": "b4AwhIPNHgzX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Masking\n",
        "\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError as RMSE\n",
        "\n",
        "from keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import RSquare as R2\n",
        "\n",
        "from scikeras.wrappers import KerasRegressor"
      ],
      "metadata": {
        "id": "Eyei4ajQHHLI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_palette('colorblind')"
      ],
      "metadata": {
        "id": "wHAvW_6rHHLJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED']=str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "HfbMAckfHHLJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "-4FxQFdYHHLK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove some tf warnings\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)"
      ],
      "metadata": {
        "id": "cJXVJecRHjMy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "Pnwfsl-aHHLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    IN_COLAB = True\n",
        "    folder=\"/content/\"\n",
        "\n",
        "    # Dataset Download \n",
        "    os.system('git clone https://github.com/arthursl12/dataset_2')\n",
        "    os.system('mv /content/dataset_2/CMaps /content/CMaps')\n",
        "    os.system('mv /content/dataset_2/data_processing /content/data_processing')\n",
        "    os.system('rm -rf dataset_2')\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    IN_COLAB = False\n",
        "    folder=\"CMaps/\"\n",
        "    %cd dataset_2/"
      ],
      "metadata": {
        "id": "tlp572nXopEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b195c9e7-a31c-46be-9cac-4f8044652982"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not running on CoLab\n",
            "C:\\Users\\Arthur Lima\\POC\\dataset_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_processing.processing import DatasetProcessing\n",
        "from data_processing.training import HyperparameterSearch, reclipper_scorer\n",
        "from data_processing.eval import Evaluation"
      ],
      "metadata": {
        "id": "QdvXViGaHHLO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proc = DatasetProcessing()"
      ],
      "metadata": {
        "id": "bDxH33UkHHLO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Integration"
      ],
      "metadata": {
        "id": "ePNldoX9HHLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:  \n",
        "\n",
        "1) unit number   \n",
        "2) time, in cycles  \n",
        "3) operational setting 1  \n",
        "4) operational setting 2  \n",
        "5) operational setting 3    \n",
        "6) sensor measurement 1    \n",
        "7) sensor measurement 2  \n",
        "...  \n",
        "26) sensor measurement 20\n",
        "\n",
        "\n",
        "There are 6 conditions (or combinations) which the 3 operational settings can take.  \n",
        "Condition 1: Altitude = 0, Mach Number = 0, TRA = 100  \n",
        "Condition 2: Altitude = 10, Mach Number = 0.25, TRA = 100  \n",
        "Condition 3: Altitude = 20, Mach Number = 0.7 TRA = 100  \n",
        "Condition 4: Altitude = 25, Mach Number = 0.62, TRA = 60  \n",
        "Condition 5: Altitude = 35 Mach Number = 0.84, TRA = 100  \n",
        "Condition 6: Altitude = 42, Mach Number = 0.84, TRA = 100  \n",
        "  \n",
        "There is slight variation in all these conditions so you may get numbers like 24.453 instead of 25 exactly.\n",
        "\n",
        "FD001: Condition 1 only  \n",
        "FD002: Mix of all the conditions  \n",
        "FD003: Condition 1 only  \n",
        "FD004: Mix of all conditions  \n"
      ],
      "metadata": {
        "id": "zdGT4L4KHHLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_cols, settings_cols, sensors_cols, cols = proc.column_names()\n",
        "train, test, y_test = proc.read_dataset(4, folder='CMaps/')\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "53be0c69-db38-41ff-eb46-f1abc6405af8",
        "id": "UoXlCtisHHLS"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       unit_number  time     op_1    op_2   op_3     s_0     s_1      s_2  \\\n",
              "0                1     1  42.0049  0.8400  100.0  445.00  549.68  1343.43   \n",
              "1                1     2  20.0020  0.7002  100.0  491.19  606.07  1477.61   \n",
              "2                1     3  42.0038  0.8409  100.0  445.00  548.95  1343.12   \n",
              "3                1     4  42.0000  0.8400  100.0  445.00  548.70  1341.24   \n",
              "4                1     5  25.0063  0.6207   60.0  462.54  536.10  1255.23   \n",
              "...            ...   ...      ...     ...    ...     ...     ...      ...   \n",
              "61244          249   251   9.9998  0.2500  100.0  489.05  605.33  1516.36   \n",
              "61245          249   252   0.0028  0.0015  100.0  518.67  643.42  1598.92   \n",
              "61246          249   253   0.0029  0.0000  100.0  518.67  643.68  1607.72   \n",
              "61247          249   254  35.0046  0.8400  100.0  449.44  555.77  1381.29   \n",
              "61248          249   255  42.0030  0.8400  100.0  445.00  549.85  1369.75   \n",
              "\n",
              "           s_3    s_4  ...    s_11     s_12     s_13     s_14  s_15  s_16  \\\n",
              "0      1112.93   3.91  ...  129.78  2387.99  8074.83   9.3335  0.02   330   \n",
              "1      1237.50   9.35  ...  312.59  2387.73  8046.13   9.1913  0.02   361   \n",
              "2      1117.05   3.91  ...  129.62  2387.97  8066.62   9.4007  0.02   329   \n",
              "3      1118.03   3.91  ...  129.80  2388.02  8076.05   9.3369  0.02   328   \n",
              "4      1033.59   7.05  ...  164.11  2028.08  7865.80  10.8366  0.02   305   \n",
              "...        ...    ...  ...     ...      ...      ...      ...   ...   ...   \n",
              "61244  1315.28  10.52  ...  380.16  2388.73  8185.69   8.4541  0.03   372   \n",
              "61245  1426.77  14.62  ...  535.02  2388.46  8185.47   8.2221  0.03   396   \n",
              "61246  1430.56  14.62  ...  535.41  2388.48  8193.94   8.2525  0.03   395   \n",
              "61247  1148.18   5.48  ...  187.92  2388.83  8125.64   9.0515  0.02   337   \n",
              "61248  1147.45   3.91  ...  134.32  2388.66  8144.33   9.1207  0.02   333   \n",
              "\n",
              "       s_17    s_18   s_19     s_20  \n",
              "0      2212  100.00  10.62   6.3670  \n",
              "1      2324  100.00  24.37  14.6552  \n",
              "2      2212  100.00  10.48   6.4213  \n",
              "3      2212  100.00  10.54   6.4176  \n",
              "4      1915   84.93  14.03   8.6754  \n",
              "...     ...     ...    ...      ...  \n",
              "61244  2319  100.00  29.11  17.5234  \n",
              "61245  2388  100.00  39.38  23.7151  \n",
              "61246  2388  100.00  39.78  23.8270  \n",
              "61247  2223  100.00  15.26   9.0774  \n",
              "61248  2212  100.00  10.66   6.4341  \n",
              "\n",
              "[61249 rows x 26 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unit_number</th>\n",
              "      <th>time</th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>42.0049</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.68</td>\n",
              "      <td>1343.43</td>\n",
              "      <td>1112.93</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.78</td>\n",
              "      <td>2387.99</td>\n",
              "      <td>8074.83</td>\n",
              "      <td>9.3335</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.62</td>\n",
              "      <td>6.3670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>20.0020</td>\n",
              "      <td>0.7002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>491.19</td>\n",
              "      <td>606.07</td>\n",
              "      <td>1477.61</td>\n",
              "      <td>1237.50</td>\n",
              "      <td>9.35</td>\n",
              "      <td>...</td>\n",
              "      <td>312.59</td>\n",
              "      <td>2387.73</td>\n",
              "      <td>8046.13</td>\n",
              "      <td>9.1913</td>\n",
              "      <td>0.02</td>\n",
              "      <td>361</td>\n",
              "      <td>2324</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24.37</td>\n",
              "      <td>14.6552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>42.0038</td>\n",
              "      <td>0.8409</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.95</td>\n",
              "      <td>1343.12</td>\n",
              "      <td>1117.05</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.62</td>\n",
              "      <td>2387.97</td>\n",
              "      <td>8066.62</td>\n",
              "      <td>9.4007</td>\n",
              "      <td>0.02</td>\n",
              "      <td>329</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>6.4213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>42.0000</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.70</td>\n",
              "      <td>1341.24</td>\n",
              "      <td>1118.03</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.80</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8076.05</td>\n",
              "      <td>9.3369</td>\n",
              "      <td>0.02</td>\n",
              "      <td>328</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.54</td>\n",
              "      <td>6.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>25.0063</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.10</td>\n",
              "      <td>1255.23</td>\n",
              "      <td>1033.59</td>\n",
              "      <td>7.05</td>\n",
              "      <td>...</td>\n",
              "      <td>164.11</td>\n",
              "      <td>2028.08</td>\n",
              "      <td>7865.80</td>\n",
              "      <td>10.8366</td>\n",
              "      <td>0.02</td>\n",
              "      <td>305</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.03</td>\n",
              "      <td>8.6754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61244</th>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>9.9998</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>100.0</td>\n",
              "      <td>489.05</td>\n",
              "      <td>605.33</td>\n",
              "      <td>1516.36</td>\n",
              "      <td>1315.28</td>\n",
              "      <td>10.52</td>\n",
              "      <td>...</td>\n",
              "      <td>380.16</td>\n",
              "      <td>2388.73</td>\n",
              "      <td>8185.69</td>\n",
              "      <td>8.4541</td>\n",
              "      <td>0.03</td>\n",
              "      <td>372</td>\n",
              "      <td>2319</td>\n",
              "      <td>100.00</td>\n",
              "      <td>29.11</td>\n",
              "      <td>17.5234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61245</th>\n",
              "      <td>249</td>\n",
              "      <td>252</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.42</td>\n",
              "      <td>1598.92</td>\n",
              "      <td>1426.77</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>535.02</td>\n",
              "      <td>2388.46</td>\n",
              "      <td>8185.47</td>\n",
              "      <td>8.2221</td>\n",
              "      <td>0.03</td>\n",
              "      <td>396</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.00</td>\n",
              "      <td>39.38</td>\n",
              "      <td>23.7151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61246</th>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.68</td>\n",
              "      <td>1607.72</td>\n",
              "      <td>1430.56</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>535.41</td>\n",
              "      <td>2388.48</td>\n",
              "      <td>8193.94</td>\n",
              "      <td>8.2525</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.00</td>\n",
              "      <td>39.78</td>\n",
              "      <td>23.8270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61247</th>\n",
              "      <td>249</td>\n",
              "      <td>254</td>\n",
              "      <td>35.0046</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>449.44</td>\n",
              "      <td>555.77</td>\n",
              "      <td>1381.29</td>\n",
              "      <td>1148.18</td>\n",
              "      <td>5.48</td>\n",
              "      <td>...</td>\n",
              "      <td>187.92</td>\n",
              "      <td>2388.83</td>\n",
              "      <td>8125.64</td>\n",
              "      <td>9.0515</td>\n",
              "      <td>0.02</td>\n",
              "      <td>337</td>\n",
              "      <td>2223</td>\n",
              "      <td>100.00</td>\n",
              "      <td>15.26</td>\n",
              "      <td>9.0774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61248</th>\n",
              "      <td>249</td>\n",
              "      <td>255</td>\n",
              "      <td>42.0030</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.85</td>\n",
              "      <td>1369.75</td>\n",
              "      <td>1147.45</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>134.32</td>\n",
              "      <td>2388.66</td>\n",
              "      <td>8144.33</td>\n",
              "      <td>9.1207</td>\n",
              "      <td>0.02</td>\n",
              "      <td>333</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.66</td>\n",
              "      <td>6.4341</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61249 rows × 26 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "MIxpl_deHHLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set Transformation \n",
        "Test set has samples for all cycles, but has annotations only for last one"
      ],
      "metadata": {
        "id": "6EAk29FIHHLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355f48ad-27b7-4d42-896e-eca9d652a921",
        "id": "fZRG6FZxHHLU"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((41214, 26), (248, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_last = proc.transform_test_keep_setting(test)\n",
        "test_last.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "210eca97-8103-48ed-aa10-3dccf0aadcf1",
        "id": "u8TAq9vFHHLV"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      op_1    op_2   op_3     s_0     s_1      s_2      s_3   s_4   s_5  \\\n",
              "0  25.0070  0.6214   60.0  462.54  537.66  1264.31  1046.41  7.05  8.99   \n",
              "1  41.9989  0.8400  100.0  445.00  549.96  1354.05  1133.55  3.91  5.72   \n",
              "2  42.0005  0.8401  100.0  445.00  549.47  1341.06  1118.90  3.91  5.69   \n",
              "3  25.0018  0.6207   60.0  462.54  536.06  1253.49  1038.53  7.05  9.00   \n",
              "4  25.0039  0.6200   60.0  462.54  537.36  1263.60  1052.52  7.05  9.03   \n",
              "\n",
              "      s_6  ...    s_11     s_12     s_13     s_14  s_15  s_16  s_17    s_18  \\\n",
              "0  176.56  ...  166.19  2028.53  7890.31  10.7615  0.02   308  1915   84.93   \n",
              "1  139.03  ...  130.17  2387.72  8073.44   9.3925  0.02   331  2212  100.00   \n",
              "2  139.26  ...  130.73  2388.18  8095.58   9.2974  0.02   330  2212  100.00   \n",
              "3  175.63  ...  164.91  2028.30  7878.63  10.8396  0.02   306  1915   84.93   \n",
              "4  175.53  ...  164.95  2028.24  7873.75  10.9094  0.02   307  1915   84.93   \n",
              "\n",
              "    s_19    s_20  \n",
              "0  14.41  8.6329  \n",
              "1  10.58  6.4325  \n",
              "2  10.61  6.3488  \n",
              "3  14.41  8.5696  \n",
              "4  14.19  8.6248  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>s_5</th>\n",
              "      <th>s_6</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0070</td>\n",
              "      <td>0.6214</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>537.66</td>\n",
              "      <td>1264.31</td>\n",
              "      <td>1046.41</td>\n",
              "      <td>7.05</td>\n",
              "      <td>8.99</td>\n",
              "      <td>176.56</td>\n",
              "      <td>...</td>\n",
              "      <td>166.19</td>\n",
              "      <td>2028.53</td>\n",
              "      <td>7890.31</td>\n",
              "      <td>10.7615</td>\n",
              "      <td>0.02</td>\n",
              "      <td>308</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.41</td>\n",
              "      <td>8.6329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41.9989</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.96</td>\n",
              "      <td>1354.05</td>\n",
              "      <td>1133.55</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.72</td>\n",
              "      <td>139.03</td>\n",
              "      <td>...</td>\n",
              "      <td>130.17</td>\n",
              "      <td>2387.72</td>\n",
              "      <td>8073.44</td>\n",
              "      <td>9.3925</td>\n",
              "      <td>0.02</td>\n",
              "      <td>331</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.58</td>\n",
              "      <td>6.4325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0005</td>\n",
              "      <td>0.8401</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.47</td>\n",
              "      <td>1341.06</td>\n",
              "      <td>1118.90</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.69</td>\n",
              "      <td>139.26</td>\n",
              "      <td>...</td>\n",
              "      <td>130.73</td>\n",
              "      <td>2388.18</td>\n",
              "      <td>8095.58</td>\n",
              "      <td>9.2974</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.61</td>\n",
              "      <td>6.3488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0018</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.06</td>\n",
              "      <td>1253.49</td>\n",
              "      <td>1038.53</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.00</td>\n",
              "      <td>175.63</td>\n",
              "      <td>...</td>\n",
              "      <td>164.91</td>\n",
              "      <td>2028.30</td>\n",
              "      <td>7878.63</td>\n",
              "      <td>10.8396</td>\n",
              "      <td>0.02</td>\n",
              "      <td>306</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.41</td>\n",
              "      <td>8.5696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0039</td>\n",
              "      <td>0.6200</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>537.36</td>\n",
              "      <td>1263.60</td>\n",
              "      <td>1052.52</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.03</td>\n",
              "      <td>175.53</td>\n",
              "      <td>...</td>\n",
              "      <td>164.95</td>\n",
              "      <td>2028.24</td>\n",
              "      <td>7873.75</td>\n",
              "      <td>10.9094</td>\n",
              "      <td>0.02</td>\n",
              "      <td>307</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.19</td>\n",
              "      <td>8.6248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_last"
      ],
      "metadata": {
        "id": "byXM_HFgHHLV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remaining Useful Life (RUL)"
      ],
      "metadata": {
        "id": "y1r4pfQgHHLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = proc.add_remaining_useful_life_linear(train)\n",
        "train[index_cols+['RUL']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "839ecdb5-3c23-452d-f515-9fb4e7f26c90",
        "id": "9sVQVvdZHHLW"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   unit_number  time  RUL\n",
              "0            1     1  320\n",
              "1            1     2  319\n",
              "2            1     3  318\n",
              "3            1     4  317\n",
              "4            1     5  316"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unit_number</th>\n",
              "      <th>time</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attributes and target separation"
      ],
      "metadata": {
        "id": "jN9jMcE-HHLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = proc.X_y_train_divide_with_settings(train)"
      ],
      "metadata": {
        "id": "Itm_P5tIHHLX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "1a416b25-a72d-4a51-c5d2-9db5e728c883",
        "id": "rbdgnmCTHHLY"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   RUL\n",
              "0  320\n",
              "1  319\n",
              "2  318\n",
              "3  317\n",
              "4  316"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "d3df8e7a-37ea-4d6e-cfca-62ff5b8b195e",
        "id": "WD5RRgu6HHLY"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      op_1    op_2   op_3     s_0     s_1      s_2      s_3   s_4    s_5  \\\n",
              "0  42.0049  0.8400  100.0  445.00  549.68  1343.43  1112.93  3.91   5.70   \n",
              "1  20.0020  0.7002  100.0  491.19  606.07  1477.61  1237.50  9.35  13.61   \n",
              "2  42.0038  0.8409  100.0  445.00  548.95  1343.12  1117.05  3.91   5.69   \n",
              "3  42.0000  0.8400  100.0  445.00  548.70  1341.24  1118.03  3.91   5.70   \n",
              "4  25.0063  0.6207   60.0  462.54  536.10  1255.23  1033.59  7.05   9.00   \n",
              "\n",
              "      s_6  ...    s_11     s_12     s_13     s_14  s_15  s_16  s_17    s_18  \\\n",
              "0  137.36  ...  129.78  2387.99  8074.83   9.3335  0.02   330  2212  100.00   \n",
              "1  332.10  ...  312.59  2387.73  8046.13   9.1913  0.02   361  2324  100.00   \n",
              "2  138.18  ...  129.62  2387.97  8066.62   9.4007  0.02   329  2212  100.00   \n",
              "3  137.98  ...  129.80  2388.02  8076.05   9.3369  0.02   328  2212  100.00   \n",
              "4  174.82  ...  164.11  2028.08  7865.80  10.8366  0.02   305  1915   84.93   \n",
              "\n",
              "    s_19     s_20  \n",
              "0  10.62   6.3670  \n",
              "1  24.37  14.6552  \n",
              "2  10.48   6.4213  \n",
              "3  10.54   6.4176  \n",
              "4  14.03   8.6754  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>s_5</th>\n",
              "      <th>s_6</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42.0049</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.68</td>\n",
              "      <td>1343.43</td>\n",
              "      <td>1112.93</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.70</td>\n",
              "      <td>137.36</td>\n",
              "      <td>...</td>\n",
              "      <td>129.78</td>\n",
              "      <td>2387.99</td>\n",
              "      <td>8074.83</td>\n",
              "      <td>9.3335</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.62</td>\n",
              "      <td>6.3670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0020</td>\n",
              "      <td>0.7002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>491.19</td>\n",
              "      <td>606.07</td>\n",
              "      <td>1477.61</td>\n",
              "      <td>1237.50</td>\n",
              "      <td>9.35</td>\n",
              "      <td>13.61</td>\n",
              "      <td>332.10</td>\n",
              "      <td>...</td>\n",
              "      <td>312.59</td>\n",
              "      <td>2387.73</td>\n",
              "      <td>8046.13</td>\n",
              "      <td>9.1913</td>\n",
              "      <td>0.02</td>\n",
              "      <td>361</td>\n",
              "      <td>2324</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24.37</td>\n",
              "      <td>14.6552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0038</td>\n",
              "      <td>0.8409</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.95</td>\n",
              "      <td>1343.12</td>\n",
              "      <td>1117.05</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.69</td>\n",
              "      <td>138.18</td>\n",
              "      <td>...</td>\n",
              "      <td>129.62</td>\n",
              "      <td>2387.97</td>\n",
              "      <td>8066.62</td>\n",
              "      <td>9.4007</td>\n",
              "      <td>0.02</td>\n",
              "      <td>329</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>6.4213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42.0000</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.70</td>\n",
              "      <td>1341.24</td>\n",
              "      <td>1118.03</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.70</td>\n",
              "      <td>137.98</td>\n",
              "      <td>...</td>\n",
              "      <td>129.80</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8076.05</td>\n",
              "      <td>9.3369</td>\n",
              "      <td>0.02</td>\n",
              "      <td>328</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.54</td>\n",
              "      <td>6.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0063</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.10</td>\n",
              "      <td>1255.23</td>\n",
              "      <td>1033.59</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.00</td>\n",
              "      <td>174.82</td>\n",
              "      <td>...</td>\n",
              "      <td>164.11</td>\n",
              "      <td>2028.08</td>\n",
              "      <td>7865.80</td>\n",
              "      <td>10.8366</td>\n",
              "      <td>0.02</td>\n",
              "      <td>305</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.03</td>\n",
              "      <td>8.6754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation functions"
      ],
      "metadata": {
        "id": "ApGlTbAtHHLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval = Evaluation()"
      ],
      "metadata": {
        "id": "-QVCTdsbHHLZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = HyperparameterSearch()"
      ],
      "metadata": {
        "id": "LvBdvzyGHHLZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Construction"
      ],
      "metadata": {
        "id": "nTPBH5fg_sFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "f3Or3dZbB5Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping Callback\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                      patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "E_f33CIB-13t"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Callback\n",
        "def printLog(epoch, logs):\n",
        "    print(\n",
        "        f\"E {epoch+1}\\t: loss={logs['loss']:.3f}, \"+\n",
        "        f\"rmse={logs['root_mean_squared_error']:.3f}, \"+\n",
        "        f\"r2={logs['r_square']:.3f}; \"+\n",
        "        f\"v_loss={logs['val_loss']:.3f}, \"+\n",
        "        f\"v_rmse={logs['val_root_mean_squared_error']:.3f}, \"+\n",
        "        f\"v_r2={logs['val_r_square']:.3f}; \"\n",
        "    )\n",
        "\n",
        "printerCallback = LambdaCallback(on_epoch_end=printLog)"
      ],
      "metadata": {
        "id": "07NuyHHfWLZ0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "DvHTMj_9_xss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train must include indices\n",
        "train3 = train.copy()\n",
        "X_train_ = train3.drop(columns=[\"RUL\"])"
      ],
      "metadata": {
        "id": "MmsURACM-Wkq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = 0"
      ],
      "metadata": {
        "id": "xKy2t3QS8gss"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper"
      ],
      "metadata": {
        "id": "9mjReYMmM08s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "class LSTMWrapperRegressor(BaseEstimator,RegressorMixin):\n",
        "    def __init__(self, basemodel=None, clip_y=-1, seq_length=40,\n",
        "                 include_settings=False, poly_degree=1,\n",
        "                 scaler=StandardScaler()):\n",
        "        # Base parameters\n",
        "        self.basemodel = basemodel\n",
        "        self.clip_y = clip_y\n",
        "        self.seq_length = seq_length\n",
        "        self.poly_degree = poly_degree\n",
        "        self.include_settings = include_settings\n",
        "\n",
        "        # Column indexers\n",
        "        self.feature_cols = sensors_cols\n",
        "        if(include_settings):\n",
        "            # self.seq_cols = settings_cols + self.cols\n",
        "            self.feature_cols = settings_cols + self.feature_cols\n",
        "        self.seq_cols = [\"time\"] + self.feature_cols\n",
        "        self.base_feature_cols = self.feature_cols\n",
        "\n",
        "        # Scaler and PolyFeatures transformers\n",
        "        self.scaler = scaler\n",
        "        self.polyft = PolynomialFeatures(degree=self.poly_degree, \n",
        "                                         include_bias=False)\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        # Merge features and target again\n",
        "        data = X.copy()\n",
        "        data[\"RUL\"] = y\n",
        "\n",
        "        # Apply polynomial features and add them to the dataframe\n",
        "        transf = self.polyft.fit_transform(data[self.base_feature_cols])\n",
        "        transf = pd.DataFrame(transf, columns=\n",
        "                              self.polyft.get_feature_names_out(),\n",
        "                              index=data.index)\n",
        "        self.feature_cols = list(self.polyft.get_feature_names_out())\n",
        "        self.seq_cols = [\"time\"] + self.feature_cols\n",
        "        data[self.feature_cols] = transf\n",
        "\n",
        "        # Scale the data\n",
        "        data[self.feature_cols] = \\\n",
        "                            self.scaler.fit_transform(data[self.feature_cols])\n",
        "\n",
        "        # Transform into time series\n",
        "        X_train = self.gen_X_wrapper(data,self.seq_length,self.seq_cols)\n",
        "        # print(data.shape,X_train.shape, X.shape)\n",
        "\n",
        "        # Clip and transform labels\n",
        "        data2 = data.copy()\n",
        "        if (self.clip_y > 0):\n",
        "            data2[\"RUL\"].clip(upper=self.clip_y, inplace=True)\n",
        "        y_train = self.gen_y_wrapper(data2,self.seq_length,[\"RUL\"])\n",
        "\n",
        "        # Update input shape for future use\n",
        "        global INPUT_SHAPE\n",
        "        # print(INPUT_SHAPE, X_train.shape)\n",
        "        INPUT_SHAPE = (X_train.shape[1],X_train.shape[2])\n",
        "\n",
        "        # Fit model\n",
        "        # print(X_train.shape, y_train.shape)\n",
        "        self.basemodel.fit(X_train,y_train)\n",
        "        return self\n",
        "        \n",
        "    def predict(self, X=None):\n",
        "        # Perform transformation, if not done\n",
        "        if (len(X.shape) < 3):\n",
        "            data = X.copy()\n",
        "\n",
        "            # Apply polynomial features\n",
        "            transf = self.polyft.transform(data[self.base_feature_cols])\n",
        "            transf = pd.DataFrame(transf, \n",
        "                                  columns=self.polyft.get_feature_names_out(),\n",
        "                                  index=data.index)\n",
        "            # data = pd.concat([data,transf], axis=1)\n",
        "            data[self.feature_cols] = transf\n",
        "\n",
        "            # Scale the data\n",
        "            data[self.feature_cols] = \\\n",
        "                            self.scaler.transform(data[self.feature_cols])\n",
        "            \n",
        "            # Transform into time series\n",
        "            X_train = self.gen_X_wrapper(data,self.seq_length,self.seq_cols)\n",
        "        else:\n",
        "            X_train = X\n",
        "        return self.basemodel.predict(X_train)\n",
        "\n",
        "    def score(self, X, y, sample_weight=None):\n",
        "        # Merge features and target again\n",
        "        data = X.copy()\n",
        "        data[\"RUL\"] = y\n",
        "\n",
        "        # Apply polynomial features\n",
        "        transf = self.polyft.transform(data[self.base_feature_cols])\n",
        "        transf = pd.DataFrame(transf, \n",
        "                              columns=self.polyft.get_feature_names_out(),\n",
        "                              index=data.index)\n",
        "        # data = pd.concat([data,transf], axis=1)\n",
        "        data[self.feature_cols] = transf\n",
        "\n",
        "        # Scale the data (with train data parameters)\n",
        "        data[self.feature_cols] = \\\n",
        "                        self.scaler.transform(data[self.feature_cols])\n",
        "        \n",
        "        # Transform into time series\n",
        "        X_test = self.gen_X_wrapper(data,self.seq_length,self.seq_cols)\n",
        "        # print(data.shape,X_test.shape, X.shape)\n",
        "\n",
        "        # Clip and transform labels\n",
        "        data2 = data.copy()\n",
        "        if (self.clip_y > 0):\n",
        "            data2[\"RUL\"].clip(upper=self.clip_y, inplace=True)\n",
        "        y_test = self.gen_y_wrapper(data2,self.seq_length,[\"RUL\"])\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = self.predict(X_test)\n",
        "        return r2_score(y_test, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "    def gen_X_data(self, df, sequence_length, columns, mask_value=-99.):\n",
        "        if df.shape[0] < sequence_length:\n",
        "            # print(\"\\t Not enough sequence:\",df.shape[0],\" < \",sequence_length)\n",
        "            data = np.full(shape=(sequence_length, len(columns)), fill_value=mask_value) # pad\n",
        "            idx = data.shape[0] - df.shape[0]\n",
        "            data[idx:,:] = df[columns].values  # fill with available data\n",
        "        else:\n",
        "            data = df[columns].values\n",
        "            \n",
        "        # # specifically yield the last possible sequence\n",
        "        # stop = num_elements = data_matrix.shape[0]\n",
        "        # start = stop - sequence_length\n",
        "        # for i in list(range(1)):\n",
        "        #     yield data_matrix[start:stop, :]\n",
        "\n",
        "\n",
        "\n",
        "        # data = df[columns].values\n",
        "        num_elements = data.shape[0]\n",
        "\n",
        "        # -1 and +1 because of Python indexing\n",
        "        for start, stop in zip(range(0, num_elements-(sequence_length-1)), \n",
        "                               range(sequence_length, num_elements+1)):\n",
        "            yield data[start:stop, :]\n",
        "\n",
        "    def gen_X_wrapper(self, df, sequence_length, columns, \n",
        "                      unit_nrs=np.array([]), idx_col=\"unit_number\"):\n",
        "        # print(\">> Wrapper called. df shape:\",df.shape)\n",
        "        if unit_nrs.size <= 0:\n",
        "            unit_nrs = df[idx_col].unique()\n",
        "            \n",
        "        data_gen = (list(self.gen_X_data(df[df[idx_col]==unit_nr], \n",
        "                                         sequence_length, columns))\n",
        "                for unit_nr in unit_nrs)\n",
        "        # print(\"\\tdatagen len:\",len(data_gen))\n",
        "        data_array = np.concatenate(list(data_gen)).astype(np.float32)\n",
        "        # print(\"\\tdata_array.shape:\",data_array.shape)\n",
        "        return data_array\n",
        "\n",
        "    def gen_y(self, df, sequence_length, label):\n",
        "        data_matrix = df[label].values\n",
        "        num_elements = data_matrix.shape[0]\n",
        "\n",
        "        # -1 because I want to predict the rul of that last row in the sequence, \n",
        "        # not the next row\n",
        "        return data_matrix[sequence_length-1:num_elements, :]  \n",
        "\n",
        "    def gen_y_wrapper(self, df, sequence_length, label, \n",
        "                      unit_nrs=np.array([]), idx_col=\"unit_number\"):\n",
        "        # print(\">> Y Wrapper called. df shape:\",df.shape)\n",
        "        if unit_nrs.size <= 0:\n",
        "            unit_nrs = df[idx_col].unique()\n",
        "            \n",
        "        label_gen = [self.gen_y(df[df[idx_col]==unit_nr], \n",
        "                                sequence_length, label) \n",
        "                    for unit_nr in unit_nrs]\n",
        "        # print(\"\\tlabelgen len:\",len(label_gen))\n",
        "        label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "        # print(\"\\tlabel_array.shape:\",label_array.shape)\n",
        "        return label_array"
      ],
      "metadata": {
        "id": "2aaP3NDdHHLe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "Z7Z5u9Bu_Q4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_test_data(df, sequence_length, columns, mask_value):\n",
        "    if df.shape[0] < sequence_length:\n",
        "        data_matrix = np.full(shape=(sequence_length, len(columns)), fill_value=mask_value) # pad\n",
        "        idx = data_matrix.shape[0] - df.shape[0]\n",
        "        data_matrix[idx:,:] = df[columns].values  # fill with available data\n",
        "    else:\n",
        "        data_matrix = df[columns].values\n",
        "        \n",
        "    # specifically yield the last possible sequence\n",
        "    stop = num_elements = data_matrix.shape[0]\n",
        "    start = stop - sequence_length\n",
        "    for i in list(range(1)):\n",
        "        yield data_matrix[start:stop, :]"
      ],
      "metadata": {
        "id": "npYlhD17_STQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_test_wrapper(X_test_scaled, sequence_length, cols, idx_col=\"unit_number\"): \n",
        "    data_gen = (\n",
        "        list(gen_test_data(X_test_scaled[X_test_scaled[idx_col]==unit_nr], \n",
        "                           sequence_length, cols, -99.))\n",
        "            for unit_nr in X_test_scaled[idx_col].unique())\n",
        "    data_array = np.concatenate(list(data_gen)).astype(np.float32)\n",
        "    return data_array"
      ],
      "metadata": {
        "id": "dHLgOufSAcAy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_test(test,model):\n",
        "    test2 = test.copy()\n",
        "\n",
        "    # Apply polynomial features\n",
        "    transf = model.polyft.transform(test2[model.base_feature_cols])\n",
        "    transf = pd.DataFrame(transf, \n",
        "                          columns=model.polyft.get_feature_names_out(),\n",
        "                          index=test2.index)\n",
        "    newcols = model.polyft.get_feature_names_out()\n",
        "    test2[newcols] = transf \n",
        "\n",
        "    # Scale the data (with train data parameters)\n",
        "    test2[model.feature_cols] = \\\n",
        "                    model.scaler.transform(test2[model.feature_cols])\n",
        "    return test2"
      ],
      "metadata": {
        "id": "OFW6XKjG5kCB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructor"
      ],
      "metadata": {
        "id": "TAC6RVwbHHLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(optim=Adam, learning_rate=1e-3, \n",
        "                 layer1=32  , activation1=\"tanh\"    , dropout1=0.1,\n",
        "                 layer2=None, activation2=\"tanh\"    , dropout2=0.1,\n",
        "                 layer3=None, activation3=\"tanh\"    , dropout3=0.1,\n",
        "                 second_dense=True,\n",
        "                 print_summary=False, loss='mean_squared_error',\n",
        "                 metrics=[tf.keras.metrics.MeanSquaredError()]):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input-masked layer\n",
        "    model.add(Masking(mask_value=-99., input_shape=INPUT_SHAPE))\n",
        "    \n",
        "    if (layer2 is None and layer3 is None):\n",
        "        # Single LSTM layer\n",
        "        model.add(LSTM(layer1, activation=activation1))\n",
        "        model.add(Dropout(dropout1))\n",
        "    elif (layer2 is not None and layer3 is None):\n",
        "        # LSTM-Dense\n",
        "        model.add(LSTM(layer1, activation=activation1))\n",
        "        model.add(Dropout(dropout1))\n",
        "        model.add(Dense(layer2, activation=activation2))\n",
        "        model.add(Dropout(dropout2))\n",
        "    elif (layer2 is not None and layer3 is not None and second_dense==False):\n",
        "        # LSTM-LSTM-Dense\n",
        "        model.add(LSTM(layer1, activation=activation1, return_sequences=True))\n",
        "        model.add(Dropout(dropout1))\n",
        "        model.add(LSTM(layer2, activation=activation2))\n",
        "        model.add(Dropout(dropout2))\n",
        "        model.add(Dense(layer3, activation=activation3))\n",
        "        model.add(Dropout(dropout3))\n",
        "    elif (layer2 is not None and layer3 is not None and second_dense==True):\n",
        "        # LSTM-Dense-Dense\n",
        "        model.add(LSTM(layer1, activation=activation1))\n",
        "        model.add(Dropout(dropout1))\n",
        "        model.add(Dense(layer2, activation=activation2))\n",
        "        model.add(Dropout(dropout2))\n",
        "        model.add(Dense(layer3, activation=activation3))\n",
        "        model.add(Dropout(dropout3))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optim(learning_rate=learning_rate), \n",
        "                  metrics=metrics)\n",
        "    \n",
        "    if(print_summary): model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "YR28IpUT5cm5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM 1-layer"
      ],
      "metadata": {
        "id": "ppByl3wN_W05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "5aqms6jMFKti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Score: 0.565997282764165  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 354),\n",
        "('basemodel__epochs', 43),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__dropout1', 0.3412271023238272),\n",
        "('basemodel__model__layer1', 117),\n",
        "('basemodel__model__learning_rate', 0.008518900361139942),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.17000957225461616),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 30)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "oGlT7ajZFKtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=30\n",
        "CLIP=-1\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=354,\n",
        "                           epochs=43,\n",
        "                           model__activation1='tanh',\n",
        "                           model__dropout1=0.3412271023238272, \n",
        "                           model__layer1=117, \n",
        "                           model__learning_rate=0.008518900361139942,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.17000957225461616, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "5p5GYkfQFKtl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5374b11-b8a5-4f45-ca7d-980255c69eac",
        "id": "wmkoVA33FKtl"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, 30, 25)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 117)               66924     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 117)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 118       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67,042\n",
            "Trainable params: 67,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=12649.581, rmse=112.470, r2=-0.767; v_loss=6483.583, v_rmse=80.521, v_r2=-0.077; \n",
            "E 2\t: loss=7031.255, rmse=83.853, r2=0.018; v_loss=4797.660, v_rmse=69.265, v_r2=0.203; \n",
            "E 3\t: loss=5856.165, rmse=76.526, r2=0.182; v_loss=4380.025, v_rmse=66.182, v_r2=0.273; \n",
            "E 4\t: loss=5477.742, rmse=74.012, r2=0.235; v_loss=4188.372, v_rmse=64.718, v_r2=0.305; \n",
            "E 5\t: loss=5265.476, rmse=72.564, r2=0.264; v_loss=3995.508, v_rmse=63.210, v_r2=0.337; \n",
            "E 6\t: loss=5140.121, rmse=71.695, r2=0.282; v_loss=3940.504, v_rmse=62.773, v_r2=0.346; \n",
            "E 7\t: loss=5001.582, rmse=70.722, r2=0.301; v_loss=3730.141, v_rmse=61.075, v_r2=0.381; \n",
            "E 8\t: loss=4976.833, rmse=70.547, r2=0.305; v_loss=3701.672, v_rmse=60.841, v_r2=0.385; \n",
            "E 9\t: loss=4929.822, rmse=70.213, r2=0.311; v_loss=3669.909, v_rmse=60.580, v_r2=0.391; \n",
            "E 10\t: loss=4938.689, rmse=70.276, r2=0.310; v_loss=3605.851, v_rmse=60.049, v_r2=0.401; \n",
            "E 11\t: loss=4916.282, rmse=70.116, r2=0.313; v_loss=3602.179, v_rmse=60.018, v_r2=0.402; \n",
            "E 12\t: loss=4896.155, rmse=69.973, r2=0.316; v_loss=3534.625, v_rmse=59.453, v_r2=0.413; \n",
            "E 13\t: loss=4645.895, rmse=68.161, r2=0.351; v_loss=3128.200, v_rmse=55.930, v_r2=0.481; \n",
            "E 14\t: loss=4178.713, rmse=64.643, r2=0.416; v_loss=2783.116, v_rmse=52.755, v_r2=0.538; \n",
            "E 15\t: loss=3742.348, rmse=61.175, r2=0.477; v_loss=3102.008, v_rmse=55.696, v_r2=0.485; \n",
            "E 16\t: loss=3580.017, rmse=59.833, r2=0.500; v_loss=2864.020, v_rmse=53.517, v_r2=0.524; \n",
            "E 17\t: loss=3338.988, rmse=57.784, r2=0.533; v_loss=2902.779, v_rmse=53.877, v_r2=0.518; \n",
            "E 18\t: loss=3509.748, rmse=59.243, r2=0.510; v_loss=2406.528, v_rmse=49.056, v_r2=0.600; \n",
            "E 19\t: loss=3192.504, rmse=56.502, r2=0.554; v_loss=2370.341, v_rmse=48.686, v_r2=0.606; \n",
            "E 20\t: loss=3227.705, rmse=56.813, r2=0.549; v_loss=2361.244, v_rmse=48.593, v_r2=0.608; \n",
            "E 21\t: loss=3189.906, rmse=56.479, r2=0.554; v_loss=2356.161, v_rmse=48.540, v_r2=0.609; \n",
            "E 22\t: loss=3113.301, rmse=55.797, r2=0.565; v_loss=2575.606, v_rmse=50.750, v_r2=0.572; \n",
            "E 23\t: loss=3190.469, rmse=56.484, r2=0.554; v_loss=2467.235, v_rmse=49.671, v_r2=0.590; \n",
            "E 24\t: loss=3086.080, rmse=55.552, r2=0.569; v_loss=2290.077, v_rmse=47.855, v_r2=0.620; \n",
            "E 25\t: loss=3046.801, rmse=55.198, r2=0.574; v_loss=2184.379, v_rmse=46.737, v_r2=0.637; \n",
            "E 26\t: loss=3063.266, rmse=55.347, r2=0.572; v_loss=2372.258, v_rmse=48.706, v_r2=0.606; \n",
            "E 27\t: loss=3172.850, rmse=56.328, r2=0.557; v_loss=2267.453, v_rmse=47.618, v_r2=0.624; \n",
            "E 28\t: loss=3104.879, rmse=55.721, r2=0.566; v_loss=2409.155, v_rmse=49.083, v_r2=0.600; \n",
            "E 29\t: loss=3087.723, rmse=55.567, r2=0.569; v_loss=2241.336, v_rmse=47.343, v_r2=0.628; \n",
            "E 30\t: loss=3084.652, rmse=55.540, r2=0.569; v_loss=2170.806, v_rmse=46.592, v_r2=0.640; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=354, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=43, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__dropout1=0.3412271023238272, model__layer1=117, model__learning_rate=0.008518900361139942, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x0000020008548F70>, <tensorflow_addons.metrics.r_square.RSquare object at 0x000002000916C850>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.17000957225461616, verbose=0),\n",
              "                     include_settings=True, seq_length=30)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227402a7-4e1d-4228-afdf-bd11d83469a1",
        "id": "RXnrciV-FKtm"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.365,RMSE=-43.447\n",
            "Finished: 2022-11-03 09:56:07.897610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL "
      ],
      "metadata": {
        "id": "WVSMJWsLFDx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.7343157247269209  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 32),\n",
        "('basemodel__epochs', 41),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__dropout1', 0.23186170929942812),\n",
        "('basemodel__model__layer1', 333),\n",
        "('basemodel__model__learning_rate', 0.0016624190580454845),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.19268056108985515),\n",
        "('clip_y', 134),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 30)\n",
        "```\n"
      ],
      "metadata": {
        "id": "3ZtOjUBNDKtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=30\n",
        "CLIP=134\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=32,\n",
        "                           epochs=41,\n",
        "                           model__activation1='tanh',\n",
        "                           model__dropout1=0.23186170929942812, \n",
        "                           model__layer1=333, \n",
        "                           model__learning_rate=0.0016624190580454845,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.19268056108985515, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "rqEuUKLF0CfS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_36P-gmRD6QM",
        "outputId": "0f326e20-69f1-4858-fc4c-c28e4cdef610"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_1 (Masking)         (None, 30, 25)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 333)               478188    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 333)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 334       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 478,522\n",
            "Trainable params: 478,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=1823.237, rmse=42.699, r2=0.089; v_loss=1188.286, v_rmse=34.472, v_r2=0.409; \n",
            "E 2\t: loss=1323.555, rmse=36.381, r2=0.339; v_loss=1166.059, v_rmse=34.148, v_r2=0.420; \n",
            "E 3\t: loss=1287.929, rmse=35.888, r2=0.357; v_loss=1115.060, v_rmse=33.393, v_r2=0.445; \n",
            "E 4\t: loss=1067.569, rmse=32.674, r2=0.467; v_loss=708.578, v_rmse=26.619, v_r2=0.648; \n",
            "E 5\t: loss=730.044, rmse=27.019, r2=0.635; v_loss=671.947, v_rmse=25.922, v_r2=0.666; \n",
            "E 6\t: loss=657.639, rmse=25.644, r2=0.672; v_loss=620.226, v_rmse=24.904, v_r2=0.692; \n",
            "E 7\t: loss=627.270, rmse=25.045, r2=0.687; v_loss=590.338, v_rmse=24.297, v_r2=0.706; \n",
            "E 8\t: loss=608.539, rmse=24.669, r2=0.696; v_loss=626.361, v_rmse=25.027, v_r2=0.688; \n",
            "E 9\t: loss=593.622, rmse=24.364, r2=0.704; v_loss=586.257, v_rmse=24.213, v_r2=0.708; \n",
            "E 10\t: loss=589.877, rmse=24.287, r2=0.705; v_loss=709.334, v_rmse=26.633, v_r2=0.647; \n",
            "E 11\t: loss=575.573, rmse=23.991, r2=0.713; v_loss=544.695, v_rmse=23.339, v_r2=0.729; \n",
            "E 12\t: loss=553.468, rmse=23.526, r2=0.724; v_loss=555.653, v_rmse=23.572, v_r2=0.724; \n",
            "E 13\t: loss=552.536, rmse=23.506, r2=0.724; v_loss=558.732, v_rmse=23.638, v_r2=0.722; \n",
            "E 14\t: loss=519.350, rmse=22.789, r2=0.741; v_loss=519.326, v_rmse=22.789, v_r2=0.742; \n",
            "E 15\t: loss=536.932, rmse=23.172, r2=0.732; v_loss=510.463, v_rmse=22.593, v_r2=0.746; \n",
            "E 16\t: loss=523.585, rmse=22.882, r2=0.738; v_loss=606.293, v_rmse=24.623, v_r2=0.698; \n",
            "E 17\t: loss=523.981, rmse=22.891, r2=0.738; v_loss=638.725, v_rmse=25.273, v_r2=0.682; \n",
            "E 18\t: loss=516.865, rmse=22.735, r2=0.742; v_loss=680.496, v_rmse=26.086, v_r2=0.662; \n",
            "E 19\t: loss=509.766, rmse=22.578, r2=0.745; v_loss=570.462, v_rmse=23.884, v_r2=0.716; \n",
            "E 20\t: loss=498.755, rmse=22.333, r2=0.751; v_loss=506.693, v_rmse=22.510, v_r2=0.748; \n",
            "E 21\t: loss=483.133, rmse=21.980, r2=0.759; v_loss=549.685, v_rmse=23.445, v_r2=0.727; \n",
            "E 22\t: loss=489.468, rmse=22.124, r2=0.756; v_loss=497.248, v_rmse=22.299, v_r2=0.753; \n",
            "E 23\t: loss=489.432, rmse=22.123, r2=0.756; v_loss=548.685, v_rmse=23.424, v_r2=0.727; \n",
            "E 24\t: loss=480.791, rmse=21.927, r2=0.760; v_loss=477.080, v_rmse=21.842, v_r2=0.763; \n",
            "E 25\t: loss=477.478, rmse=21.851, r2=0.762; v_loss=468.345, v_rmse=21.641, v_r2=0.767; \n",
            "E 26\t: loss=448.922, rmse=21.188, r2=0.776; v_loss=466.252, v_rmse=21.593, v_r2=0.768; \n",
            "E 27\t: loss=453.740, rmse=21.301, r2=0.773; v_loss=549.835, v_rmse=23.449, v_r2=0.727; \n",
            "E 28\t: loss=435.719, rmse=20.874, r2=0.782; v_loss=445.374, v_rmse=21.104, v_r2=0.778; \n",
            "E 29\t: loss=451.470, rmse=21.248, r2=0.775; v_loss=599.434, v_rmse=24.483, v_r2=0.702; \n",
            "E 30\t: loss=451.678, rmse=21.253, r2=0.774; v_loss=519.847, v_rmse=22.800, v_r2=0.741; \n",
            "E 31\t: loss=418.907, rmse=20.467, r2=0.791; v_loss=507.602, v_rmse=22.530, v_r2=0.748; \n",
            "E 32\t: loss=423.603, rmse=20.582, r2=0.788; v_loss=452.801, v_rmse=21.279, v_r2=0.775; \n",
            "E 33\t: loss=440.402, rmse=20.986, r2=0.780; v_loss=522.937, v_rmse=22.868, v_r2=0.740; \n",
            "E 34\t: loss=389.728, rmse=19.742, r2=0.805; v_loss=496.001, v_rmse=22.271, v_r2=0.753; \n",
            "E 35\t: loss=407.950, rmse=20.198, r2=0.796; v_loss=582.450, v_rmse=24.134, v_r2=0.710; \n",
            "E 36\t: loss=390.466, rmse=19.760, r2=0.805; v_loss=464.328, v_rmse=21.548, v_r2=0.769; \n",
            "E 37\t: loss=401.011, rmse=20.025, r2=0.800; v_loss=1202.715, v_rmse=34.680, v_r2=0.402; \n",
            "E 38\t: loss=456.974, rmse=21.377, r2=0.772; v_loss=508.130, v_rmse=22.542, v_r2=0.747; \n",
            "E 39\t: loss=389.486, rmse=19.735, r2=0.805; v_loss=445.491, v_rmse=21.107, v_r2=0.778; \n",
            "E 40\t: loss=380.151, rmse=19.497, r2=0.810; v_loss=462.215, v_rmse=21.499, v_r2=0.770; \n",
            "E 41\t: loss=362.958, rmse=19.051, r2=0.819; v_loss=489.516, v_rmse=22.125, v_r2=0.757; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=32, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=41, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__dropout1=0.23186170929942812, model__layer1=333, model__learning_rate=0...624190580454845, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x0000020025125E50>, <tensorflow_addons.metrics.r_square.RSquare object at 0x000002007EAFDE50>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.19268056108985515, verbose=0),\n",
              "                     clip_y=134, include_settings=True, seq_length=30)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NE5L88ftEB7C",
        "outputId": "cd734691-c7c9-41fe-d13a-37bb5fe44806"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.725,RMSE=-23.884\n",
            "Finished: 2022-11-03 10:10:46.590054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-Dense-1"
      ],
      "metadata": {
        "id": "SvWawqOFYvO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "bWDbtK4RYvPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Score: 0.5081060519759618  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 304),\n",
        "('basemodel__epochs', 31),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'relu'),\n",
        "('basemodel__model__dropout1', 0.3074381764174994),\n",
        "('basemodel__model__dropout2', 0.5435400589103845),\n",
        "('basemodel__model__layer1', 16),\n",
        "('basemodel__model__layer2', 254),\n",
        "('basemodel__model__learning_rate', 0.006871390161766782),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__model__second_dense', True),\n",
        "('basemodel__validation_split', 0.2925386448085039),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 34)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "18mXMMo2YvPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=34\n",
        "CLIP=-1\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=304,\n",
        "                           epochs=31,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='relu',\n",
        "                           model__dropout1=0.3074381764174994, \n",
        "                           model__dropout2=0.5435400589103845, \n",
        "                           model__layer1=16, \n",
        "                           model__layer2=254, \n",
        "                           model__learning_rate=0.006871390161766782,\n",
        "                           model__optim=Adam,\n",
        "                           model__second_dense=True,\n",
        "                           validation_split=0.2925386448085039, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "2EKUQXh0YvPB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917c2cf3-788a-4b3d-a60e-0223e037a0d7",
        "id": "5uHgjmJZYvPC"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_2 (Masking)         (None, 34, 25)            0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16)                2688      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 254)               4318      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 254)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,261\n",
            "Trainable params: 7,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=8428.751, rmse=91.808, r2=-0.200; v_loss=4432.042, v_rmse=66.574, v_r2=0.302; \n",
            "E 2\t: loss=5503.104, rmse=74.183, r2=0.216; v_loss=4374.253, v_rmse=66.138, v_r2=0.311; \n",
            "E 3\t: loss=5446.660, rmse=73.801, r2=0.224; v_loss=4333.349, v_rmse=65.828, v_r2=0.317; \n",
            "E 4\t: loss=5291.608, rmse=72.743, r2=0.246; v_loss=4302.628, v_rmse=65.594, v_r2=0.322; \n",
            "E 5\t: loss=5176.584, rmse=71.948, r2=0.263; v_loss=4162.690, v_rmse=64.519, v_r2=0.344; \n",
            "E 6\t: loss=4971.737, rmse=70.511, r2=0.292; v_loss=3912.319, v_rmse=62.549, v_r2=0.384; \n",
            "E 7\t: loss=4697.060, rmse=68.535, r2=0.331; v_loss=3736.643, v_rmse=61.128, v_r2=0.411; \n",
            "E 8\t: loss=4467.897, rmse=66.842, r2=0.364; v_loss=3825.473, v_rmse=61.850, v_r2=0.397; \n",
            "E 9\t: loss=4276.486, rmse=65.395, r2=0.391; v_loss=3334.755, v_rmse=57.747, v_r2=0.475; \n",
            "E 10\t: loss=4092.995, rmse=63.977, r2=0.417; v_loss=3208.299, v_rmse=56.642, v_r2=0.495; \n",
            "E 11\t: loss=4006.002, rmse=63.293, r2=0.429; v_loss=3333.049, v_rmse=57.733, v_r2=0.475; \n",
            "E 12\t: loss=3934.744, rmse=62.728, r2=0.440; v_loss=3143.906, v_rmse=56.071, v_r2=0.505; \n",
            "E 13\t: loss=3867.228, rmse=62.187, r2=0.449; v_loss=3315.832, v_rmse=57.583, v_r2=0.478; \n",
            "E 14\t: loss=3814.989, rmse=61.766, r2=0.457; v_loss=3308.272, v_rmse=57.518, v_r2=0.479; \n",
            "E 15\t: loss=3830.899, rmse=61.894, r2=0.454; v_loss=3041.356, v_rmse=55.148, v_r2=0.521; \n",
            "E 16\t: loss=3758.024, rmse=61.303, r2=0.465; v_loss=3280.883, v_rmse=57.279, v_r2=0.483; \n",
            "E 17\t: loss=3764.831, rmse=61.358, r2=0.464; v_loss=3256.739, v_rmse=57.068, v_r2=0.487; \n",
            "E 18\t: loss=3773.707, rmse=61.431, r2=0.463; v_loss=2971.748, v_rmse=54.514, v_r2=0.532; \n",
            "E 19\t: loss=3757.503, rmse=61.298, r2=0.465; v_loss=3419.857, v_rmse=58.480, v_r2=0.461; \n",
            "E 20\t: loss=3682.566, rmse=60.684, r2=0.476; v_loss=3623.477, v_rmse=60.195, v_r2=0.429; \n",
            "E 21\t: loss=3590.050, rmse=59.917, r2=0.489; v_loss=3146.794, v_rmse=56.096, v_r2=0.504; \n",
            "E 22\t: loss=3613.330, rmse=60.111, r2=0.485; v_loss=3094.043, v_rmse=55.624, v_r2=0.513; \n",
            "E 23\t: loss=3632.104, rmse=60.267, r2=0.483; v_loss=3256.167, v_rmse=57.063, v_r2=0.487; \n",
            "E 24\t: loss=3609.818, rmse=60.082, r2=0.486; v_loss=3250.235, v_rmse=57.011, v_r2=0.488; \n",
            "E 25\t: loss=3547.534, rmse=59.561, r2=0.495; v_loss=3221.238, v_rmse=56.756, v_r2=0.493; \n",
            "E 26\t: loss=3602.802, rmse=60.023, r2=0.487; v_loss=3099.443, v_rmse=55.673, v_r2=0.512; \n",
            "E 27\t: loss=3585.182, rmse=59.876, r2=0.489; v_loss=3189.959, v_rmse=56.480, v_r2=0.498; \n",
            "E 28\t: loss=3515.583, rmse=59.292, r2=0.499; v_loss=3728.933, v_rmse=61.065, v_r2=0.413; \n",
            "E 29\t: loss=3544.920, rmse=59.539, r2=0.495; v_loss=3458.788, v_rmse=58.811, v_r2=0.455; \n",
            "E 30\t: loss=3586.000, rmse=59.883, r2=0.489; v_loss=3191.006, v_rmse=56.489, v_r2=0.497; \n",
            "E 31\t: loss=3534.350, rmse=59.450, r2=0.497; v_loss=3138.721, v_rmse=56.024, v_r2=0.506; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=304, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=31, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='relu', model__dropout1=0.3074381764174994, model__dropout...82, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x000002001A7F8460>, <tensorflow_addons.metrics.r_square.RSquare object at 0x0000020008548040>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, model__second_dense=True, print_summary=True, validation_split=0.2925386448085039, verbose=0),\n",
              "                     include_settings=True, seq_length=34)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e69165-025e-4786-d045-6e6d5da7946d",
        "id": "svjWhd4FYvPD"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.504,RMSE=-38.404\n",
            "Finished: 2022-11-03 10:14:38.695897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL "
      ],
      "metadata": {
        "id": "6LznVkFxYvPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.730501800938371  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 225),\n",
        "('basemodel__epochs', 20),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'relu'),\n",
        "('basemodel__model__dropout1', 0.3310559320251978),\n",
        "('basemodel__model__dropout2', 0.3142813474813816),\n",
        "('basemodel__model__layer1', 473),\n",
        "('basemodel__model__layer2', 468),\n",
        "('basemodel__model__learning_rate', 0.0008125349625551938),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__model__second_dense', True),\n",
        "('basemodel__validation_split', 0.1),\n",
        "('clip_y', 103),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 65)\n",
        "```\n"
      ],
      "metadata": {
        "id": "KiL-gZkZYvPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=65\n",
        "CLIP=103\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=225,\n",
        "                           epochs=20,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='relu',\n",
        "                           model__dropout1=0.3310559320251978, \n",
        "                           model__dropout2=0.3142813474813816, \n",
        "                           model__layer1=473, \n",
        "                           model__layer2=468, \n",
        "                           model__learning_rate=0.0008125349625551938,\n",
        "                           model__optim=RMSprop,\n",
        "                           model__second_dense=True,\n",
        "                           validation_split=0.1, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "tEwHSsUCYvPE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3ca503-81bc-45e6-92ab-65b0f5f0b1dd",
        "id": "AMEzooPSYvPF"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_9 (Masking)         (None, 65, 25)            0         \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 473)               944108    \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 473)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 468)               221832    \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 468)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 469       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,166,409\n",
            "Trainable params: 1,166,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=1031.732, rmse=32.121, r2=0.117; v_loss=894.364, v_rmse=29.906, v_r2=0.234; \n",
            "E 2\t: loss=906.394, rmse=30.106, r2=0.224; v_loss=1113.153, v_rmse=33.364, v_r2=0.047; \n",
            "E 3\t: loss=848.086, rmse=29.122, r2=0.274; v_loss=684.411, v_rmse=26.161, v_r2=0.414; \n",
            "E 4\t: loss=755.045, rmse=27.478, r2=0.354; v_loss=721.013, v_rmse=26.852, v_r2=0.383; \n",
            "E 5\t: loss=676.104, rmse=26.002, r2=0.422; v_loss=406.029, v_rmse=20.150, v_r2=0.652; \n",
            "E 6\t: loss=614.404, rmse=24.787, r2=0.474; v_loss=467.234, v_rmse=21.616, v_r2=0.600; \n",
            "E 7\t: loss=552.375, rmse=23.503, r2=0.527; v_loss=396.130, v_rmse=19.903, v_r2=0.661; \n",
            "E 8\t: loss=505.623, rmse=22.486, r2=0.567; v_loss=405.747, v_rmse=20.143, v_r2=0.653; \n",
            "E 9\t: loss=470.500, rmse=21.691, r2=0.597; v_loss=330.207, v_rmse=18.172, v_r2=0.717; \n",
            "E 10\t: loss=425.332, rmse=20.624, r2=0.636; v_loss=385.761, v_rmse=19.641, v_r2=0.670; \n",
            "E 11\t: loss=402.135, rmse=20.053, r2=0.656; v_loss=387.220, v_rmse=19.678, v_r2=0.668; \n",
            "E 12\t: loss=377.839, rmse=19.438, r2=0.677; v_loss=312.575, v_rmse=17.680, v_r2=0.732; \n",
            "E 13\t: loss=360.033, rmse=18.975, r2=0.692; v_loss=267.143, v_rmse=16.345, v_r2=0.771; \n",
            "E 14\t: loss=339.594, rmse=18.428, r2=0.709; v_loss=251.987, v_rmse=15.874, v_r2=0.784; \n",
            "E 15\t: loss=320.440, rmse=17.901, r2=0.726; v_loss=515.667, v_rmse=22.708, v_r2=0.558; \n",
            "E 16\t: loss=311.032, rmse=17.636, r2=0.734; v_loss=262.944, v_rmse=16.216, v_r2=0.775; \n",
            "E 17\t: loss=287.172, rmse=16.946, r2=0.754; v_loss=255.685, v_rmse=15.990, v_r2=0.781; \n",
            "E 18\t: loss=267.642, rmse=16.360, r2=0.771; v_loss=300.068, v_rmse=17.322, v_r2=0.743; \n",
            "E 19\t: loss=251.014, rmse=15.843, r2=0.785; v_loss=198.432, v_rmse=14.087, v_r2=0.830; \n",
            "E 20\t: loss=237.699, rmse=15.417, r2=0.797; v_loss=270.835, v_rmse=16.457, v_r2=0.768; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=225, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=20, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='relu', model__dropout1=0.3310559320251978, model__dropout..., model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x0000020185485C40>, <tensorflow_addons.metrics.r_square.RSquare object at 0x0000020174E42580>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, model__second_dense=True, print_summary=True, validation_split=0.1, verbose=0),\n",
              "                     clip_y=103, include_settings=True, seq_length=65)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ffb8d2-684f-4de1-892b-cf0762983c95",
        "id": "WU1dYRTwYvPG"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.538,RMSE=-24.343\n",
            "Finished: 2022-11-03 12:33:35.991428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-Dense-2 "
      ],
      "metadata": {
        "id": "y28M-FK8zDq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL "
      ],
      "metadata": {
        "id": "9Zl5AM1czDq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Score: 0.4975628097395217  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 183),\n",
        "('basemodel__epochs', 31),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'elu'),\n",
        "('basemodel__model__activation3', 'elu'),\n",
        "('basemodel__model__dropout1', 0.3224261325682778),\n",
        "('basemodel__model__dropout2', 0.45581827067675496),\n",
        "('basemodel__model__dropout3', 0.26574521774822357),\n",
        "('basemodel__model__layer1', 512),\n",
        "('basemodel__model__layer2', 339),\n",
        "('basemodel__model__layer3', 50),\n",
        "('basemodel__model__learning_rate', 0.0010131951305302183),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__model__second_dense', True),\n",
        "('basemodel__validation_split', 0.8409177578463917),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 30)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y5_j9VhBzDq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=30\n",
        "CLIP=-1\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=183,\n",
        "                           epochs=31,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='elu',\n",
        "                           model__activation3='elu',\n",
        "                           model__dropout1=0.3224261325682778, \n",
        "                           model__dropout2=0.45581827067675496,\n",
        "                           model__dropout3=0.26574521774822357, \n",
        "                           model__layer1=512, \n",
        "                           model__layer2=339, \n",
        "                           model__layer3=50, \n",
        "                           model__learning_rate=0.0010131951305302183,\n",
        "                           model__optim=Adam,\n",
        "                           model__second_dense=True,\n",
        "                           validation_split=0.8409177578463917, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "yP84kQK2zDq5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd41367-635f-4fe0-bd74-128dfef7002d",
        "id": "dpx_rNVCzDq6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_5 (Masking)         (None, 30, 25)            0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 512)               1101824   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 339)               173907    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 339)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                17000     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 50)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,292,782\n",
            "Trainable params: 1,292,782\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=6848.453, rmse=82.755, r2=-0.184; v_loss=4962.357, v_rmse=70.444, v_r2=0.310; \n",
            "E 2\t: loss=3413.269, rmse=58.423, r2=0.410; v_loss=4993.637, v_rmse=70.666, v_r2=0.306; \n",
            "E 3\t: loss=3319.908, rmse=57.619, r2=0.426; v_loss=5011.012, v_rmse=70.788, v_r2=0.303; \n",
            "E 4\t: loss=3301.110, rmse=57.455, r2=0.429; v_loss=5004.138, v_rmse=70.740, v_r2=0.304; \n",
            "E 5\t: loss=3256.840, rmse=57.069, r2=0.437; v_loss=4862.338, v_rmse=69.730, v_r2=0.324; \n",
            "E 6\t: loss=3195.695, rmse=56.530, r2=0.448; v_loss=4895.626, v_rmse=69.969, v_r2=0.320; \n",
            "E 7\t: loss=3215.667, rmse=56.707, r2=0.444; v_loss=4937.556, v_rmse=70.268, v_r2=0.314; \n",
            "E 8\t: loss=3188.558, rmse=56.467, r2=0.449; v_loss=4908.395, v_rmse=70.060, v_r2=0.318; \n",
            "E 9\t: loss=3123.239, rmse=55.886, r2=0.460; v_loss=4869.430, v_rmse=69.781, v_r2=0.323; \n",
            "E 10\t: loss=3095.524, rmse=55.637, r2=0.465; v_loss=4828.215, v_rmse=69.485, v_r2=0.329; \n",
            "E 11\t: loss=2922.977, rmse=54.065, r2=0.495; v_loss=5219.806, v_rmse=72.248, v_r2=0.274; \n",
            "E 12\t: loss=2768.877, rmse=52.620, r2=0.521; v_loss=4055.179, v_rmse=63.680, v_r2=0.436; \n",
            "E 13\t: loss=2419.098, rmse=49.184, r2=0.582; v_loss=3990.941, v_rmse=63.174, v_r2=0.445; \n",
            "E 14\t: loss=2109.956, rmse=45.934, r2=0.635; v_loss=3489.246, v_rmse=59.070, v_r2=0.515; \n",
            "E 15\t: loss=1948.788, rmse=44.145, r2=0.663; v_loss=3562.616, v_rmse=59.688, v_r2=0.505; \n",
            "E 16\t: loss=1955.254, rmse=44.218, r2=0.662; v_loss=3586.189, v_rmse=59.885, v_r2=0.502; \n",
            "E 17\t: loss=1699.751, rmse=41.228, r2=0.706; v_loss=3593.442, v_rmse=59.945, v_r2=0.501; \n",
            "E 18\t: loss=1834.531, rmse=42.831, r2=0.683; v_loss=3378.377, v_rmse=58.124, v_r2=0.530; \n",
            "E 19\t: loss=1656.112, rmse=40.695, r2=0.714; v_loss=3502.161, v_rmse=59.179, v_r2=0.513; \n",
            "E 20\t: loss=1563.820, rmse=39.545, r2=0.730; v_loss=3493.875, v_rmse=59.109, v_r2=0.514; \n",
            "E 21\t: loss=1298.672, rmse=36.037, r2=0.776; v_loss=3475.155, v_rmse=58.950, v_r2=0.517; \n",
            "E 22\t: loss=1347.529, rmse=36.709, r2=0.767; v_loss=3728.029, v_rmse=61.058, v_r2=0.482; \n",
            "E 23\t: loss=1212.371, rmse=34.819, r2=0.790; v_loss=3654.585, v_rmse=60.453, v_r2=0.492; \n",
            "E 24\t: loss=1091.112, rmse=33.032, r2=0.811; v_loss=3605.078, v_rmse=60.042, v_r2=0.499; \n",
            "E 25\t: loss=1059.277, rmse=32.547, r2=0.817; v_loss=3692.550, v_rmse=60.766, v_r2=0.487; \n",
            "E 26\t: loss=1069.551, rmse=32.704, r2=0.815; v_loss=3705.028, v_rmse=60.869, v_r2=0.485; \n",
            "E 27\t: loss=939.698, rmse=30.654, r2=0.838; v_loss=3617.921, v_rmse=60.149, v_r2=0.497; \n",
            "E 28\t: loss=936.539, rmse=30.603, r2=0.838; v_loss=3750.486, v_rmse=61.241, v_r2=0.479; \n",
            "E 29\t: loss=866.086, rmse=29.429, r2=0.850; v_loss=3557.247, v_rmse=59.643, v_r2=0.506; \n",
            "E 30\t: loss=793.460, rmse=28.168, r2=0.863; v_loss=3744.373, v_rmse=61.191, v_r2=0.480; \n",
            "E 31\t: loss=766.786, rmse=27.691, r2=0.867; v_loss=3920.802, v_rmse=62.616, v_r2=0.455; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=183, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=31, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='elu', model__activation3='elu', model__dropout1=0.3224261...83, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x000002015CA53D60>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000201865DF610>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, model__second_dense=True, print_summary=True, validation_split=0.8409177578463917, verbose=0),\n",
              "                     include_settings=True, seq_length=30)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f492c0-d27c-4453-f31e-a6c18ae8baca",
        "id": "qzoIZZmlzDq7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002015CDB43A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "R2=-0.056,RMSE=-56.017\n",
            "Finished: 2022-11-03 10:40:45.938988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL \n"
      ],
      "metadata": {
        "id": "wrpte1WXzDq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.7786741619787652  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 229),\n",
        "('basemodel__epochs', 37),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'relu'),\n",
        "('basemodel__model__activation3', 'sigmoid'),\n",
        "('basemodel__model__dropout1', 0.4312949059884308),\n",
        "('basemodel__model__dropout2', 0.3807450679193153),\n",
        "('basemodel__model__dropout3', 0.6916033873523364),\n",
        "('basemodel__model__layer1', 167),\n",
        "('basemodel__model__layer2', 337),\n",
        "('basemodel__model__layer3', 289),\n",
        "('basemodel__model__learning_rate', 0.0013556548021189216),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__model__second_dense', True),\n",
        "('basemodel__validation_split', 0.16231298330980526),\n",
        "('clip_y', 120),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 33)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-RCr6qz8zDq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=33\n",
        "CLIP=120\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=229,\n",
        "                           epochs=37,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='relu',\n",
        "                           model__activation3='sigmoid',\n",
        "                           model__dropout1=0.4312949059884308, \n",
        "                           model__dropout2=0.3807450679193153,\n",
        "                           model__dropout3=0.6916033873523364, \n",
        "                           model__layer1=167, \n",
        "                           model__layer2=337, \n",
        "                           model__layer3=289, \n",
        "                           model__learning_rate=0.0013556548021189216,\n",
        "                           model__optim=Adam,\n",
        "                           model__second_dense=True,\n",
        "                           validation_split=0.16231298330980526, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "TCGZ2Pr4zDq8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac0c15c-feb5-4fa7-af92-a5bbd585b4a7",
        "id": "Fd65WHLTzDq9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_6 (Masking)         (None, 33, 25)            0         \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 167)               128924    \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 167)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 337)               56616     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 337)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 289)               97682     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 289)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 290       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 283,512\n",
            "Trainable params: 283,512\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=4848.964, rmse=69.635, r2=-2.090; v_loss=2844.121, v_rmse=53.330, v_r2=-0.774; \n",
            "E 2\t: loss=2295.310, rmse=47.909, r2=-0.463; v_loss=1765.120, v_rmse=42.013, v_r2=-0.101; \n",
            "E 3\t: loss=1751.040, rmse=41.845, r2=-0.116; v_loss=1610.639, v_rmse=40.133, v_r2=-0.005; \n",
            "E 4\t: loss=1678.909, rmse=40.974, r2=-0.070; v_loss=1603.516, v_rmse=40.044, v_r2=-0.000; \n",
            "E 5\t: loss=1674.912, rmse=40.926, r2=-0.067; v_loss=1604.949, v_rmse=40.062, v_r2=-0.001; \n",
            "E 6\t: loss=1673.253, rmse=40.905, r2=-0.066; v_loss=1604.974, v_rmse=40.062, v_r2=-0.001; \n",
            "E 7\t: loss=1669.121, rmse=40.855, r2=-0.064; v_loss=1604.861, v_rmse=40.061, v_r2=-0.001; \n",
            "E 8\t: loss=1614.012, rmse=40.175, r2=-0.029; v_loss=1186.743, v_rmse=34.449, v_r2=0.260; \n",
            "E 9\t: loss=1265.988, rmse=35.581, r2=0.193; v_loss=990.964, v_rmse=31.480, v_r2=0.382; \n",
            "E 10\t: loss=1010.370, rmse=31.786, r2=0.356; v_loss=606.018, v_rmse=24.617, v_r2=0.622; \n",
            "E 11\t: loss=682.182, rmse=26.119, r2=0.565; v_loss=534.583, v_rmse=23.121, v_r2=0.667; \n",
            "E 12\t: loss=610.845, rmse=24.715, r2=0.611; v_loss=428.297, v_rmse=20.695, v_r2=0.733; \n",
            "E 13\t: loss=570.311, rmse=23.881, r2=0.637; v_loss=540.997, v_rmse=23.259, v_r2=0.663; \n",
            "E 14\t: loss=571.375, rmse=23.903, r2=0.636; v_loss=394.562, v_rmse=19.864, v_r2=0.754; \n",
            "E 15\t: loss=542.153, rmse=23.284, r2=0.654; v_loss=386.175, v_rmse=19.651, v_r2=0.759; \n",
            "E 16\t: loss=526.047, rmse=22.936, r2=0.665; v_loss=390.695, v_rmse=19.766, v_r2=0.756; \n",
            "E 17\t: loss=502.706, rmse=22.421, r2=0.680; v_loss=348.380, v_rmse=18.665, v_r2=0.783; \n",
            "E 18\t: loss=491.324, rmse=22.166, r2=0.687; v_loss=337.587, v_rmse=18.374, v_r2=0.789; \n",
            "E 19\t: loss=486.129, rmse=22.048, r2=0.690; v_loss=416.801, v_rmse=20.416, v_r2=0.740; \n",
            "E 20\t: loss=465.774, rmse=21.582, r2=0.703; v_loss=372.233, v_rmse=19.293, v_r2=0.768; \n",
            "E 21\t: loss=455.050, rmse=21.332, r2=0.710; v_loss=301.267, v_rmse=17.357, v_r2=0.812; \n",
            "E 22\t: loss=444.925, rmse=21.093, r2=0.716; v_loss=308.981, v_rmse=17.578, v_r2=0.807; \n",
            "E 23\t: loss=431.132, rmse=20.764, r2=0.725; v_loss=310.538, v_rmse=17.622, v_r2=0.806; \n",
            "E 24\t: loss=417.613, rmse=20.436, r2=0.734; v_loss=299.490, v_rmse=17.306, v_r2=0.813; \n",
            "E 25\t: loss=398.933, rmse=19.973, r2=0.746; v_loss=271.798, v_rmse=16.486, v_r2=0.830; \n",
            "E 26\t: loss=389.236, rmse=19.729, r2=0.752; v_loss=268.905, v_rmse=16.398, v_r2=0.832; \n",
            "E 27\t: loss=380.240, rmse=19.500, r2=0.758; v_loss=260.492, v_rmse=16.140, v_r2=0.838; \n",
            "E 28\t: loss=361.933, rmse=19.025, r2=0.769; v_loss=262.928, v_rmse=16.215, v_r2=0.836; \n",
            "E 29\t: loss=355.231, rmse=18.848, r2=0.774; v_loss=258.257, v_rmse=16.070, v_r2=0.839; \n",
            "E 30\t: loss=343.292, rmse=18.528, r2=0.781; v_loss=253.639, v_rmse=15.926, v_r2=0.842; \n",
            "E 31\t: loss=333.673, rmse=18.267, r2=0.787; v_loss=254.446, v_rmse=15.951, v_r2=0.841; \n",
            "E 32\t: loss=331.078, rmse=18.196, r2=0.789; v_loss=263.044, v_rmse=16.219, v_r2=0.836; \n",
            "E 33\t: loss=323.754, rmse=17.993, r2=0.794; v_loss=254.790, v_rmse=15.962, v_r2=0.841; \n",
            "E 34\t: loss=330.462, rmse=18.179, r2=0.789; v_loss=294.882, v_rmse=17.172, v_r2=0.816; \n",
            "E 35\t: loss=313.665, rmse=17.711, r2=0.800; v_loss=265.789, v_rmse=16.303, v_r2=0.834; \n",
            "E 36\t: loss=316.886, rmse=17.801, r2=0.798; v_loss=288.650, v_rmse=16.990, v_r2=0.820; \n",
            "E 37\t: loss=315.941, rmse=17.775, r2=0.799; v_loss=261.803, v_rmse=16.180, v_r2=0.837; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=229, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=37, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='relu', model__activation3='sigmoid', model__dropout1=0.43...ss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000201B29833D0>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000201B2983400>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, model__second_dense=True, print_summary=True, validation_split=0.16231298330980526, verbose=0),\n",
              "                     clip_y=120, include_settings=True, seq_length=33)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401027d7-218d-4524-e424-b1ad144c2625",
        "id": "O8csd7JVzDq-"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000201853E8A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "R2=0.756,RMSE=-20.492\n",
            "Finished: 2022-11-03 10:42:57.002708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-LSTM-Dense (TODO)"
      ],
      "metadata": {
        "id": "5z_LZWx32lXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "Pu0gM_uE2lXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.4617027829919557  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 107),\n",
        "('basemodel__epochs', 24),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'tanh'),\n",
        "('basemodel__model__activation3', 'selu'),\n",
        "('basemodel__model__dropout1', 0.7024252271635242),\n",
        "('basemodel__model__dropout2', 0.16380096029242686),\n",
        "('basemodel__model__dropout3', 0.30428197114787425),\n",
        "('basemodel__model__layer1', 16),\n",
        "('basemodel__model__layer2', 161),\n",
        "('basemodel__model__layer3', 291),\n",
        "('basemodel__model__learning_rate', 0.002733349824393456),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__model__second_dense', False),\n",
        "('basemodel__validation_split', 0.10395419197584942),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 31)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HRabKU0p2lXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=31\n",
        "CLIP=-1\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=107,\n",
        "                           epochs=24,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='tanh',\n",
        "                           model__activation3='selu',\n",
        "                           model__dropout1=0.7024252271635242, \n",
        "                           model__dropout2=0.16380096029242686,\n",
        "                           model__dropout3=0.30428197114787425, \n",
        "                           model__layer1=16, \n",
        "                           model__layer2=161, \n",
        "                           model__layer3=291, \n",
        "                           model__learning_rate=0.002733349824393456,\n",
        "                           model__optim=RMSprop,\n",
        "                           model__second_dense=False,\n",
        "                           validation_split=0.10395419197584942, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "6BGiwksX2lXG"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f6cd48-e1a7-428b-9ac4-4edecdb389a0",
        "id": "Yert7PMH2lXI"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_7 (Masking)         (None, 31, 25)            0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 31, 16)            2688      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 31, 16)            0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 161)               114632    \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 161)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 291)               47142     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 291)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 292       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164,754\n",
            "Trainable params: 164,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=5378.973, rmse=73.341, r2=0.235; v_loss=4271.350, v_rmse=65.356, v_r2=0.303; \n",
            "E 2\t: loss=4537.351, rmse=67.360, r2=0.354; v_loss=3209.946, v_rmse=56.656, v_r2=0.477; \n",
            "E 3\t: loss=4003.044, rmse=63.270, r2=0.430; v_loss=2895.792, v_rmse=53.813, v_r2=0.528; \n",
            "E 4\t: loss=3687.333, rmse=60.723, r2=0.475; v_loss=3748.501, v_rmse=61.225, v_r2=0.389; \n",
            "E 5\t: loss=3465.279, rmse=58.867, r2=0.507; v_loss=2589.646, v_rmse=50.889, v_r2=0.578; \n",
            "E 6\t: loss=3297.818, rmse=57.427, r2=0.531; v_loss=2527.543, v_rmse=50.275, v_r2=0.588; \n",
            "E 7\t: loss=3179.426, rmse=56.386, r2=0.548; v_loss=2485.308, v_rmse=49.853, v_r2=0.595; \n",
            "E 8\t: loss=3098.582, rmse=55.665, r2=0.559; v_loss=2532.980, v_rmse=50.329, v_r2=0.587; \n",
            "E 9\t: loss=3016.461, rmse=54.922, r2=0.571; v_loss=2682.075, v_rmse=51.789, v_r2=0.563; \n",
            "E 10\t: loss=2973.512, rmse=54.530, r2=0.577; v_loss=2747.709, v_rmse=52.419, v_r2=0.552; \n",
            "E 11\t: loss=2917.083, rmse=54.010, r2=0.585; v_loss=3179.414, v_rmse=56.386, v_r2=0.482; \n",
            "E 12\t: loss=2854.147, rmse=53.424, r2=0.594; v_loss=2789.207, v_rmse=52.813, v_r2=0.545; \n",
            "E 13\t: loss=2832.271, rmse=53.219, r2=0.597; v_loss=2802.523, v_rmse=52.939, v_r2=0.543; \n",
            "E 14\t: loss=2772.221, rmse=52.652, r2=0.606; v_loss=3212.629, v_rmse=56.680, v_r2=0.476; \n",
            "E 15\t: loss=2768.018, rmse=52.612, r2=0.606; v_loss=3047.728, v_rmse=55.206, v_r2=0.503; \n",
            "E 16\t: loss=2735.629, rmse=52.303, r2=0.611; v_loss=2955.608, v_rmse=54.366, v_r2=0.518; \n",
            "E 17\t: loss=2703.232, rmse=51.993, r2=0.615; v_loss=3444.243, v_rmse=58.688, v_r2=0.438; \n",
            "E 18\t: loss=2692.286, rmse=51.887, r2=0.617; v_loss=3571.252, v_rmse=59.760, v_r2=0.418; \n",
            "E 19\t: loss=2664.685, rmse=51.621, r2=0.621; v_loss=3487.817, v_rmse=59.058, v_r2=0.431; \n",
            "E 20\t: loss=2620.928, rmse=51.195, r2=0.627; v_loss=3474.688, v_rmse=58.946, v_r2=0.433; \n",
            "E 21\t: loss=2603.128, rmse=51.021, r2=0.630; v_loss=3531.263, v_rmse=59.424, v_r2=0.424; \n",
            "E 22\t: loss=2566.393, rmse=50.660, r2=0.635; v_loss=4340.374, v_rmse=65.882, v_r2=0.292; \n",
            "E 23\t: loss=2572.996, rmse=50.725, r2=0.634; v_loss=3509.864, v_rmse=59.244, v_r2=0.428; \n",
            "E 24\t: loss=2542.564, rmse=50.424, r2=0.638; v_loss=3424.425, v_rmse=58.519, v_r2=0.442; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=107, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=24, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='tanh', model__activation3='selu', model__dropout1=0.70242...__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x0000020174E5CAF0>, <tensorflow_addons.metrics.r_square.RSquare object at 0x0000020174E5CEE0>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, model__second_dense=False, print_summary=True, validation_split=0.10395419197584942, verbose=0),\n",
              "                     include_settings=True, seq_length=31)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f296c77-013b-490b-8682-f1864f13c15f",
        "id": "rqyVHO5n2lXK"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.278,RMSE=-46.342\n",
            "Finished: 2022-11-03 10:47:23.442986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL"
      ],
      "metadata": {
        "id": "DZEmsy2m2lXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.6675592008245431  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 450),\n",
        "('basemodel__epochs', 48),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'tanh'),\n",
        "('basemodel__model__activation3', 'elu'),\n",
        "('basemodel__model__dropout1', 0.8145333249976955),\n",
        "('basemodel__model__dropout2', 0.4289024842480126),\n",
        "('basemodel__model__dropout3', 0.1),\n",
        "('basemodel__model__layer1', 496),\n",
        "('basemodel__model__layer2', 168),\n",
        "('basemodel__model__layer3', 254),\n",
        "('basemodel__model__learning_rate', 0.003118118507191013),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__model__second_dense', False),\n",
        "('basemodel__validation_split', 0.49238676803302384),\n",
        "('clip_y', 120),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler()),\n",
        "('seq_length', 42)\n",
        "```"
      ],
      "metadata": {
        "id": "5340ZcMG2lXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH=42\n",
        "CLIP=120\n",
        "\n",
        "model = LSTMWrapperRegressor(\n",
        "        clip_y=CLIP, seq_length=SEQ_LENGTH, scaler=StandardScaler(),\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=450,\n",
        "                           epochs=48,     \n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='tanh',\n",
        "                           model__activation3='elu',\n",
        "                           model__dropout1=0.8145333249976955, \n",
        "                           model__dropout2=0.4289024842480126,\n",
        "                           model__dropout3=0.1, \n",
        "                           model__layer1=496, \n",
        "                           model__layer2=168, \n",
        "                           model__layer3=254, \n",
        "                           model__learning_rate=0.003118118507191013,\n",
        "                           model__optim=Adam,\n",
        "                           model__second_dense=False,\n",
        "                           validation_split=0.49238676803302384, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )"
      ],
      "metadata": {
        "id": "njy3V5mz2lXL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52eb7fe-c7ba-437e-9e92-afb650b4241a",
        "id": "0ZDq9Rf-2lXM"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_11 (Masking)        (None, 42, 25)            0         \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, 42, 496)           1035648   \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 42, 496)           0         \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 168)               446880    \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 168)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 254)               42926     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 254)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,525,709\n",
            "Trainable params: 1,525,709\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=2380.800, rmse=48.793, r2=-0.496; v_loss=1592.997, v_rmse=39.912, v_r2=0.001; \n",
            "E 2\t: loss=1582.558, rmse=39.781, r2=0.006; v_loss=1502.669, v_rmse=38.764, v_r2=0.058; \n",
            "E 3\t: loss=1439.686, rmse=37.943, r2=0.096; v_loss=1284.522, v_rmse=35.840, v_r2=0.195; \n",
            "E 4\t: loss=1279.918, rmse=35.776, r2=0.196; v_loss=1296.944, v_rmse=36.013, v_r2=0.187; \n",
            "E 5\t: loss=1248.351, rmse=35.332, r2=0.216; v_loss=1275.035, v_rmse=35.708, v_r2=0.201; \n",
            "E 6\t: loss=1225.506, rmse=35.007, r2=0.230; v_loss=1250.316, v_rmse=35.360, v_r2=0.216; \n",
            "E 7\t: loss=1201.665, rmse=34.665, r2=0.245; v_loss=1228.349, v_rmse=35.048, v_r2=0.230; \n",
            "E 8\t: loss=1178.836, rmse=34.334, r2=0.259; v_loss=1207.363, v_rmse=34.747, v_r2=0.243; \n",
            "E 9\t: loss=1164.281, rmse=34.122, r2=0.269; v_loss=1147.315, v_rmse=33.872, v_r2=0.281; \n",
            "E 10\t: loss=1135.721, rmse=33.700, r2=0.287; v_loss=1182.159, v_rmse=34.383, v_r2=0.259; \n",
            "E 11\t: loss=1128.107, rmse=33.587, r2=0.291; v_loss=1176.303, v_rmse=34.297, v_r2=0.263; \n",
            "E 12\t: loss=1110.651, rmse=33.326, r2=0.302; v_loss=1134.220, v_rmse=33.678, v_r2=0.289; \n",
            "E 13\t: loss=1083.505, rmse=32.917, r2=0.319; v_loss=1107.596, v_rmse=33.281, v_r2=0.306; \n",
            "E 14\t: loss=1021.794, rmse=31.966, r2=0.358; v_loss=925.391, v_rmse=30.420, v_r2=0.420; \n",
            "E 15\t: loss=1011.030, rmse=31.797, r2=0.365; v_loss=804.552, v_rmse=28.365, v_r2=0.496; \n",
            "E 16\t: loss=726.903, rmse=26.961, r2=0.543; v_loss=627.998, v_rmse=25.060, v_r2=0.606; \n",
            "E 17\t: loss=608.726, rmse=24.672, r2=0.618; v_loss=536.647, v_rmse=23.166, v_r2=0.664; \n",
            "E 18\t: loss=533.319, rmse=23.094, r2=0.665; v_loss=541.196, v_rmse=23.264, v_r2=0.661; \n",
            "E 19\t: loss=496.742, rmse=22.288, r2=0.688; v_loss=624.790, v_rmse=24.996, v_r2=0.608; \n",
            "E 20\t: loss=497.322, rmse=22.301, r2=0.688; v_loss=516.402, v_rmse=22.724, v_r2=0.676; \n",
            "E 21\t: loss=467.419, rmse=21.620, r2=0.706; v_loss=484.233, v_rmse=22.005, v_r2=0.696; \n",
            "E 22\t: loss=434.029, rmse=20.833, r2=0.727; v_loss=635.587, v_rmse=25.211, v_r2=0.602; \n",
            "E 23\t: loss=437.652, rmse=20.920, r2=0.725; v_loss=477.440, v_rmse=21.850, v_r2=0.701; \n",
            "E 24\t: loss=412.520, rmse=20.311, r2=0.741; v_loss=462.113, v_rmse=21.497, v_r2=0.710; \n",
            "E 25\t: loss=396.233, rmse=19.906, r2=0.751; v_loss=460.603, v_rmse=21.462, v_r2=0.711; \n",
            "E 26\t: loss=388.120, rmse=19.701, r2=0.756; v_loss=437.670, v_rmse=20.921, v_r2=0.726; \n",
            "E 27\t: loss=386.288, rmse=19.654, r2=0.757; v_loss=499.940, v_rmse=22.359, v_r2=0.687; \n",
            "E 28\t: loss=367.751, rmse=19.177, r2=0.769; v_loss=457.049, v_rmse=21.379, v_r2=0.714; \n",
            "E 29\t: loss=364.981, rmse=19.104, r2=0.771; v_loss=457.062, v_rmse=21.379, v_r2=0.714; \n",
            "E 30\t: loss=335.634, rmse=18.320, r2=0.789; v_loss=437.001, v_rmse=20.905, v_r2=0.726; \n",
            "E 31\t: loss=343.042, rmse=18.521, r2=0.785; v_loss=392.653, v_rmse=19.815, v_r2=0.754; \n",
            "E 32\t: loss=322.390, rmse=17.955, r2=0.797; v_loss=434.086, v_rmse=20.835, v_r2=0.728; \n",
            "E 33\t: loss=298.324, rmse=17.272, r2=0.813; v_loss=412.631, v_rmse=20.313, v_r2=0.741; \n",
            "E 34\t: loss=286.760, rmse=16.934, r2=0.820; v_loss=404.005, v_rmse=20.100, v_r2=0.747; \n",
            "E 35\t: loss=309.878, rmse=17.603, r2=0.805; v_loss=391.577, v_rmse=19.788, v_r2=0.755; \n",
            "E 36\t: loss=281.991, rmse=16.793, r2=0.823; v_loss=379.499, v_rmse=19.481, v_r2=0.762; \n",
            "E 37\t: loss=272.856, rmse=16.518, r2=0.829; v_loss=440.811, v_rmse=20.995, v_r2=0.724; \n",
            "E 38\t: loss=241.524, rmse=15.541, r2=0.848; v_loss=390.446, v_rmse=19.760, v_r2=0.755; \n",
            "E 39\t: loss=234.092, rmse=15.300, r2=0.853; v_loss=391.961, v_rmse=19.798, v_r2=0.754; \n",
            "E 40\t: loss=220.120, rmse=14.836, r2=0.862; v_loss=470.140, v_rmse=21.683, v_r2=0.705; \n",
            "E 41\t: loss=244.621, rmse=15.640, r2=0.846; v_loss=422.238, v_rmse=20.548, v_r2=0.735; \n",
            "E 42\t: loss=212.997, rmse=14.594, r2=0.866; v_loss=399.871, v_rmse=19.997, v_r2=0.749; \n",
            "E 43\t: loss=201.116, rmse=14.182, r2=0.874; v_loss=393.161, v_rmse=19.828, v_r2=0.754; \n",
            "E 44\t: loss=194.399, rmse=13.943, r2=0.878; v_loss=392.003, v_rmse=19.799, v_r2=0.754; \n",
            "E 45\t: loss=190.932, rmse=13.818, r2=0.880; v_loss=399.642, v_rmse=19.991, v_r2=0.749; \n",
            "E 46\t: loss=200.101, rmse=14.146, r2=0.874; v_loss=419.477, v_rmse=20.481, v_r2=0.737; \n",
            "E 47\t: loss=232.695, rmse=15.254, r2=0.854; v_loss=378.056, v_rmse=19.444, v_r2=0.763; \n",
            "E 48\t: loss=170.137, rmse=13.044, r2=0.893; v_loss=421.308, v_rmse=20.526, v_r2=0.736; \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMWrapperRegressor(basemodel=KerasRegressor(batch_size=450, callbacks=[<keras.callbacks.EarlyStopping object at 0x000002007EAFDFD0>, <keras.callbacks.LambdaCallback object at 0x0000020009161910>], epochs=48, model=<function create_model at 0x0000020009692AF0>, model__activation1='tanh', model__activation2='tanh', model__activation3='elu', model__dropout1=0.814533...s='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000201D0458E20>, <tensorflow_addons.metrics.r_square.RSquare object at 0x000002001A3A8BB0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, model__second_dense=False, print_summary=True, validation_split=0.49238676803302384, verbose=0),\n",
              "                     clip_y=120, include_settings=True, seq_length=42)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(test,model)\n",
        "test_wr = gen_test_wrapper(test_sc, SEQ_LENGTH, cols=model.seq_cols)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_wr))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecde733-92b5-40e6-a77a-104ccf47c043",
        "id": "GsrsS-Ga2lXO"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.775,RMSE=-19.677\n",
            "Finished: 2022-11-03 12:56:20.532875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pr8HlKjxjOvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}