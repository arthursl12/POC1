{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iqDPLB2EDXSK",
        "AU6ipeRozJz-",
        "QinQ4hWStzHt",
        "boZqFQNlraCh",
        "IIXnBTkfxpCf",
        "SL1dv6EX4NUk",
        "DU8TxguXIChd",
        "FDHk5EzS8XuY"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMM9i/TqOIaOQl6C5bprGS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthursl12/POC1/blob/main/POC2_FD004_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iqDPLB2EDXSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install scikeras;\n",
        "%pip install -U tensorflow-addons;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ObuYEARk28q",
        "outputId": "04bb8d4d-a0ec-4b5b-8710-380340f46d1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in h:\\anaconda3\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in h:\\anaconda3\\lib\\site-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in h:\\anaconda3\\lib\\site-packages (from scikeras) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in h:\\anaconda3\\lib\\site-packages (from packaging>=0.21->scikeras) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in h:\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (2.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow-addons in h:\\anaconda3\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in h:\\anaconda3\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: packaging in h:\\anaconda3\\lib\\site-packages (from tensorflow-addons) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in h:\\anaconda3\\lib\\site-packages (from packaging->tensorflow-addons) (2.4.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import glob\n",
        "import datetime"
      ],
      "metadata": {
        "id": "i0Z0Zs7YcgTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc11417-147b-41b3-c2ff-22ff942bac7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "H:\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "H:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
            "H:\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from sklearn.base import BaseEstimator,RegressorMixin"
      ],
      "metadata": {
        "id": "wmJyWwoFHcFL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError as RMSE\n",
        "\n",
        "from keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import RSquare as R2\n",
        "\n",
        "\n",
        "\n",
        "from scikeras.wrappers import KerasRegressor"
      ],
      "metadata": {
        "id": "LnMMh6xN33s4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_palette('colorblind')"
      ],
      "metadata": {
        "id": "yIpSdBdJ-uWd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED']=str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "oWUcQTaa3lth"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "fwRwlCA7Yt4C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "AU6ipeRozJz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Running on CoLab')\n",
        "    IN_COLAB = True\n",
        "    folder=\"/content/CMaps/\"\n",
        "\n",
        "    # Dataset Download \n",
        "    os.system('git clone https://github.com/arthursl12/dataset_2')\n",
        "    os.system('mv /content/dataset_2/CMaps /content/CMaps')\n",
        "    os.system('mv /content/dataset_2/data_processing /content/data_processing')\n",
        "    os.system('rm -rf dataset_2')\n",
        "else:\n",
        "    print('Not running on CoLab')\n",
        "    IN_COLAB = False\n",
        "    folder=\"CMaps/\"\n",
        "    %cd dataset_2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgkQUCfsWAGK",
        "outputId": "69c73eca-89b5-47c1-ac46-538e01db30a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not running on CoLab\n",
            "C:\\Users\\Arthur Lima\\POC\\dataset_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_processing.processing import DatasetProcessing\n",
        "from data_processing.training import HyperparameterSearch, reclipper_scorer\n",
        "from data_processing.eval import Evaluation"
      ],
      "metadata": {
        "id": "FUQ5tHe4Eu7z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proc = DatasetProcessing()"
      ],
      "metadata": {
        "id": "g1BmyudxzUz-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Integration"
      ],
      "metadata": {
        "id": "Q4QwyfhXs_hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data are provided as a zip-compressed text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:  \n",
        "\n",
        "1) unit number   \n",
        "2) time, in cycles  \n",
        "3) operational setting 1  \n",
        "4) operational setting 2  \n",
        "5) operational setting 3    \n",
        "6) sensor measurement 1    \n",
        "7) sensor measurement 2  \n",
        "...  \n",
        "26) sensor measurement 20\n",
        "\n",
        "\n",
        "There are 6 conditions (or combinations) which the 3 operational settings can take.  \n",
        "Condition 1: Altitude = 0, Mach Number = 0, TRA = 100  \n",
        "Condition 2: Altitude = 10, Mach Number = 0.25, TRA = 100  \n",
        "Condition 3: Altitude = 20, Mach Number = 0.7 TRA = 100  \n",
        "Condition 4: Altitude = 25, Mach Number = 0.62, TRA = 60  \n",
        "Condition 5: Altitude = 35 Mach Number = 0.84, TRA = 100  \n",
        "Condition 6: Altitude = 42, Mach Number = 0.84, TRA = 100  \n",
        "  \n",
        "There is slight variation in all these conditions so you may get numbers like 24.453 instead of 25 exactly.\n",
        "\n",
        "FD001: Condition 1 only  \n",
        "FD002: Mix of all the conditions  \n",
        "FD003: Condition 1 only  \n",
        "FD004: Mix of all conditions  \n"
      ],
      "metadata": {
        "id": "PQe-SyeYc6Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_cols, settings_cols, sensors_cols, cols = proc.column_names()\n",
        "train, test, y_test = proc.read_dataset(4,folder=folder)\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "-yRYxz2hh4xE",
        "outputId": "0d38cf9b-daac-4d59-8252-8f21919fe040"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       unit_number  time     op_1    op_2   op_3     s_0     s_1      s_2  \\\n",
              "0                1     1  42.0049  0.8400  100.0  445.00  549.68  1343.43   \n",
              "1                1     2  20.0020  0.7002  100.0  491.19  606.07  1477.61   \n",
              "2                1     3  42.0038  0.8409  100.0  445.00  548.95  1343.12   \n",
              "3                1     4  42.0000  0.8400  100.0  445.00  548.70  1341.24   \n",
              "4                1     5  25.0063  0.6207   60.0  462.54  536.10  1255.23   \n",
              "...            ...   ...      ...     ...    ...     ...     ...      ...   \n",
              "61244          249   251   9.9998  0.2500  100.0  489.05  605.33  1516.36   \n",
              "61245          249   252   0.0028  0.0015  100.0  518.67  643.42  1598.92   \n",
              "61246          249   253   0.0029  0.0000  100.0  518.67  643.68  1607.72   \n",
              "61247          249   254  35.0046  0.8400  100.0  449.44  555.77  1381.29   \n",
              "61248          249   255  42.0030  0.8400  100.0  445.00  549.85  1369.75   \n",
              "\n",
              "           s_3    s_4  ...    s_11     s_12     s_13     s_14  s_15  s_16  \\\n",
              "0      1112.93   3.91  ...  129.78  2387.99  8074.83   9.3335  0.02   330   \n",
              "1      1237.50   9.35  ...  312.59  2387.73  8046.13   9.1913  0.02   361   \n",
              "2      1117.05   3.91  ...  129.62  2387.97  8066.62   9.4007  0.02   329   \n",
              "3      1118.03   3.91  ...  129.80  2388.02  8076.05   9.3369  0.02   328   \n",
              "4      1033.59   7.05  ...  164.11  2028.08  7865.80  10.8366  0.02   305   \n",
              "...        ...    ...  ...     ...      ...      ...      ...   ...   ...   \n",
              "61244  1315.28  10.52  ...  380.16  2388.73  8185.69   8.4541  0.03   372   \n",
              "61245  1426.77  14.62  ...  535.02  2388.46  8185.47   8.2221  0.03   396   \n",
              "61246  1430.56  14.62  ...  535.41  2388.48  8193.94   8.2525  0.03   395   \n",
              "61247  1148.18   5.48  ...  187.92  2388.83  8125.64   9.0515  0.02   337   \n",
              "61248  1147.45   3.91  ...  134.32  2388.66  8144.33   9.1207  0.02   333   \n",
              "\n",
              "       s_17    s_18   s_19     s_20  \n",
              "0      2212  100.00  10.62   6.3670  \n",
              "1      2324  100.00  24.37  14.6552  \n",
              "2      2212  100.00  10.48   6.4213  \n",
              "3      2212  100.00  10.54   6.4176  \n",
              "4      1915   84.93  14.03   8.6754  \n",
              "...     ...     ...    ...      ...  \n",
              "61244  2319  100.00  29.11  17.5234  \n",
              "61245  2388  100.00  39.38  23.7151  \n",
              "61246  2388  100.00  39.78  23.8270  \n",
              "61247  2223  100.00  15.26   9.0774  \n",
              "61248  2212  100.00  10.66   6.4341  \n",
              "\n",
              "[61249 rows x 26 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unit_number</th>\n",
              "      <th>time</th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>42.0049</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.68</td>\n",
              "      <td>1343.43</td>\n",
              "      <td>1112.93</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.78</td>\n",
              "      <td>2387.99</td>\n",
              "      <td>8074.83</td>\n",
              "      <td>9.3335</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.62</td>\n",
              "      <td>6.3670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>20.0020</td>\n",
              "      <td>0.7002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>491.19</td>\n",
              "      <td>606.07</td>\n",
              "      <td>1477.61</td>\n",
              "      <td>1237.50</td>\n",
              "      <td>9.35</td>\n",
              "      <td>...</td>\n",
              "      <td>312.59</td>\n",
              "      <td>2387.73</td>\n",
              "      <td>8046.13</td>\n",
              "      <td>9.1913</td>\n",
              "      <td>0.02</td>\n",
              "      <td>361</td>\n",
              "      <td>2324</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24.37</td>\n",
              "      <td>14.6552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>42.0038</td>\n",
              "      <td>0.8409</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.95</td>\n",
              "      <td>1343.12</td>\n",
              "      <td>1117.05</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.62</td>\n",
              "      <td>2387.97</td>\n",
              "      <td>8066.62</td>\n",
              "      <td>9.4007</td>\n",
              "      <td>0.02</td>\n",
              "      <td>329</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>6.4213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>42.0000</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.70</td>\n",
              "      <td>1341.24</td>\n",
              "      <td>1118.03</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>129.80</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8076.05</td>\n",
              "      <td>9.3369</td>\n",
              "      <td>0.02</td>\n",
              "      <td>328</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.54</td>\n",
              "      <td>6.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>25.0063</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.10</td>\n",
              "      <td>1255.23</td>\n",
              "      <td>1033.59</td>\n",
              "      <td>7.05</td>\n",
              "      <td>...</td>\n",
              "      <td>164.11</td>\n",
              "      <td>2028.08</td>\n",
              "      <td>7865.80</td>\n",
              "      <td>10.8366</td>\n",
              "      <td>0.02</td>\n",
              "      <td>305</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.03</td>\n",
              "      <td>8.6754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61244</th>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>9.9998</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>100.0</td>\n",
              "      <td>489.05</td>\n",
              "      <td>605.33</td>\n",
              "      <td>1516.36</td>\n",
              "      <td>1315.28</td>\n",
              "      <td>10.52</td>\n",
              "      <td>...</td>\n",
              "      <td>380.16</td>\n",
              "      <td>2388.73</td>\n",
              "      <td>8185.69</td>\n",
              "      <td>8.4541</td>\n",
              "      <td>0.03</td>\n",
              "      <td>372</td>\n",
              "      <td>2319</td>\n",
              "      <td>100.00</td>\n",
              "      <td>29.11</td>\n",
              "      <td>17.5234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61245</th>\n",
              "      <td>249</td>\n",
              "      <td>252</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.42</td>\n",
              "      <td>1598.92</td>\n",
              "      <td>1426.77</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>535.02</td>\n",
              "      <td>2388.46</td>\n",
              "      <td>8185.47</td>\n",
              "      <td>8.2221</td>\n",
              "      <td>0.03</td>\n",
              "      <td>396</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.00</td>\n",
              "      <td>39.38</td>\n",
              "      <td>23.7151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61246</th>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>518.67</td>\n",
              "      <td>643.68</td>\n",
              "      <td>1607.72</td>\n",
              "      <td>1430.56</td>\n",
              "      <td>14.62</td>\n",
              "      <td>...</td>\n",
              "      <td>535.41</td>\n",
              "      <td>2388.48</td>\n",
              "      <td>8193.94</td>\n",
              "      <td>8.2525</td>\n",
              "      <td>0.03</td>\n",
              "      <td>395</td>\n",
              "      <td>2388</td>\n",
              "      <td>100.00</td>\n",
              "      <td>39.78</td>\n",
              "      <td>23.8270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61247</th>\n",
              "      <td>249</td>\n",
              "      <td>254</td>\n",
              "      <td>35.0046</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>449.44</td>\n",
              "      <td>555.77</td>\n",
              "      <td>1381.29</td>\n",
              "      <td>1148.18</td>\n",
              "      <td>5.48</td>\n",
              "      <td>...</td>\n",
              "      <td>187.92</td>\n",
              "      <td>2388.83</td>\n",
              "      <td>8125.64</td>\n",
              "      <td>9.0515</td>\n",
              "      <td>0.02</td>\n",
              "      <td>337</td>\n",
              "      <td>2223</td>\n",
              "      <td>100.00</td>\n",
              "      <td>15.26</td>\n",
              "      <td>9.0774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61248</th>\n",
              "      <td>249</td>\n",
              "      <td>255</td>\n",
              "      <td>42.0030</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.85</td>\n",
              "      <td>1369.75</td>\n",
              "      <td>1147.45</td>\n",
              "      <td>3.91</td>\n",
              "      <td>...</td>\n",
              "      <td>134.32</td>\n",
              "      <td>2388.66</td>\n",
              "      <td>8144.33</td>\n",
              "      <td>9.1207</td>\n",
              "      <td>0.02</td>\n",
              "      <td>333</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.66</td>\n",
              "      <td>6.4341</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61249 rows × 26 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "n7MBDuPasy-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set Transformation \n",
        "Test set has samples for all cycles, but has annotations only for last one"
      ],
      "metadata": {
        "id": "QinQ4hWStzHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wtvRNsfuUwg",
        "outputId": "a03616ae-8e88-4b19-cab0-bb1ff9c46043"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((41214, 26), (248, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_last = proc.transform_test_keep_setting(test)\n",
        "test_last.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "onw4pCwZy-1s",
        "outputId": "1ff0bc05-a076-45af-83da-55d0c7162af9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      op_1    op_2   op_3     s_0     s_1      s_2      s_3   s_4   s_5  \\\n",
              "0  25.0070  0.6214   60.0  462.54  537.66  1264.31  1046.41  7.05  8.99   \n",
              "1  41.9989  0.8400  100.0  445.00  549.96  1354.05  1133.55  3.91  5.72   \n",
              "2  42.0005  0.8401  100.0  445.00  549.47  1341.06  1118.90  3.91  5.69   \n",
              "3  25.0018  0.6207   60.0  462.54  536.06  1253.49  1038.53  7.05  9.00   \n",
              "4  25.0039  0.6200   60.0  462.54  537.36  1263.60  1052.52  7.05  9.03   \n",
              "\n",
              "      s_6  ...    s_11     s_12     s_13     s_14  s_15  s_16  s_17    s_18  \\\n",
              "0  176.56  ...  166.19  2028.53  7890.31  10.7615  0.02   308  1915   84.93   \n",
              "1  139.03  ...  130.17  2387.72  8073.44   9.3925  0.02   331  2212  100.00   \n",
              "2  139.26  ...  130.73  2388.18  8095.58   9.2974  0.02   330  2212  100.00   \n",
              "3  175.63  ...  164.91  2028.30  7878.63  10.8396  0.02   306  1915   84.93   \n",
              "4  175.53  ...  164.95  2028.24  7873.75  10.9094  0.02   307  1915   84.93   \n",
              "\n",
              "    s_19    s_20  \n",
              "0  14.41  8.6329  \n",
              "1  10.58  6.4325  \n",
              "2  10.61  6.3488  \n",
              "3  14.41  8.5696  \n",
              "4  14.19  8.6248  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>s_5</th>\n",
              "      <th>s_6</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0070</td>\n",
              "      <td>0.6214</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>537.66</td>\n",
              "      <td>1264.31</td>\n",
              "      <td>1046.41</td>\n",
              "      <td>7.05</td>\n",
              "      <td>8.99</td>\n",
              "      <td>176.56</td>\n",
              "      <td>...</td>\n",
              "      <td>166.19</td>\n",
              "      <td>2028.53</td>\n",
              "      <td>7890.31</td>\n",
              "      <td>10.7615</td>\n",
              "      <td>0.02</td>\n",
              "      <td>308</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.41</td>\n",
              "      <td>8.6329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>41.9989</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.96</td>\n",
              "      <td>1354.05</td>\n",
              "      <td>1133.55</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.72</td>\n",
              "      <td>139.03</td>\n",
              "      <td>...</td>\n",
              "      <td>130.17</td>\n",
              "      <td>2387.72</td>\n",
              "      <td>8073.44</td>\n",
              "      <td>9.3925</td>\n",
              "      <td>0.02</td>\n",
              "      <td>331</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.58</td>\n",
              "      <td>6.4325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0005</td>\n",
              "      <td>0.8401</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.47</td>\n",
              "      <td>1341.06</td>\n",
              "      <td>1118.90</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.69</td>\n",
              "      <td>139.26</td>\n",
              "      <td>...</td>\n",
              "      <td>130.73</td>\n",
              "      <td>2388.18</td>\n",
              "      <td>8095.58</td>\n",
              "      <td>9.2974</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.61</td>\n",
              "      <td>6.3488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0018</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.06</td>\n",
              "      <td>1253.49</td>\n",
              "      <td>1038.53</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.00</td>\n",
              "      <td>175.63</td>\n",
              "      <td>...</td>\n",
              "      <td>164.91</td>\n",
              "      <td>2028.30</td>\n",
              "      <td>7878.63</td>\n",
              "      <td>10.8396</td>\n",
              "      <td>0.02</td>\n",
              "      <td>306</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.41</td>\n",
              "      <td>8.5696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0039</td>\n",
              "      <td>0.6200</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>537.36</td>\n",
              "      <td>1263.60</td>\n",
              "      <td>1052.52</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.03</td>\n",
              "      <td>175.53</td>\n",
              "      <td>...</td>\n",
              "      <td>164.95</td>\n",
              "      <td>2028.24</td>\n",
              "      <td>7873.75</td>\n",
              "      <td>10.9094</td>\n",
              "      <td>0.02</td>\n",
              "      <td>307</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.19</td>\n",
              "      <td>8.6248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_last"
      ],
      "metadata": {
        "id": "ar3xxOQvIbHW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remaining Useful Life (RUL)"
      ],
      "metadata": {
        "id": "boZqFQNlraCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = proc.add_remaining_useful_life_linear(train)\n",
        "train[index_cols+['RUL']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "lmFKjQaeip1b",
        "outputId": "0f8c094f-fb8b-4f03-bf1e-b67d58b3c199"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   unit_number  time  RUL\n",
              "0            1     1  320\n",
              "1            1     2  319\n",
              "2            1     3  318\n",
              "3            1     4  317\n",
              "4            1     5  316"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unit_number</th>\n",
              "      <th>time</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attributes and target separation"
      ],
      "metadata": {
        "id": "IIXnBTkfxpCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = proc.X_y_train_divide_with_settings(train)"
      ],
      "metadata": {
        "id": "4SzUk6ZLxv6H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "fuAnHn4GxzwM",
        "outputId": "98891529-773b-49aa-c7bb-2dae6b0c9534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   RUL\n",
              "0  320\n",
              "1  319\n",
              "2  318\n",
              "3  317\n",
              "4  316"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "26hK4VWkx1R7",
        "outputId": "c627b069-dcc0-47e6-d4e2-bb6a6570daf6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      op_1    op_2   op_3     s_0     s_1      s_2      s_3   s_4    s_5  \\\n",
              "0  42.0049  0.8400  100.0  445.00  549.68  1343.43  1112.93  3.91   5.70   \n",
              "1  20.0020  0.7002  100.0  491.19  606.07  1477.61  1237.50  9.35  13.61   \n",
              "2  42.0038  0.8409  100.0  445.00  548.95  1343.12  1117.05  3.91   5.69   \n",
              "3  42.0000  0.8400  100.0  445.00  548.70  1341.24  1118.03  3.91   5.70   \n",
              "4  25.0063  0.6207   60.0  462.54  536.10  1255.23  1033.59  7.05   9.00   \n",
              "\n",
              "      s_6  ...    s_11     s_12     s_13     s_14  s_15  s_16  s_17    s_18  \\\n",
              "0  137.36  ...  129.78  2387.99  8074.83   9.3335  0.02   330  2212  100.00   \n",
              "1  332.10  ...  312.59  2387.73  8046.13   9.1913  0.02   361  2324  100.00   \n",
              "2  138.18  ...  129.62  2387.97  8066.62   9.4007  0.02   329  2212  100.00   \n",
              "3  137.98  ...  129.80  2388.02  8076.05   9.3369  0.02   328  2212  100.00   \n",
              "4  174.82  ...  164.11  2028.08  7865.80  10.8366  0.02   305  1915   84.93   \n",
              "\n",
              "    s_19     s_20  \n",
              "0  10.62   6.3670  \n",
              "1  24.37  14.6552  \n",
              "2  10.48   6.4213  \n",
              "3  10.54   6.4176  \n",
              "4  14.03   8.6754  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>op_1</th>\n",
              "      <th>op_2</th>\n",
              "      <th>op_3</th>\n",
              "      <th>s_0</th>\n",
              "      <th>s_1</th>\n",
              "      <th>s_2</th>\n",
              "      <th>s_3</th>\n",
              "      <th>s_4</th>\n",
              "      <th>s_5</th>\n",
              "      <th>s_6</th>\n",
              "      <th>...</th>\n",
              "      <th>s_11</th>\n",
              "      <th>s_12</th>\n",
              "      <th>s_13</th>\n",
              "      <th>s_14</th>\n",
              "      <th>s_15</th>\n",
              "      <th>s_16</th>\n",
              "      <th>s_17</th>\n",
              "      <th>s_18</th>\n",
              "      <th>s_19</th>\n",
              "      <th>s_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42.0049</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>549.68</td>\n",
              "      <td>1343.43</td>\n",
              "      <td>1112.93</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.70</td>\n",
              "      <td>137.36</td>\n",
              "      <td>...</td>\n",
              "      <td>129.78</td>\n",
              "      <td>2387.99</td>\n",
              "      <td>8074.83</td>\n",
              "      <td>9.3335</td>\n",
              "      <td>0.02</td>\n",
              "      <td>330</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.62</td>\n",
              "      <td>6.3670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.0020</td>\n",
              "      <td>0.7002</td>\n",
              "      <td>100.0</td>\n",
              "      <td>491.19</td>\n",
              "      <td>606.07</td>\n",
              "      <td>1477.61</td>\n",
              "      <td>1237.50</td>\n",
              "      <td>9.35</td>\n",
              "      <td>13.61</td>\n",
              "      <td>332.10</td>\n",
              "      <td>...</td>\n",
              "      <td>312.59</td>\n",
              "      <td>2387.73</td>\n",
              "      <td>8046.13</td>\n",
              "      <td>9.1913</td>\n",
              "      <td>0.02</td>\n",
              "      <td>361</td>\n",
              "      <td>2324</td>\n",
              "      <td>100.00</td>\n",
              "      <td>24.37</td>\n",
              "      <td>14.6552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0038</td>\n",
              "      <td>0.8409</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.95</td>\n",
              "      <td>1343.12</td>\n",
              "      <td>1117.05</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.69</td>\n",
              "      <td>138.18</td>\n",
              "      <td>...</td>\n",
              "      <td>129.62</td>\n",
              "      <td>2387.97</td>\n",
              "      <td>8066.62</td>\n",
              "      <td>9.4007</td>\n",
              "      <td>0.02</td>\n",
              "      <td>329</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>6.4213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42.0000</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>100.0</td>\n",
              "      <td>445.00</td>\n",
              "      <td>548.70</td>\n",
              "      <td>1341.24</td>\n",
              "      <td>1118.03</td>\n",
              "      <td>3.91</td>\n",
              "      <td>5.70</td>\n",
              "      <td>137.98</td>\n",
              "      <td>...</td>\n",
              "      <td>129.80</td>\n",
              "      <td>2388.02</td>\n",
              "      <td>8076.05</td>\n",
              "      <td>9.3369</td>\n",
              "      <td>0.02</td>\n",
              "      <td>328</td>\n",
              "      <td>2212</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.54</td>\n",
              "      <td>6.4176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0063</td>\n",
              "      <td>0.6207</td>\n",
              "      <td>60.0</td>\n",
              "      <td>462.54</td>\n",
              "      <td>536.10</td>\n",
              "      <td>1255.23</td>\n",
              "      <td>1033.59</td>\n",
              "      <td>7.05</td>\n",
              "      <td>9.00</td>\n",
              "      <td>174.82</td>\n",
              "      <td>...</td>\n",
              "      <td>164.11</td>\n",
              "      <td>2028.08</td>\n",
              "      <td>7865.80</td>\n",
              "      <td>10.8366</td>\n",
              "      <td>0.02</td>\n",
              "      <td>305</td>\n",
              "      <td>1915</td>\n",
              "      <td>84.93</td>\n",
              "      <td>14.03</td>\n",
              "      <td>8.6754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation functions"
      ],
      "metadata": {
        "id": "fQA-YtFMM81M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval = Evaluation()"
      ],
      "metadata": {
        "id": "u88P6scrNTRu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = HyperparameterSearch()"
      ],
      "metadata": {
        "id": "OUcS61OqPFhp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Construction"
      ],
      "metadata": {
        "id": "SL1dv6EX4NUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "PA_LrxmV4NUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping Callback\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
        "                                      patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "BV6PD9sl4NUw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Callback\n",
        "def printLog(epoch, logs):\n",
        "    print(\n",
        "        f\"E {epoch+1}\\t: loss={logs['loss']:.3f}, \"+\n",
        "        f\"rmse={logs['root_mean_squared_error']:.3f}, \"+\n",
        "        f\"r2={logs['r_square']:.3f}; \"+\n",
        "        f\"v_loss={logs['val_loss']:.3f}, \"+\n",
        "        f\"v_rmse={logs['val_root_mean_squared_error']:.3f}, \"+\n",
        "        f\"v_r2={logs['val_r_square']:.3f}; \"\n",
        "    )\n",
        "\n",
        "printerCallback = LambdaCallback(on_epoch_end=printLog)"
      ],
      "metadata": {
        "id": "83kJj9eJ4NU1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "DvHTMj_9_xss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train must include indices\n",
        "train3 = train.copy()\n",
        "X_train_ = train3.drop(columns=[index_cols[1]]+[\"RUL\"])"
      ],
      "metadata": {
        "id": "MmsURACM-Wkq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = 0"
      ],
      "metadata": {
        "id": "xKy2t3QS8gss"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper"
      ],
      "metadata": {
        "id": "9mjReYMmM08s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import is_finalizing\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "class MLPWrapperRegressor(BaseEstimator,RegressorMixin):\n",
        "    def __init__(self, basemodel=None, clip_y=-1,\n",
        "                 include_settings=False, poly_degree=1,\n",
        "                 scaler=StandardScaler()):\n",
        "        # Base parameters\n",
        "        self.basemodel = basemodel\n",
        "        self.clip_y = clip_y\n",
        "        self.poly_degree = poly_degree\n",
        "        self.include_settings = include_settings\n",
        "\n",
        "        # Column indexers\n",
        "        self.feature_cols = sensors_cols\n",
        "        if(include_settings):\n",
        "            self.feature_cols = settings_cols + self.feature_cols\n",
        "        self.base_feature_cols = self.feature_cols\n",
        "\n",
        "        # Scaler and PolyFeatures transformers\n",
        "        self.scaler = scaler\n",
        "        self.polyft = PolynomialFeatures(degree=self.poly_degree, \n",
        "                                         include_bias=False)\n",
        "\n",
        "    def clean_cols(self,df):\n",
        "        if(\"unit_number\" in df.columns): del df[\"unit_number\"]\n",
        "        if(\"unit_number\" in df.columns): del df[\"unit_number\"]\n",
        "        if(\"time\" in df.columns): del df[\"time\"]\n",
        "        if((not self.include_settings)): \n",
        "            for col in settings_cols:\n",
        "                if(col in df.columns): del df[col]\n",
        "        return df\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        # Drop irrelevant column\n",
        "        data = pd.DataFrame(X).copy()\n",
        "        data = self.clean_cols(data)\n",
        "        \n",
        "        # Apply polynomial features and add them to the dataframe\n",
        "        transf = self.polyft.fit_transform(data)\n",
        "        data = pd.DataFrame(transf, columns=\n",
        "                              self.polyft.get_feature_names_out())\n",
        "        self.feature_cols = list(self.polyft.get_feature_names_out())\n",
        "\n",
        "        # Scale the data\n",
        "        data = self.scaler.fit_transform(data)\n",
        "        data = pd.DataFrame(data, \n",
        "                            columns=self.scaler.get_feature_names_out())\n",
        "        X_train = data.copy()\n",
        "        \n",
        "        # Clip and transform labels\n",
        "        data2 = pd.DataFrame(y, columns=[\"RUL\"]).copy()\n",
        "        if (self.clip_y > 0):\n",
        "            data2[\"RUL\"].clip(upper=self.clip_y, inplace=True)\n",
        "        y_train = data2\n",
        "\n",
        "        # Update input shape for future use\n",
        "        global INPUT_SHAPE\n",
        "        INPUT_SHAPE = X_train.shape[1]\n",
        "\n",
        "        # Fit model\n",
        "        self.basemodel.fit(X_train,y_train)\n",
        "        return self\n",
        "        \n",
        "    def predict(self, X=None):\n",
        "        # Perform transformation, if not done\n",
        "        if (len(X.columns) != len(self.feature_cols)):\n",
        "            X_train = self.transform_features(X)\n",
        "        else:\n",
        "            X_train = X\n",
        "        return self.basemodel.predict(X_train)\n",
        "\n",
        "    def transform_features(self, df):\n",
        "        # Drop irrelevant column\n",
        "        data = pd.DataFrame(df).copy()\n",
        "        data = self.clean_cols(data)\n",
        "        \n",
        "        # Apply polynomial features and add them to the dataframe\n",
        "        transf = self.polyft.transform(data)\n",
        "        data = pd.DataFrame(transf, columns=\n",
        "                              self.polyft.get_feature_names_out())\n",
        "        # self.feature_cols = list(self.polyft.get_feature_names_out())\n",
        "\n",
        "        # Scale the data\n",
        "        data = self.scaler.transform(data)\n",
        "        data = pd.DataFrame(data, \n",
        "                            columns=self.scaler.get_feature_names_out())\n",
        "        return data\n",
        "\n",
        "    def score(self, X, y, sample_weight=None):\n",
        "        X_test = self.transform_features(X)\n",
        "\n",
        "        # Clip and transform labels\n",
        "        data2 = pd.DataFrame(y, columns=[\"RUL\"]).copy()\n",
        "        if (self.clip_y > 0):\n",
        "            data2[\"RUL\"].clip(upper=self.clip_y, inplace=True)\n",
        "        y_test = data2\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = self.predict(X_test)\n",
        "        return r2_score(y_test, y_pred, sample_weight=sample_weight)"
      ],
      "metadata": {
        "id": "-mG7sVkcpALn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "Z7Z5u9Bu_Q4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_test(test,model):\n",
        "    test2 = pd.DataFrame(test)\n",
        "    test2 = model.clean_cols(test2)\n",
        "\n",
        "    # Apply polynomial features\n",
        "    transf = model.polyft.transform(test2)\n",
        "    test2 = pd.DataFrame(transf, \n",
        "                         columns=model.polyft.get_feature_names_out())\n",
        "\n",
        "    # Scale the data (with train data parameters)\n",
        "    test2 = model.scaler.transform(test2)\n",
        "    test2 = pd.DataFrame(test2, \n",
        "                         columns=model.polyft.get_feature_names_out())\n",
        "    return test2"
      ],
      "metadata": {
        "id": "OFW6XKjG5kCB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructor"
      ],
      "metadata": {
        "id": "Ha2fY8VlWD7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(optim=Adam, learning_rate=1e-3, \n",
        "                 layer1=32  , activation1=\"tanh\"    , dropout1=0.1,\n",
        "                 layer2=None, activation2=\"tanh\"    , dropout2=0.1,\n",
        "                 layer3=None, activation3=\"tanh\"    , dropout3=0.1,\n",
        "                 layer4=None, activation4=\"tanh\"    , dropout4=0.1,\n",
        "                 print_summary=False, loss='mean_squared_error',\n",
        "                 metrics=[tf.keras.metrics.MeanSquaredError()]):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(Dense(layer1, input_dim=INPUT_SHAPE, activation=activation1))\n",
        "    model.add(Dropout(dropout1))\n",
        "\n",
        "    # Additional hidden layers\n",
        "    if(layer2 is not None):\n",
        "        model.add(Dense(layer2, activation=activation2))\n",
        "        model.add(Dropout(dropout2))\n",
        "        if (layer3 is not None):\n",
        "            model.add(Dense(layer3, activation=activation3))\n",
        "            model.add(Dropout(dropout3))\n",
        "            if (layer4 is not None):\n",
        "                model.add(Dense(layer4, activation=activation4))\n",
        "                model.add(Dropout(dropout4))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optim(learning_rate=learning_rate), \n",
        "                  metrics=metrics)\n",
        "    \n",
        "    if(print_summary): model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "NbakKD-DlU5-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP-1"
      ],
      "metadata": {
        "id": "DU8TxguXIChd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL "
      ],
      "metadata": {
        "id": "zkCJJsiS-J7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.540842574601971  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 275),\n",
        "('basemodel__epochs', 37),\n",
        "('basemodel__model__activation1', 'relu'),\n",
        "('basemodel__model__dropout1', 0.2142001322493293),\n",
        "('basemodel__model__layer1', 512),\n",
        "('basemodel__model__learning_rate', 0.007899917433901438),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.1),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "iWX0RiAL4uGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=275,\n",
        "                           epochs=37,\n",
        "                           model__activation1='relu',\n",
        "                           model__dropout1=0.2142001322493293, \n",
        "                           model__layer1=512, \n",
        "                           model__learning_rate=0.007899917433901438,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.1, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "id": "xfYRKHQKi3Xv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5128cae4-7a1f-4b9a-c534-56ccd94bf44a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=275, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=37, model=<function create_model at 0x00000281C1861DC0>, model__activation1='relu', model__dropout1=0.2142001322493293, model__layer1=512, model__learning_rate=0.007899917433901438, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000281BF13FC70>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000281C1996610>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.1, verbose=0),\n",
              "                    include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "SF4CE-ab5IKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9baca29-571d-4b4a-b318-4e363edb6e19"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               12800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,313\n",
            "Trainable params: 13,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=9231.953, rmse=96.083, r2=-0.127; v_loss=5201.883, v_rmse=72.124, v_r2=0.244; \n",
            "E 2\t: loss=5315.319, rmse=72.906, r2=0.351; v_loss=3927.683, v_rmse=62.671, v_r2=0.429; \n",
            "E 3\t: loss=4330.736, rmse=65.808, r2=0.471; v_loss=3549.014, v_rmse=59.574, v_r2=0.484; \n",
            "E 4\t: loss=4053.428, rmse=63.667, r2=0.505; v_loss=3784.502, v_rmse=61.518, v_r2=0.450; \n",
            "E 5\t: loss=3977.614, rmse=63.068, r2=0.514; v_loss=3534.938, v_rmse=59.455, v_r2=0.486; \n",
            "E 6\t: loss=3938.952, rmse=62.761, r2=0.519; v_loss=3509.623, v_rmse=59.242, v_r2=0.490; \n",
            "E 7\t: loss=3897.622, rmse=62.431, r2=0.524; v_loss=3515.960, v_rmse=59.296, v_r2=0.489; \n",
            "E 8\t: loss=3859.272, rmse=62.123, r2=0.529; v_loss=3515.847, v_rmse=59.295, v_r2=0.489; \n",
            "E 9\t: loss=3887.610, rmse=62.351, r2=0.525; v_loss=3372.925, v_rmse=58.077, v_r2=0.510; \n",
            "E 10\t: loss=3849.015, rmse=62.040, r2=0.530; v_loss=3220.269, v_rmse=56.747, v_r2=0.532; \n",
            "E 11\t: loss=3830.848, rmse=61.894, r2=0.532; v_loss=3392.298, v_rmse=58.243, v_r2=0.507; \n",
            "E 12\t: loss=3833.659, rmse=61.917, r2=0.532; v_loss=3346.358, v_rmse=57.848, v_r2=0.513; \n",
            "E 13\t: loss=3817.875, rmse=61.789, r2=0.534; v_loss=3420.147, v_rmse=58.482, v_r2=0.503; \n",
            "E 14\t: loss=3808.346, rmse=61.712, r2=0.535; v_loss=3417.675, v_rmse=58.461, v_r2=0.503; \n",
            "E 15\t: loss=3812.239, rmse=61.743, r2=0.535; v_loss=3490.819, v_rmse=59.083, v_r2=0.492; \n",
            "E 16\t: loss=3779.288, rmse=61.476, r2=0.539; v_loss=3384.657, v_rmse=58.178, v_r2=0.508; \n",
            "E 17\t: loss=3771.634, rmse=61.414, r2=0.540; v_loss=3474.314, v_rmse=58.943, v_r2=0.495; \n",
            "E 18\t: loss=3807.808, rmse=61.707, r2=0.535; v_loss=3272.837, v_rmse=57.209, v_r2=0.524; \n",
            "E 19\t: loss=3786.087, rmse=61.531, r2=0.538; v_loss=3403.258, v_rmse=58.337, v_r2=0.505; \n",
            "E 20\t: loss=3773.722, rmse=61.431, r2=0.539; v_loss=3347.330, v_rmse=57.856, v_r2=0.513; \n",
            "E 21\t: loss=3772.615, rmse=61.422, r2=0.539; v_loss=3354.287, v_rmse=57.916, v_r2=0.512; \n",
            "E 22\t: loss=3752.935, rmse=61.261, r2=0.542; v_loss=3226.932, v_rmse=56.806, v_r2=0.531; \n",
            "E 23\t: loss=3769.092, rmse=61.393, r2=0.540; v_loss=3389.913, v_rmse=58.223, v_r2=0.507; \n",
            "E 24\t: loss=3793.171, rmse=61.589, r2=0.537; v_loss=3286.302, v_rmse=57.326, v_r2=0.522; \n",
            "E 25\t: loss=3762.944, rmse=61.343, r2=0.541; v_loss=3266.461, v_rmse=57.153, v_r2=0.525; \n",
            "E 26\t: loss=3750.911, rmse=61.245, r2=0.542; v_loss=3234.355, v_rmse=56.871, v_r2=0.530; \n",
            "E 27\t: loss=3752.785, rmse=61.260, r2=0.542; v_loss=3377.527, v_rmse=58.116, v_r2=0.509; \n",
            "E 28\t: loss=3739.568, rmse=61.152, r2=0.543; v_loss=3257.395, v_rmse=57.074, v_r2=0.526; \n",
            "E 29\t: loss=3743.850, rmse=61.187, r2=0.543; v_loss=3229.893, v_rmse=56.832, v_r2=0.530; \n",
            "E 30\t: loss=3742.169, rmse=61.173, r2=0.543; v_loss=3456.000, v_rmse=58.788, v_r2=0.497; \n",
            "E 31\t: loss=3740.302, rmse=61.158, r2=0.543; v_loss=3243.602, v_rmse=56.953, v_r2=0.528; \n",
            "E 32\t: loss=3729.128, rmse=61.067, r2=0.545; v_loss=3416.967, v_rmse=58.455, v_r2=0.503; \n",
            "E 33\t: loss=3743.381, rmse=61.183, r2=0.543; v_loss=3428.277, v_rmse=58.551, v_r2=0.501; \n",
            "E 34\t: loss=3749.464, rmse=61.233, r2=0.542; v_loss=3213.803, v_rmse=56.690, v_r2=0.533; \n",
            "E 35\t: loss=3729.193, rmse=61.067, r2=0.545; v_loss=3294.888, v_rmse=57.401, v_r2=0.521; \n",
            "E 36\t: loss=3763.334, rmse=61.346, r2=0.541; v_loss=3342.993, v_rmse=57.819, v_r2=0.514; \n",
            "E 37\t: loss=3741.511, rmse=61.168, r2=0.543; v_loss=3232.804, v_rmse=56.858, v_r2=0.530; \n",
            "Finished: 2022-11-03 08:35:57.761936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "9vLfPZkw5Ixg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92761cb-3df4-4da9-d900-9a481053a897"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.154,RMSE=-50.149\n",
            "Finished: 2022-11-03 08:35:57.871934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL"
      ],
      "metadata": {
        "id": "mMYPm8b65n0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.8250779110568915  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 32),\n",
        "('basemodel__epochs', 49),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__dropout1', 0.1),\n",
        "('basemodel__model__layer1', 512),\n",
        "('basemodel__model__learning_rate', 0.01),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__validation_split', 0.14787301913541884),\n",
        "('clip_y', 80),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "uooUt5Yq5n0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=80\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=32,\n",
        "                           epochs=49,\n",
        "                           model__activation1='tanh',\n",
        "                           model__dropout1=0.1, \n",
        "                           model__layer1=512, \n",
        "                           model__learning_rate=0.01,\n",
        "                           model__optim=RMSprop,\n",
        "                           validation_split=0.14787301913541884, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43ca5e2-e2d8-4d31-dcca-b6dc9e5590c8",
        "id": "Lj8fIDg15n0X"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=32, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=49, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__dropout1=0.1, model__layer1=512, model__learning_rate=0.01, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000281D7DB2550>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000281BF079EB0>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, print_summary=True, validation_split=0.14787301913541884, verbose=0),\n",
              "                    clip_y=80, include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "y1uxSqWy5n0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd44a2f-bcd5-4a52-873d-8a343209a244"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 512)               12800     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,313\n",
            "Trainable params: 13,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=338.300, rmse=18.393, r2=0.363; v_loss=184.123, v_rmse=13.569, v_r2=0.662; \n",
            "E 2\t: loss=204.642, rmse=14.305, r2=0.615; v_loss=140.924, v_rmse=11.871, v_r2=0.741; \n",
            "E 3\t: loss=172.777, rmse=13.144, r2=0.675; v_loss=126.212, v_rmse=11.234, v_r2=0.768; \n",
            "E 4\t: loss=156.098, rmse=12.494, r2=0.706; v_loss=111.478, v_rmse=10.558, v_r2=0.795; \n",
            "E 5\t: loss=144.817, rmse=12.034, r2=0.727; v_loss=108.172, v_rmse=10.401, v_r2=0.801; \n",
            "E 6\t: loss=138.471, rmse=11.767, r2=0.739; v_loss=113.786, v_rmse=10.667, v_r2=0.791; \n",
            "E 7\t: loss=133.779, rmse=11.566, r2=0.748; v_loss=140.828, v_rmse=11.867, v_r2=0.741; \n",
            "E 8\t: loss=128.791, rmse=11.349, r2=0.758; v_loss=100.818, v_rmse=10.041, v_r2=0.815; \n",
            "E 9\t: loss=124.709, rmse=11.167, r2=0.765; v_loss=95.392, v_rmse=9.767, v_r2=0.825; \n",
            "E 10\t: loss=122.827, rmse=11.083, r2=0.769; v_loss=93.892, v_rmse=9.690, v_r2=0.828; \n",
            "E 11\t: loss=119.896, rmse=10.950, r2=0.774; v_loss=127.739, v_rmse=11.302, v_r2=0.765; \n",
            "E 12\t: loss=118.099, rmse=10.867, r2=0.778; v_loss=125.397, v_rmse=11.198, v_r2=0.770; \n",
            "E 13\t: loss=116.689, rmse=10.802, r2=0.780; v_loss=90.416, v_rmse=9.509, v_r2=0.834; \n",
            "E 14\t: loss=115.134, rmse=10.730, r2=0.783; v_loss=79.070, v_rmse=8.892, v_r2=0.855; \n",
            "E 15\t: loss=113.857, rmse=10.670, r2=0.786; v_loss=92.847, v_rmse=9.636, v_r2=0.830; \n",
            "E 16\t: loss=113.080, rmse=10.634, r2=0.787; v_loss=83.075, v_rmse=9.115, v_r2=0.847; \n",
            "E 17\t: loss=111.889, rmse=10.578, r2=0.789; v_loss=104.103, v_rmse=10.203, v_r2=0.809; \n",
            "E 18\t: loss=110.668, rmse=10.520, r2=0.792; v_loss=86.243, v_rmse=9.287, v_r2=0.842; \n",
            "E 19\t: loss=110.635, rmse=10.518, r2=0.792; v_loss=88.049, v_rmse=9.383, v_r2=0.838; \n",
            "E 20\t: loss=109.147, rmse=10.447, r2=0.795; v_loss=91.343, v_rmse=9.557, v_r2=0.832; \n",
            "E 21\t: loss=109.468, rmse=10.463, r2=0.794; v_loss=86.900, v_rmse=9.322, v_r2=0.840; \n",
            "E 22\t: loss=109.047, rmse=10.443, r2=0.795; v_loss=79.674, v_rmse=8.926, v_r2=0.854; \n",
            "E 23\t: loss=108.230, rmse=10.403, r2=0.796; v_loss=73.940, v_rmse=8.599, v_r2=0.864; \n",
            "E 24\t: loss=107.575, rmse=10.372, r2=0.798; v_loss=76.688, v_rmse=8.757, v_r2=0.859; \n",
            "E 25\t: loss=108.058, rmse=10.395, r2=0.797; v_loss=77.256, v_rmse=8.790, v_r2=0.858; \n",
            "E 26\t: loss=107.224, rmse=10.355, r2=0.798; v_loss=74.102, v_rmse=8.608, v_r2=0.864; \n",
            "E 27\t: loss=105.913, rmse=10.291, r2=0.801; v_loss=76.305, v_rmse=8.735, v_r2=0.860; \n",
            "E 28\t: loss=106.072, rmse=10.299, r2=0.800; v_loss=77.475, v_rmse=8.802, v_r2=0.858; \n",
            "E 29\t: loss=105.097, rmse=10.252, r2=0.802; v_loss=105.722, v_rmse=10.282, v_r2=0.806; \n",
            "E 30\t: loss=105.745, rmse=10.283, r2=0.801; v_loss=77.544, v_rmse=8.806, v_r2=0.858; \n",
            "E 31\t: loss=105.273, rmse=10.260, r2=0.802; v_loss=84.258, v_rmse=9.179, v_r2=0.845; \n",
            "E 32\t: loss=104.688, rmse=10.232, r2=0.803; v_loss=80.283, v_rmse=8.960, v_r2=0.853; \n",
            "E 33\t: loss=104.949, rmse=10.244, r2=0.802; v_loss=95.138, v_rmse=9.754, v_r2=0.825; \n",
            "E 34\t: loss=103.765, rmse=10.187, r2=0.805; v_loss=74.061, v_rmse=8.606, v_r2=0.864; \n",
            "E 35\t: loss=103.922, rmse=10.194, r2=0.804; v_loss=73.221, v_rmse=8.557, v_r2=0.866; \n",
            "E 36\t: loss=104.318, rmse=10.214, r2=0.804; v_loss=76.674, v_rmse=8.756, v_r2=0.859; \n",
            "E 37\t: loss=103.381, rmse=10.168, r2=0.805; v_loss=74.907, v_rmse=8.655, v_r2=0.862; \n",
            "E 38\t: loss=103.303, rmse=10.164, r2=0.806; v_loss=94.437, v_rmse=9.718, v_r2=0.827; \n",
            "E 39\t: loss=102.951, rmse=10.146, r2=0.806; v_loss=76.050, v_rmse=8.721, v_r2=0.860; \n",
            "E 40\t: loss=103.721, rmse=10.184, r2=0.805; v_loss=76.697, v_rmse=8.758, v_r2=0.859; \n",
            "E 41\t: loss=103.251, rmse=10.161, r2=0.806; v_loss=91.337, v_rmse=9.557, v_r2=0.832; \n",
            "E 42\t: loss=102.793, rmse=10.139, r2=0.807; v_loss=79.827, v_rmse=8.935, v_r2=0.853; \n",
            "E 43\t: loss=102.747, rmse=10.136, r2=0.807; v_loss=74.413, v_rmse=8.626, v_r2=0.863; \n",
            "E 44\t: loss=102.496, rmse=10.124, r2=0.807; v_loss=71.945, v_rmse=8.482, v_r2=0.868; \n",
            "E 45\t: loss=101.981, rmse=10.099, r2=0.808; v_loss=82.231, v_rmse=9.068, v_r2=0.849; \n",
            "E 46\t: loss=102.509, rmse=10.125, r2=0.807; v_loss=82.357, v_rmse=9.075, v_r2=0.849; \n",
            "E 47\t: loss=101.948, rmse=10.097, r2=0.808; v_loss=77.221, v_rmse=8.788, v_r2=0.858; \n",
            "E 48\t: loss=102.374, rmse=10.118, r2=0.807; v_loss=73.556, v_rmse=8.576, v_r2=0.865; \n",
            "E 49\t: loss=101.703, rmse=10.085, r2=0.809; v_loss=74.195, v_rmse=8.614, v_r2=0.864; \n",
            "Finished: 2022-11-03 08:45:06.722869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "Q1waoIXF5n0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8c1a05-823c-494b-9225-62d093f09436"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.834,RMSE=-11.002\n",
            "Finished: 2022-11-03 08:45:06.828842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Linear RUL "
      ],
      "metadata": {
        "id": "X4OKIiMD6X4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Score: 0.5269773808976338  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 315),\n",
        "('basemodel__epochs', 38),\n",
        "('basemodel__model__activation1', 'relu'),\n",
        "('basemodel__model__dropout1', 0.38326321899773097),\n",
        "('basemodel__model__layer1', 350),\n",
        "('basemodel__model__learning_rate', 0.0024970992234534897),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__validation_split', 0.23038975772370296),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "OUMvs31w6X4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=315,\n",
        "                           epochs=38,\n",
        "                           model__activation1='relu',\n",
        "                           model__dropout1=0.38326321899773097, \n",
        "                           model__layer1=350, \n",
        "                           model__learning_rate=0.0024970992234534897,\n",
        "                           model__optim=RMSprop,\n",
        "                           validation_split=0.23038975772370296, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec93e6f8-3810-4040-ae36-c7b13accef07",
        "id": "dpKrE34E6X4j"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=315, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=38, model=<function create_model at 0x00000281C1861DC0>, model__activation1='relu', model__dropout1=0.38326321899773097, model__layer1=350, model__learning_rate=0.0024970992234534897, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F5643070>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F56432E0>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, print_summary=True, validation_split=0.23038975772370296, verbose=0),\n",
              "                    include_settings=True, poly_degree=2)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "HHlBlEju6X4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55075ac-e567-4fef-c0d3-c68e86237e0b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 350)               113750    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 350)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 351       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114,101\n",
            "Trainable params: 114,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=9158.690, rmse=95.701, r2=-0.094; v_loss=6166.658, v_rmse=78.528, v_r2=0.112; \n",
            "E 2\t: loss=6969.643, rmse=83.484, r2=0.168; v_loss=5068.129, v_rmse=71.191, v_r2=0.270; \n",
            "E 3\t: loss=6014.202, rmse=77.551, r2=0.282; v_loss=5139.114, v_rmse=71.688, v_r2=0.260; \n",
            "E 4\t: loss=5373.119, rmse=73.302, r2=0.358; v_loss=3577.996, v_rmse=59.816, v_r2=0.485; \n",
            "E 5\t: loss=5093.715, rmse=71.370, r2=0.392; v_loss=4127.597, v_rmse=64.246, v_r2=0.406; \n",
            "E 6\t: loss=4855.068, rmse=69.678, r2=0.420; v_loss=3495.146, v_rmse=59.120, v_r2=0.497; \n",
            "E 7\t: loss=4766.956, rmse=69.043, r2=0.431; v_loss=6314.488, v_rmse=79.464, v_r2=0.091; \n",
            "E 8\t: loss=4687.782, rmse=68.467, r2=0.440; v_loss=3772.068, v_rmse=61.417, v_r2=0.457; \n",
            "E 9\t: loss=4614.622, rmse=67.931, r2=0.449; v_loss=3755.399, v_rmse=61.281, v_r2=0.459; \n",
            "E 10\t: loss=4570.101, rmse=67.603, r2=0.454; v_loss=3365.727, v_rmse=58.015, v_r2=0.515; \n",
            "E 11\t: loss=4540.591, rmse=67.384, r2=0.458; v_loss=3361.627, v_rmse=57.980, v_r2=0.516; \n",
            "E 12\t: loss=4487.118, rmse=66.986, r2=0.464; v_loss=3288.861, v_rmse=57.349, v_r2=0.526; \n",
            "E 13\t: loss=4467.896, rmse=66.842, r2=0.467; v_loss=4351.793, v_rmse=65.968, v_r2=0.373; \n",
            "E 14\t: loss=4466.522, rmse=66.832, r2=0.467; v_loss=3201.582, v_rmse=56.583, v_r2=0.539; \n",
            "E 15\t: loss=4390.082, rmse=66.258, r2=0.476; v_loss=3644.431, v_rmse=60.369, v_r2=0.475; \n",
            "E 16\t: loss=4403.705, rmse=66.360, r2=0.474; v_loss=3695.823, v_rmse=60.793, v_r2=0.468; \n",
            "E 17\t: loss=4390.098, rmse=66.258, r2=0.476; v_loss=3179.154, v_rmse=56.384, v_r2=0.542; \n",
            "E 18\t: loss=4351.953, rmse=65.969, r2=0.480; v_loss=3373.837, v_rmse=58.085, v_r2=0.514; \n",
            "E 19\t: loss=4353.141, rmse=65.978, r2=0.480; v_loss=3629.022, v_rmse=60.241, v_r2=0.477; \n",
            "E 20\t: loss=4359.654, rmse=66.028, r2=0.479; v_loss=3317.173, v_rmse=57.595, v_r2=0.522; \n",
            "E 21\t: loss=4322.028, rmse=65.742, r2=0.484; v_loss=3186.777, v_rmse=56.452, v_r2=0.541; \n",
            "E 22\t: loss=4275.420, rmse=65.387, r2=0.490; v_loss=4265.272, v_rmse=65.309, v_r2=0.386; \n",
            "E 23\t: loss=4304.304, rmse=65.607, r2=0.486; v_loss=3308.088, v_rmse=57.516, v_r2=0.524; \n",
            "E 24\t: loss=4277.173, rmse=65.400, r2=0.489; v_loss=3114.110, v_rmse=55.804, v_r2=0.552; \n",
            "E 25\t: loss=4273.292, rmse=65.370, r2=0.490; v_loss=3076.178, v_rmse=55.463, v_r2=0.557; \n",
            "E 26\t: loss=4259.843, rmse=65.267, r2=0.491; v_loss=3368.795, v_rmse=58.041, v_r2=0.515; \n",
            "E 27\t: loss=4244.281, rmse=65.148, r2=0.493; v_loss=3674.918, v_rmse=60.621, v_r2=0.471; \n",
            "E 28\t: loss=4230.674, rmse=65.044, r2=0.495; v_loss=3782.503, v_rmse=61.502, v_r2=0.455; \n",
            "E 29\t: loss=4230.161, rmse=65.040, r2=0.495; v_loss=3077.947, v_rmse=55.479, v_r2=0.557; \n",
            "E 30\t: loss=4229.092, rmse=65.031, r2=0.495; v_loss=3051.408, v_rmse=55.240, v_r2=0.561; \n",
            "E 31\t: loss=4214.292, rmse=64.918, r2=0.497; v_loss=3335.838, v_rmse=57.757, v_r2=0.520; \n",
            "E 32\t: loss=4216.795, rmse=64.937, r2=0.497; v_loss=3113.506, v_rmse=55.799, v_r2=0.552; \n",
            "E 33\t: loss=4221.571, rmse=64.974, r2=0.496; v_loss=3278.271, v_rmse=57.256, v_r2=0.528; \n",
            "E 34\t: loss=4191.119, rmse=64.739, r2=0.500; v_loss=3028.852, v_rmse=55.035, v_r2=0.564; \n",
            "E 35\t: loss=4212.634, rmse=64.905, r2=0.497; v_loss=3079.003, v_rmse=55.489, v_r2=0.557; \n",
            "E 36\t: loss=4194.617, rmse=64.766, r2=0.499; v_loss=3047.948, v_rmse=55.208, v_r2=0.561; \n",
            "E 37\t: loss=4155.244, rmse=64.461, r2=0.504; v_loss=3052.111, v_rmse=55.246, v_r2=0.560; \n",
            "E 38\t: loss=4165.062, rmse=64.537, r2=0.503; v_loss=3007.052, v_rmse=54.837, v_r2=0.567; \n",
            "Finished: 2022-11-03 08:47:03.020815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "XrGQOj3e6X4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8590b31-f855-4884-b7ce-6c9109a4a36a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.320,RMSE=-44.954\n",
            "Finished: 2022-11-03 08:47:03.111816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Non-Linear RUL\n"
      ],
      "metadata": {
        "id": "xUq7j-FC7J6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.7360051567025246  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 318),\n",
        "('basemodel__epochs', 37),\n",
        "('basemodel__model__activation1', 'elu'),\n",
        "('basemodel__model__dropout1', 0.1),\n",
        "('basemodel__model__layer1', 31),\n",
        "('basemodel__model__learning_rate', 0.0064651627718996674),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.1),\n",
        "('clip_y', 93),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "XfyXyuQl7J65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=93\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=318,\n",
        "                           epochs=37,\n",
        "                           model__activation1='elu',\n",
        "                           model__dropout1=0.1, \n",
        "                           model__layer1=31, \n",
        "                           model__learning_rate=0.0064651627718996674,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.1, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92abfabb-3e12-46f9-d32a-6cdd9b1e709c",
        "id": "AxhumNq67J7A"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=318, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=37, model=<function create_model at 0x00000281C1861DC0>, model__activation1='elu', model__dropout1=0.1, model__layer1=31, model__learning_rate=0.0064651627718996674, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F5A48B20>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F580B460>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.1, verbose=0),\n",
              "                    clip_y=93, include_settings=True, poly_degree=2)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "3IOHJ_u87J7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0b71e0-0cb7-444f-8145-4206262843bf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 31)                10075     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 31)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 32        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,107\n",
            "Trainable params: 10,107\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=1210.043, rmse=34.786, r2=-0.528; v_loss=578.250, v_rmse=24.047, v_r2=0.272; \n",
            "E 2\t: loss=580.147, rmse=24.086, r2=0.267; v_loss=370.215, v_rmse=19.241, v_r2=0.534; \n",
            "E 3\t: loss=369.801, rmse=19.230, r2=0.533; v_loss=243.712, v_rmse=15.611, v_r2=0.693; \n",
            "E 4\t: loss=309.925, rmse=17.605, r2=0.609; v_loss=198.697, v_rmse=14.096, v_r2=0.750; \n",
            "E 5\t: loss=288.056, rmse=16.972, r2=0.636; v_loss=198.970, v_rmse=14.106, v_r2=0.749; \n",
            "E 6\t: loss=278.688, rmse=16.694, r2=0.648; v_loss=187.686, v_rmse=13.700, v_r2=0.764; \n",
            "E 7\t: loss=271.874, rmse=16.489, r2=0.657; v_loss=192.822, v_rmse=13.886, v_r2=0.757; \n",
            "E 8\t: loss=262.774, rmse=16.210, r2=0.668; v_loss=177.346, v_rmse=13.317, v_r2=0.777; \n",
            "E 9\t: loss=260.137, rmse=16.129, r2=0.671; v_loss=170.127, v_rmse=13.043, v_r2=0.786; \n",
            "E 10\t: loss=254.780, rmse=15.962, r2=0.678; v_loss=206.460, v_rmse=14.369, v_r2=0.740; \n",
            "E 11\t: loss=254.696, rmse=15.959, r2=0.678; v_loss=173.015, v_rmse=13.154, v_r2=0.782; \n",
            "E 12\t: loss=244.447, rmse=15.635, r2=0.691; v_loss=167.860, v_rmse=12.956, v_r2=0.789; \n",
            "E 13\t: loss=240.727, rmse=15.515, r2=0.696; v_loss=163.097, v_rmse=12.771, v_r2=0.795; \n",
            "E 14\t: loss=242.266, rmse=15.565, r2=0.694; v_loss=154.306, v_rmse=12.422, v_r2=0.806; \n",
            "E 15\t: loss=230.080, rmse=15.168, r2=0.709; v_loss=164.403, v_rmse=12.822, v_r2=0.793; \n",
            "E 16\t: loss=232.867, rmse=15.260, r2=0.706; v_loss=162.924, v_rmse=12.764, v_r2=0.795; \n",
            "E 17\t: loss=223.628, rmse=14.954, r2=0.718; v_loss=162.411, v_rmse=12.744, v_r2=0.795; \n",
            "E 18\t: loss=229.663, rmse=15.155, r2=0.710; v_loss=156.721, v_rmse=12.519, v_r2=0.803; \n",
            "E 19\t: loss=223.146, rmse=14.938, r2=0.718; v_loss=167.346, v_rmse=12.936, v_r2=0.789; \n",
            "E 20\t: loss=218.235, rmse=14.773, r2=0.724; v_loss=150.324, v_rmse=12.261, v_r2=0.811; \n",
            "E 21\t: loss=219.250, rmse=14.807, r2=0.723; v_loss=163.816, v_rmse=12.799, v_r2=0.794; \n",
            "E 22\t: loss=216.417, rmse=14.711, r2=0.727; v_loss=149.562, v_rmse=12.230, v_r2=0.812; \n",
            "E 23\t: loss=211.270, rmse=14.535, r2=0.733; v_loss=151.967, v_rmse=12.327, v_r2=0.809; \n",
            "E 24\t: loss=212.611, rmse=14.581, r2=0.731; v_loss=154.986, v_rmse=12.449, v_r2=0.805; \n",
            "E 25\t: loss=209.531, rmse=14.475, r2=0.735; v_loss=156.118, v_rmse=12.495, v_r2=0.803; \n",
            "E 26\t: loss=210.523, rmse=14.509, r2=0.734; v_loss=153.221, v_rmse=12.378, v_r2=0.807; \n",
            "E 27\t: loss=207.772, rmse=14.414, r2=0.738; v_loss=159.732, v_rmse=12.639, v_r2=0.799; \n",
            "E 28\t: loss=206.743, rmse=14.379, r2=0.739; v_loss=151.641, v_rmse=12.314, v_r2=0.809; \n",
            "E 29\t: loss=209.531, rmse=14.475, r2=0.735; v_loss=152.075, v_rmse=12.332, v_r2=0.808; \n",
            "E 30\t: loss=204.336, rmse=14.295, r2=0.742; v_loss=169.978, v_rmse=13.038, v_r2=0.786; \n",
            "E 31\t: loss=204.148, rmse=14.288, r2=0.742; v_loss=168.221, v_rmse=12.970, v_r2=0.788; \n",
            "E 32\t: loss=205.406, rmse=14.332, r2=0.741; v_loss=155.894, v_rmse=12.486, v_r2=0.804; \n",
            "E 33\t: loss=203.522, rmse=14.266, r2=0.743; v_loss=150.144, v_rmse=12.253, v_r2=0.811; \n",
            "E 34\t: loss=200.723, rmse=14.168, r2=0.746; v_loss=148.339, v_rmse=12.179, v_r2=0.813; \n",
            "E 35\t: loss=199.135, rmse=14.112, r2=0.748; v_loss=148.187, v_rmse=12.173, v_r2=0.813; \n",
            "E 36\t: loss=200.735, rmse=14.168, r2=0.746; v_loss=152.038, v_rmse=12.330, v_r2=0.808; \n",
            "E 37\t: loss=198.822, rmse=14.100, r2=0.749; v_loss=150.124, v_rmse=12.253, v_r2=0.811; \n",
            "Finished: 2022-11-03 08:48:40.205557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "bQgkkrV47J7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb03415-4e93-4ad9-c2ec-6ea39da7734a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.771,RMSE=-15.395\n",
            "Finished: 2022-11-03 08:48:40.293557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP-2 "
      ],
      "metadata": {
        "id": "FDHk5EzS8XuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "w3w40nFu8Xug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.5434403412387998  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 475),\n",
        "('basemodel__epochs', 31),\n",
        "('basemodel__model__activation1', 'selu'),\n",
        "('basemodel__model__activation2', 'sigmoid'),\n",
        "('basemodel__model__dropout1', 0.1551853147879086),\n",
        "('basemodel__model__dropout2', 0.34328552130273915),\n",
        "('basemodel__model__layer1', 115),\n",
        "('basemodel__model__layer2', 512),\n",
        "('basemodel__model__learning_rate', 0.006720220722226725),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.29870359291003556),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "Bt8LxsyX8Xun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=475,\n",
        "                           epochs=31,\n",
        "                           model__activation1='selu',\n",
        "                           model__activation2='sigmoid',\n",
        "                           model__dropout1=0.1551853147879086, \n",
        "                           model__dropout2=0.34328552130273915, \n",
        "                           model__layer1=115, \n",
        "                           model__layer2=512, \n",
        "                           model__learning_rate=0.006720220722226725,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.29870359291003556, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc4329b-b0a7-4373-8062-8ef1456ccef7",
        "id": "AzAbUlFJ8Xuv"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=475, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=31, model=<function create_model at 0x00000281C1861DC0>, model__activation1='selu', model__activation2='sigmoid', model__dropout1=0.1551853147879086, model__dropo...odel__learning_rate=0.006720220722226725, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F5D3B130>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000281CA839640>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.29870359291003556, verbose=0),\n",
              "                    include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "x0gUTFtS8Xu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c972f3b-6b61-4cbb-be2a-ae8768a688ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 115)               2875      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 115)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               59392     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,780\n",
            "Trainable params: 62,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=11412.845, rmse=106.831, r2=-0.391; v_loss=6356.325, v_rmse=79.727, v_r2=0.175; \n",
            "E 2\t: loss=5945.147, rmse=77.105, r2=0.275; v_loss=5396.596, v_rmse=73.462, v_r2=0.300; \n",
            "E 3\t: loss=5277.817, rmse=72.649, r2=0.357; v_loss=4715.444, v_rmse=68.669, v_r2=0.388; \n",
            "E 4\t: loss=4851.600, rmse=69.653, r2=0.409; v_loss=4380.777, v_rmse=66.187, v_r2=0.432; \n",
            "E 5\t: loss=4683.132, rmse=68.433, r2=0.429; v_loss=4226.966, v_rmse=65.015, v_r2=0.452; \n",
            "E 6\t: loss=4508.153, rmse=67.143, r2=0.451; v_loss=4149.666, v_rmse=64.418, v_r2=0.462; \n",
            "E 7\t: loss=4356.769, rmse=66.006, r2=0.469; v_loss=3874.763, v_rmse=62.248, v_r2=0.497; \n",
            "E 8\t: loss=4305.915, rmse=65.619, r2=0.475; v_loss=3760.175, v_rmse=61.320, v_r2=0.512; \n",
            "E 9\t: loss=4135.891, rmse=64.311, r2=0.496; v_loss=3714.424, v_rmse=60.946, v_r2=0.518; \n",
            "E 10\t: loss=4112.001, rmse=64.125, r2=0.499; v_loss=3723.338, v_rmse=61.019, v_r2=0.517; \n",
            "E 11\t: loss=4087.519, rmse=63.934, r2=0.502; v_loss=3888.968, v_rmse=62.362, v_r2=0.495; \n",
            "E 12\t: loss=4016.612, rmse=63.377, r2=0.510; v_loss=3690.266, v_rmse=60.748, v_r2=0.521; \n",
            "E 13\t: loss=3995.586, rmse=63.211, r2=0.513; v_loss=3558.496, v_rmse=59.653, v_r2=0.538; \n",
            "E 14\t: loss=3959.472, rmse=62.924, r2=0.517; v_loss=3671.296, v_rmse=60.591, v_r2=0.524; \n",
            "E 15\t: loss=4011.348, rmse=63.335, r2=0.511; v_loss=3687.719, v_rmse=60.727, v_r2=0.522; \n",
            "E 16\t: loss=3969.790, rmse=63.006, r2=0.516; v_loss=3525.207, v_rmse=59.373, v_r2=0.543; \n",
            "E 17\t: loss=3906.680, rmse=62.503, r2=0.524; v_loss=3539.453, v_rmse=59.493, v_r2=0.541; \n",
            "E 18\t: loss=3893.196, rmse=62.395, r2=0.525; v_loss=3755.824, v_rmse=61.285, v_r2=0.513; \n",
            "E 19\t: loss=3865.230, rmse=62.171, r2=0.529; v_loss=3498.592, v_rmse=59.149, v_r2=0.546; \n",
            "E 20\t: loss=3867.498, rmse=62.189, r2=0.529; v_loss=3527.484, v_rmse=59.393, v_r2=0.542; \n",
            "E 21\t: loss=3942.606, rmse=62.790, r2=0.519; v_loss=3834.963, v_rmse=61.927, v_r2=0.502; \n",
            "E 22\t: loss=3882.397, rmse=62.309, r2=0.527; v_loss=3482.044, v_rmse=59.009, v_r2=0.548; \n",
            "E 23\t: loss=3853.673, rmse=62.078, r2=0.530; v_loss=3497.748, v_rmse=59.142, v_r2=0.546; \n",
            "E 24\t: loss=3819.355, rmse=61.801, r2=0.534; v_loss=3447.289, v_rmse=58.714, v_r2=0.553; \n",
            "E 25\t: loss=3800.141, rmse=61.645, r2=0.537; v_loss=3506.534, v_rmse=59.216, v_r2=0.545; \n",
            "E 26\t: loss=3834.130, rmse=61.920, r2=0.533; v_loss=3487.682, v_rmse=59.057, v_r2=0.548; \n",
            "E 27\t: loss=3839.041, rmse=61.960, r2=0.532; v_loss=3589.738, v_rmse=59.914, v_r2=0.534; \n",
            "E 28\t: loss=3799.181, rmse=61.638, r2=0.537; v_loss=3428.869, v_rmse=58.557, v_r2=0.555; \n",
            "E 29\t: loss=3856.398, rmse=62.100, r2=0.530; v_loss=3452.709, v_rmse=58.760, v_r2=0.552; \n",
            "E 30\t: loss=3822.480, rmse=61.826, r2=0.534; v_loss=3588.001, v_rmse=59.900, v_r2=0.535; \n",
            "E 31\t: loss=3831.317, rmse=61.898, r2=0.533; v_loss=3434.701, v_rmse=58.606, v_r2=0.554; \n",
            "Finished: 2022-11-03 08:50:00.076937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "WTB1R-9-8Xu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9e270f-ba8e-4555-97c6-2c9539187ebb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.244,RMSE=-47.413\n",
            "Finished: 2022-11-03 08:50:00.179244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL"
      ],
      "metadata": {
        "id": "P3IBawiY8XvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.8092955367135408  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 190),\n",
        "('basemodel__epochs', 50),\n",
        "('basemodel__model__activation1', 'elu'),\n",
        "('basemodel__model__activation2', 'elu'),\n",
        "('basemodel__model__dropout1', 0.10508709834130409),\n",
        "('basemodel__model__dropout2', 0.15866472852550143),\n",
        "('basemodel__model__layer1', 130),\n",
        "('basemodel__model__layer2', 512),\n",
        "('basemodel__model__learning_rate', 0.01),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.5585747596345988),\n",
        "('clip_y', 80),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "95Sz7A0l8XvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=80\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=190,\n",
        "                           epochs=50,\n",
        "                           model__activation1='elu',\n",
        "                           model__activation2='elu',\n",
        "                           model__dropout1=0.10508709834130409, \n",
        "                           model__dropout2=0.15866472852550143, \n",
        "                           model__layer1=130, \n",
        "                           model__layer2=512, \n",
        "                           model__learning_rate=0.01,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.5585747596345988, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39abe87a-d8a7-4421-8b65-5ea464424834",
        "id": "8P2llTd78XvH"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=190, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=50, model=<function create_model at 0x00000281C1861DC0>, model__activation1='elu', model__activation2='elu', model__dropout1=0.10508709834130409, model__dropout2=...2=512, model__learning_rate=0.01, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000281CA774430>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F5D2FAC0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.5585747596345988, verbose=0),\n",
              "                    clip_y=80, include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "vh-GouSB8XvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59713e7-8f1c-43d2-9783-85a01282e89a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_11 (Dense)            (None, 130)               3250      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 130)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               67072     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 70,835\n",
            "Trainable params: 70,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=593.733, rmse=24.367, r2=-0.113; v_loss=334.483, v_rmse=18.289, v_r2=0.373; \n",
            "E 2\t: loss=239.771, rmse=15.485, r2=0.551; v_loss=195.883, v_rmse=13.996, v_r2=0.633; \n",
            "E 3\t: loss=193.930, rmse=13.926, r2=0.637; v_loss=153.126, v_rmse=12.374, v_r2=0.713; \n",
            "E 4\t: loss=170.129, rmse=13.043, r2=0.681; v_loss=137.161, v_rmse=11.712, v_r2=0.743; \n",
            "E 5\t: loss=169.232, rmse=13.009, r2=0.683; v_loss=143.867, v_rmse=11.994, v_r2=0.730; \n",
            "E 6\t: loss=146.492, rmse=12.103, r2=0.726; v_loss=121.007, v_rmse=11.000, v_r2=0.773; \n",
            "E 7\t: loss=131.888, rmse=11.484, r2=0.753; v_loss=120.149, v_rmse=10.961, v_r2=0.775; \n",
            "E 8\t: loss=128.168, rmse=11.321, r2=0.760; v_loss=111.702, v_rmse=10.569, v_r2=0.790; \n",
            "E 9\t: loss=122.842, rmse=11.083, r2=0.770; v_loss=108.456, v_rmse=10.414, v_r2=0.797; \n",
            "E 10\t: loss=125.595, rmse=11.207, r2=0.765; v_loss=118.559, v_rmse=10.889, v_r2=0.778; \n",
            "E 11\t: loss=119.842, rmse=10.947, r2=0.775; v_loss=104.459, v_rmse=10.220, v_r2=0.804; \n",
            "E 12\t: loss=118.466, rmse=10.884, r2=0.778; v_loss=114.295, v_rmse=10.691, v_r2=0.786; \n",
            "E 13\t: loss=126.130, rmse=11.231, r2=0.764; v_loss=108.980, v_rmse=10.439, v_r2=0.796; \n",
            "E 14\t: loss=112.028, rmse=10.584, r2=0.790; v_loss=151.602, v_rmse=12.313, v_r2=0.716; \n",
            "E 15\t: loss=119.621, rmse=10.937, r2=0.776; v_loss=112.878, v_rmse=10.624, v_r2=0.788; \n",
            "E 16\t: loss=111.973, rmse=10.582, r2=0.790; v_loss=118.573, v_rmse=10.889, v_r2=0.778; \n",
            "E 17\t: loss=111.531, rmse=10.561, r2=0.791; v_loss=113.953, v_rmse=10.675, v_r2=0.786; \n",
            "E 18\t: loss=114.390, rmse=10.695, r2=0.786; v_loss=102.988, v_rmse=10.148, v_r2=0.807; \n",
            "E 19\t: loss=113.517, rmse=10.654, r2=0.787; v_loss=102.941, v_rmse=10.146, v_r2=0.807; \n",
            "E 20\t: loss=105.839, rmse=10.288, r2=0.802; v_loss=160.843, v_rmse=12.682, v_r2=0.698; \n",
            "E 21\t: loss=110.971, rmse=10.534, r2=0.792; v_loss=107.136, v_rmse=10.351, v_r2=0.799; \n",
            "E 22\t: loss=103.890, rmse=10.193, r2=0.805; v_loss=108.779, v_rmse=10.430, v_r2=0.796; \n",
            "E 23\t: loss=109.056, rmse=10.443, r2=0.796; v_loss=127.964, v_rmse=11.312, v_r2=0.760; \n",
            "E 24\t: loss=106.687, rmse=10.329, r2=0.800; v_loss=104.914, v_rmse=10.243, v_r2=0.803; \n",
            "E 25\t: loss=103.055, rmse=10.152, r2=0.807; v_loss=108.627, v_rmse=10.422, v_r2=0.796; \n",
            "E 26\t: loss=108.614, rmse=10.422, r2=0.796; v_loss=120.233, v_rmse=10.965, v_r2=0.774; \n",
            "E 27\t: loss=102.841, rmse=10.141, r2=0.807; v_loss=113.250, v_rmse=10.642, v_r2=0.788; \n",
            "E 28\t: loss=106.188, rmse=10.305, r2=0.801; v_loss=102.752, v_rmse=10.137, v_r2=0.807; \n",
            "E 29\t: loss=106.279, rmse=10.309, r2=0.801; v_loss=97.540, v_rmse=9.876, v_r2=0.817; \n",
            "E 30\t: loss=101.829, rmse=10.091, r2=0.809; v_loss=97.785, v_rmse=9.889, v_r2=0.817; \n",
            "E 31\t: loss=108.465, rmse=10.415, r2=0.797; v_loss=126.605, v_rmse=11.252, v_r2=0.763; \n",
            "E 32\t: loss=101.294, rmse=10.064, r2=0.810; v_loss=102.355, v_rmse=10.117, v_r2=0.808; \n",
            "E 33\t: loss=100.519, rmse=10.026, r2=0.812; v_loss=108.127, v_rmse=10.398, v_r2=0.797; \n",
            "E 34\t: loss=101.634, rmse=10.081, r2=0.810; v_loss=136.919, v_rmse=11.701, v_r2=0.743; \n",
            "E 35\t: loss=99.289, rmse=9.964, r2=0.814; v_loss=118.833, v_rmse=10.901, v_r2=0.777; \n",
            "E 36\t: loss=104.564, rmse=10.226, r2=0.804; v_loss=97.879, v_rmse=9.893, v_r2=0.816; \n",
            "E 37\t: loss=104.842, rmse=10.239, r2=0.804; v_loss=99.273, v_rmse=9.964, v_r2=0.814; \n",
            "E 38\t: loss=99.332, rmse=9.967, r2=0.814; v_loss=97.938, v_rmse=9.896, v_r2=0.816; \n",
            "E 39\t: loss=103.396, rmse=10.168, r2=0.806; v_loss=102.035, v_rmse=10.101, v_r2=0.809; \n",
            "E 40\t: loss=98.087, rmse=9.904, r2=0.816; v_loss=123.365, v_rmse=11.107, v_r2=0.769; \n",
            "E 41\t: loss=100.378, rmse=10.019, r2=0.812; v_loss=139.790, v_rmse=11.823, v_r2=0.738; \n",
            "E 42\t: loss=102.150, rmse=10.107, r2=0.809; v_loss=102.573, v_rmse=10.128, v_r2=0.808; \n",
            "E 43\t: loss=100.345, rmse=10.017, r2=0.812; v_loss=116.747, v_rmse=10.805, v_r2=0.781; \n",
            "E 44\t: loss=97.588, rmse=9.879, r2=0.817; v_loss=99.813, v_rmse=9.991, v_r2=0.813; \n",
            "E 45\t: loss=100.148, rmse=10.007, r2=0.812; v_loss=98.194, v_rmse=9.909, v_r2=0.816; \n",
            "E 46\t: loss=99.871, rmse=9.994, r2=0.813; v_loss=98.908, v_rmse=9.945, v_r2=0.814; \n",
            "E 47\t: loss=102.207, rmse=10.110, r2=0.808; v_loss=113.372, v_rmse=10.648, v_r2=0.787; \n",
            "E 48\t: loss=98.486, rmse=9.924, r2=0.815; v_loss=121.592, v_rmse=11.027, v_r2=0.772; \n",
            "E 49\t: loss=100.494, rmse=10.025, r2=0.812; v_loss=106.744, v_rmse=10.332, v_r2=0.800; \n",
            "Finished: 2022-11-03 08:51:25.743778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "Sc37islE8XvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801a7f1f-7d09-4191-c98d-aeeea4ba293c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028301B55DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "R2=0.825,RMSE=-11.318\n",
            "Finished: 2022-11-03 08:51:25.841779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Linear RUL"
      ],
      "metadata": {
        "id": "wZocoM5X8XvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.5403727057267015  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 123),\n",
        "('basemodel__epochs', 50),\n",
        "('basemodel__model__activation1', 'selu'),\n",
        "('basemodel__model__activation2', 'selu'),\n",
        "('basemodel__model__dropout1', 0.2440452992785985),\n",
        "('basemodel__model__dropout2', 0.7488072091140527),\n",
        "('basemodel__model__layer1', 289),\n",
        "('basemodel__model__layer2', 490),\n",
        "('basemodel__model__learning_rate', 0.0002194806594294514),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.2571693518261989),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "AlgXpCVb8XvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=123,\n",
        "                           epochs=50,\n",
        "                           model__activation1='selu',\n",
        "                           model__activation2='selu',\n",
        "                           model__dropout1=0.2440452992785985, \n",
        "                           model__dropout2=0.7488072091140527, \n",
        "                           model__layer1=289, \n",
        "                           model__layer2=490, \n",
        "                           model__learning_rate=0.0002194806594294514,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.2571693518261989, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5111e9b-2d24-4089-df97-ed3c91f7cf99",
        "id": "6TR5_NRP8XvP"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=123, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=50, model=<function create_model at 0x00000281C1861DC0>, model__activation1='selu', model__activation2='selu', model__dropout1=0.2440452992785985, model__dropout2..._rate=0.0002194806594294514, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F61CE730>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F5DAF6A0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.2571693518261989, verbose=0),\n",
              "                    include_settings=True, poly_degree=2)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "kGivi_vW8XvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4383ec3-ca6e-48a2-98a3-f012170841ac"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 289)               93925     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 289)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 490)               142100    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 490)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 491       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 236,516\n",
            "Trainable params: 236,516\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=11667.834, rmse=108.018, r2=-0.395; v_loss=6658.820, v_rmse=81.602, v_r2=0.069; \n",
            "E 2\t: loss=7378.051, rmse=85.896, r2=0.118; v_loss=4884.748, v_rmse=69.891, v_r2=0.317; \n",
            "E 3\t: loss=5581.100, rmse=74.707, r2=0.333; v_loss=3562.145, v_rmse=59.684, v_r2=0.502; \n",
            "E 4\t: loss=4758.098, rmse=68.979, r2=0.431; v_loss=3317.595, v_rmse=57.599, v_r2=0.536; \n",
            "E 5\t: loss=4590.487, rmse=67.753, r2=0.451; v_loss=3241.772, v_rmse=56.937, v_r2=0.547; \n",
            "E 6\t: loss=4485.217, rmse=66.972, r2=0.464; v_loss=3154.376, v_rmse=56.164, v_r2=0.559; \n",
            "E 7\t: loss=4443.148, rmse=66.657, r2=0.469; v_loss=3139.391, v_rmse=56.030, v_r2=0.561; \n",
            "E 8\t: loss=4348.035, rmse=65.940, r2=0.480; v_loss=3154.307, v_rmse=56.163, v_r2=0.559; \n",
            "E 9\t: loss=4324.676, rmse=65.762, r2=0.483; v_loss=3541.871, v_rmse=59.514, v_r2=0.505; \n",
            "E 10\t: loss=4260.687, rmse=65.274, r2=0.491; v_loss=3131.581, v_rmse=55.961, v_r2=0.562; \n",
            "E 11\t: loss=4260.993, rmse=65.276, r2=0.491; v_loss=3105.705, v_rmse=55.729, v_r2=0.566; \n",
            "E 12\t: loss=4243.628, rmse=65.143, r2=0.493; v_loss=3195.732, v_rmse=56.531, v_r2=0.553; \n",
            "E 13\t: loss=4214.003, rmse=64.915, r2=0.496; v_loss=3210.400, v_rmse=56.660, v_r2=0.551; \n",
            "E 14\t: loss=4212.355, rmse=64.903, r2=0.496; v_loss=3131.755, v_rmse=55.962, v_r2=0.562; \n",
            "E 15\t: loss=4208.208, rmse=64.871, r2=0.497; v_loss=3146.328, v_rmse=56.092, v_r2=0.560; \n",
            "E 16\t: loss=4195.625, rmse=64.774, r2=0.498; v_loss=3109.110, v_rmse=55.759, v_r2=0.565; \n",
            "E 17\t: loss=4183.701, rmse=64.682, r2=0.500; v_loss=3092.384, v_rmse=55.609, v_r2=0.567; \n",
            "E 18\t: loss=4186.354, rmse=64.702, r2=0.499; v_loss=3085.309, v_rmse=55.546, v_r2=0.568; \n",
            "E 19\t: loss=4153.922, rmse=64.451, r2=0.503; v_loss=3087.390, v_rmse=55.564, v_r2=0.568; \n",
            "E 20\t: loss=4181.792, rmse=64.667, r2=0.500; v_loss=3068.969, v_rmse=55.398, v_r2=0.571; \n",
            "E 21\t: loss=4162.543, rmse=64.518, r2=0.502; v_loss=3069.947, v_rmse=55.407, v_r2=0.571; \n",
            "E 22\t: loss=4127.999, rmse=64.250, r2=0.506; v_loss=3055.470, v_rmse=55.276, v_r2=0.573; \n",
            "E 23\t: loss=4126.602, rmse=64.239, r2=0.507; v_loss=3424.005, v_rmse=58.515, v_r2=0.521; \n",
            "E 24\t: loss=4127.977, rmse=64.249, r2=0.506; v_loss=3092.311, v_rmse=55.609, v_r2=0.567; \n",
            "E 25\t: loss=4111.245, rmse=64.119, r2=0.508; v_loss=3078.475, v_rmse=55.484, v_r2=0.569; \n",
            "E 26\t: loss=4114.412, rmse=64.144, r2=0.508; v_loss=3112.066, v_rmse=55.786, v_r2=0.565; \n",
            "E 27\t: loss=4091.945, rmse=63.968, r2=0.511; v_loss=3076.267, v_rmse=55.464, v_r2=0.570; \n",
            "E 28\t: loss=4103.781, rmse=64.061, r2=0.509; v_loss=3049.672, v_rmse=55.224, v_r2=0.573; \n",
            "E 29\t: loss=4088.450, rmse=63.941, r2=0.511; v_loss=3099.994, v_rmse=55.678, v_r2=0.566; \n",
            "E 30\t: loss=4060.270, rmse=63.720, r2=0.515; v_loss=3033.532, v_rmse=55.078, v_r2=0.576; \n",
            "E 31\t: loss=4084.584, rmse=63.911, r2=0.512; v_loss=3051.014, v_rmse=55.236, v_r2=0.573; \n",
            "E 32\t: loss=4084.608, rmse=63.911, r2=0.512; v_loss=3041.877, v_rmse=55.153, v_r2=0.574; \n",
            "E 33\t: loss=4082.923, rmse=63.898, r2=0.512; v_loss=3073.707, v_rmse=55.441, v_r2=0.570; \n",
            "E 34\t: loss=4083.649, rmse=63.903, r2=0.512; v_loss=3040.265, v_rmse=55.139, v_r2=0.575; \n",
            "E 35\t: loss=4073.002, rmse=63.820, r2=0.513; v_loss=3071.042, v_rmse=55.417, v_r2=0.570; \n",
            "Finished: 2022-11-03 08:53:36.511465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "W4aA1r2q8XvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b91625-69b4-4902-e27a-6200408ff183"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000282F629F4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "R2=0.241,RMSE=-47.491\n",
            "Finished: 2022-11-03 08:53:36.605466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Non-Linear RUL\n"
      ],
      "metadata": {
        "id": "01ti8j0I8Xva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.8137440377581179  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 330),\n",
        "('basemodel__epochs', 28),\n",
        "('basemodel__model__activation1', 'selu'),\n",
        "('basemodel__model__activation2', 'sigmoid'),\n",
        "('basemodel__model__dropout1', 0.35104184791262427),\n",
        "('basemodel__model__dropout2', 0.13083828310831028),\n",
        "('basemodel__model__layer1', 340),\n",
        "('basemodel__model__layer2', 191),\n",
        "('basemodel__model__learning_rate', 0.009916920939867237),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.1),\n",
        "('clip_y', 87),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "dNgvLPHf8Xvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=87\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=MinMaxScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=163,\n",
        "                           epochs=28,\n",
        "                           model__activation1='selu',\n",
        "                           model__activation2='relu',\n",
        "                           model__dropout1=0.1203636705927238, \n",
        "                           model__dropout2=0.3108545284702169, \n",
        "                           model__layer1=18, \n",
        "                           model__layer2=364, \n",
        "                           model__learning_rate=0.009076852273996898,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.14877733495106543, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a9ef0b-9b90-4b5b-d604-0be97020d180",
        "id": "dAWLzDAC8Xvg"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=163, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=28, model=<function create_model at 0x00000281C1861DC0>, model__activation1='selu', model__activation2='relu', model__dropout1=0.1203636705927238, model__dropout2...__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F3D84670>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F3D76EE0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.14877733495106543, verbose=0),\n",
              "                    clip_y=87, include_settings=True, poly_degree=2,\n",
              "                    scaler=MinMaxScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "w4AgmUsB8Xvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6987bda9-8e31-4944-84dd-f8a615a9b881"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_26 (Dense)            (None, 18)                5850      \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 18)                0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 364)               6916      \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 364)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1)                 365       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,131\n",
            "Trainable params: 13,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=797.643, rmse=28.243, r2=-0.202; v_loss=525.891, v_rmse=22.932, v_r2=0.222; \n",
            "E 2\t: loss=422.050, rmse=20.544, r2=0.364; v_loss=221.165, v_rmse=14.872, v_r2=0.673; \n",
            "E 3\t: loss=323.803, rmse=17.995, r2=0.512; v_loss=230.380, v_rmse=15.178, v_r2=0.659; \n",
            "E 4\t: loss=284.621, rmse=16.871, r2=0.571; v_loss=180.889, v_rmse=13.449, v_r2=0.733; \n",
            "E 5\t: loss=264.930, rmse=16.277, r2=0.601; v_loss=162.657, v_rmse=12.754, v_r2=0.760; \n",
            "E 6\t: loss=242.756, rmse=15.581, r2=0.634; v_loss=186.286, v_rmse=13.649, v_r2=0.725; \n",
            "E 7\t: loss=240.443, rmse=15.506, r2=0.638; v_loss=297.792, v_rmse=17.257, v_r2=0.560; \n",
            "E 8\t: loss=219.696, rmse=14.822, r2=0.669; v_loss=174.693, v_rmse=13.217, v_r2=0.742; \n",
            "E 9\t: loss=229.774, rmse=15.158, r2=0.654; v_loss=172.048, v_rmse=13.117, v_r2=0.746; \n",
            "E 10\t: loss=219.107, rmse=14.802, r2=0.670; v_loss=144.859, v_rmse=12.036, v_r2=0.786; \n",
            "E 11\t: loss=212.354, rmse=14.572, r2=0.680; v_loss=193.898, v_rmse=13.925, v_r2=0.713; \n",
            "E 12\t: loss=217.726, rmse=14.756, r2=0.672; v_loss=171.814, v_rmse=13.108, v_r2=0.746; \n",
            "E 13\t: loss=215.613, rmse=14.684, r2=0.675; v_loss=152.156, v_rmse=12.335, v_r2=0.775; \n",
            "E 14\t: loss=211.968, rmse=14.559, r2=0.681; v_loss=193.686, v_rmse=13.917, v_r2=0.714; \n",
            "E 15\t: loss=216.468, rmse=14.713, r2=0.674; v_loss=154.777, v_rmse=12.441, v_r2=0.771; \n",
            "E 16\t: loss=195.910, rmse=13.997, r2=0.705; v_loss=223.293, v_rmse=14.943, v_r2=0.670; \n",
            "E 17\t: loss=200.883, rmse=14.173, r2=0.697; v_loss=149.129, v_rmse=12.212, v_r2=0.780; \n",
            "E 18\t: loss=202.725, rmse=14.238, r2=0.695; v_loss=137.005, v_rmse=11.705, v_r2=0.797; \n",
            "E 19\t: loss=201.628, rmse=14.200, r2=0.696; v_loss=137.017, v_rmse=11.705, v_r2=0.797; \n",
            "E 20\t: loss=200.761, rmse=14.169, r2=0.698; v_loss=126.434, v_rmse=11.244, v_r2=0.813; \n",
            "E 21\t: loss=199.342, rmse=14.119, r2=0.700; v_loss=166.541, v_rmse=12.905, v_r2=0.754; \n",
            "Finished: 2022-11-03 09:01:04.647720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "id": "N-Pj7cIL8Xvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeafb2d1-594f-4883-aed7-1bf87b701c42"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.573,RMSE=-19.508\n",
            "Finished: 2022-11-03 09:01:04.753720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP-3 "
      ],
      "metadata": {
        "id": "00TdIEZ6FBx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "kL9N0ocMFBx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.5387760282104624    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 314),\n",
        "('basemodel__epochs', 28),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'selu'),\n",
        "('basemodel__model__activation3', 'selu'),\n",
        "('basemodel__model__dropout1', 0.23024902825797536),\n",
        "('basemodel__model__dropout2', 0.27996962001171605),\n",
        "('basemodel__model__dropout3', 0.6837395507589162),\n",
        "('basemodel__model__layer1', 288),\n",
        "('basemodel__model__layer2', 158),\n",
        "('basemodel__model__layer3', 381),\n",
        "('basemodel__model__learning_rate', 0.002110763064656101),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.5017895339783691),\n",
        "('include_settings', True),\n",
        "('scaler', MinMaxScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "zmVugjXMFByB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=MinMaxScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=314,\n",
        "                           epochs=28,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='selu',\n",
        "                           model__activation3='selu',\n",
        "                           model__dropout1=0.23024902825797536, \n",
        "                           model__dropout2=0.27996962001171605, \n",
        "                           model__dropout3=0.6837395507589162, \n",
        "                           model__layer1=288, \n",
        "                           model__layer2=158, \n",
        "                           model__layer3=381, \n",
        "                           model__learning_rate=0.002110763064656101,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.5017895339783691, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47b1150-2db7-4ef9-d227-92ab2f84d507",
        "id": "hI5gTmNEFByH"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=314, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=28, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__activation2='selu', model__activation3='selu', model__dropout1=0.230249....002110763064656101, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F47B5A30>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F3DDE0D0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.5017895339783691, verbose=0),\n",
              "                    include_settings=True, scaler=MinMaxScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6649b71-2b4b-489f-b7c2-bd8ffffd3002",
        "id": "kcvF0xLyFByO"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_29 (Dense)            (None, 288)               7200      \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 158)               45662     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 158)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 381)               60579     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 381)               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 382       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 113,823\n",
            "Trainable params: 113,823\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=9402.268, rmse=96.965, r2=-0.201; v_loss=8283.586, v_rmse=91.014, v_r2=0.000; \n",
            "E 2\t: loss=7970.029, rmse=89.275, r2=-0.018; v_loss=8231.700, v_rmse=90.729, v_r2=0.007; \n",
            "E 3\t: loss=7780.113, rmse=88.205, r2=0.007; v_loss=7879.628, v_rmse=88.767, v_r2=0.049; \n",
            "E 4\t: loss=6463.113, rmse=80.393, r2=0.175; v_loss=9691.872, v_rmse=98.447, v_r2=-0.170; \n",
            "E 5\t: loss=5594.582, rmse=74.797, r2=0.286; v_loss=4935.063, v_rmse=70.250, v_r2=0.404; \n",
            "E 6\t: loss=4830.703, rmse=69.503, r2=0.383; v_loss=4638.437, v_rmse=68.106, v_r2=0.440; \n",
            "E 7\t: loss=4306.736, rmse=65.626, r2=0.450; v_loss=4020.597, v_rmse=63.408, v_r2=0.515; \n",
            "E 8\t: loss=4102.642, rmse=64.052, r2=0.476; v_loss=5966.528, v_rmse=77.243, v_r2=0.280; \n",
            "E 9\t: loss=4165.503, rmse=64.541, r2=0.468; v_loss=4215.737, v_rmse=64.929, v_r2=0.491; \n",
            "E 10\t: loss=3957.940, rmse=62.912, r2=0.495; v_loss=3963.141, v_rmse=62.953, v_r2=0.522; \n",
            "E 11\t: loss=3981.150, rmse=63.096, r2=0.492; v_loss=3891.865, v_rmse=62.385, v_r2=0.530; \n",
            "E 12\t: loss=4044.913, rmse=63.600, r2=0.484; v_loss=3964.725, v_rmse=62.966, v_r2=0.522; \n",
            "E 13\t: loss=4000.997, rmse=63.253, r2=0.489; v_loss=4812.498, v_rmse=69.372, v_r2=0.419; \n",
            "E 14\t: loss=3936.263, rmse=62.740, r2=0.497; v_loss=3874.715, v_rmse=62.247, v_r2=0.532; \n",
            "E 15\t: loss=3889.723, rmse=62.368, r2=0.503; v_loss=4225.442, v_rmse=65.003, v_r2=0.490; \n",
            "E 16\t: loss=3960.337, rmse=62.931, r2=0.494; v_loss=4313.946, v_rmse=65.681, v_r2=0.479; \n",
            "E 17\t: loss=3936.127, rmse=62.739, r2=0.497; v_loss=4143.517, v_rmse=64.370, v_r2=0.500; \n",
            "E 18\t: loss=3910.411, rmse=62.533, r2=0.501; v_loss=3887.304, v_rmse=62.348, v_r2=0.531; \n",
            "E 19\t: loss=3951.663, rmse=62.862, r2=0.495; v_loss=3892.544, v_rmse=62.390, v_r2=0.530; \n",
            "E 20\t: loss=3901.786, rmse=62.464, r2=0.502; v_loss=4556.143, v_rmse=67.499, v_r2=0.450; \n",
            "Finished: 2022-11-03 09:03:03.466532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4128eed7-f895-46fc-da18-be35b43bcaee",
        "id": "QG0jhrtKFByS"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=-0.243,RMSE=-60.796\n",
            "Finished: 2022-11-03 09:03:03.565239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL"
      ],
      "metadata": {
        "id": "1Xs7NWLvFByZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.8191894908542713    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 277),\n",
        "('basemodel__epochs', 50),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'selu'),\n",
        "('basemodel__model__activation3', 'relu'),\n",
        "('basemodel__model__dropout1', 0.15180815010798232),\n",
        "('basemodel__model__dropout2', 0.4099007005621048),\n",
        "('basemodel__model__dropout3', 0.1),\n",
        "('basemodel__model__layer1', 197),\n",
        "('basemodel__model__layer2', 353),\n",
        "('basemodel__model__layer3', 261),\n",
        "('basemodel__model__learning_rate', 0.0041467234962615715),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.17508243717937616),\n",
        "('clip_y', 80),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "RbLuNAHAFBye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=80\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=277,\n",
        "                           epochs=50,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='selu',\n",
        "                           model__activation3='relu',\n",
        "                           model__dropout1=0.15180815010798232, \n",
        "                           model__dropout2=0.4099007005621048, \n",
        "                           model__dropout3=0.1, \n",
        "                           model__layer1=197, \n",
        "                           model__layer2=353, \n",
        "                           model__layer3=261,  \n",
        "                           model__learning_rate=0.0041467234962615715,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.17508243717937616, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812a6669-e792-4cef-ae29-070e829bd9e4",
        "id": "PSnWYxQiFByi"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=277, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=50, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__activation2='selu', model__activation3='relu', model__dropout1=0.151808...ing_rate=0.0041467234962615715, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F4963D30>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F4963F40>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.17508243717937616, verbose=0),\n",
              "                    clip_y=80, include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4878a8-62a4-414b-a9fc-2b9dea8f54cc",
        "id": "wU3RI7KvFByq"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_33 (Dense)            (None, 197)               4925      \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 197)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 353)               69894     \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 353)               0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 261)               92394     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 261)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 1)                 262       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 167,475\n",
            "Trainable params: 167,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=490.888, rmse=22.156, r2=0.075; v_loss=200.483, v_rmse=14.159, v_r2=0.632; \n",
            "E 2\t: loss=205.653, rmse=14.341, r2=0.613; v_loss=143.154, v_rmse=11.965, v_r2=0.737; \n",
            "E 3\t: loss=179.676, rmse=13.404, r2=0.662; v_loss=126.151, v_rmse=11.232, v_r2=0.768; \n",
            "E 4\t: loss=165.497, rmse=12.865, r2=0.688; v_loss=122.548, v_rmse=11.070, v_r2=0.775; \n",
            "E 5\t: loss=159.449, rmse=12.627, r2=0.700; v_loss=105.243, v_rmse=10.259, v_r2=0.807; \n",
            "E 6\t: loss=154.436, rmse=12.427, r2=0.709; v_loss=109.408, v_rmse=10.460, v_r2=0.799; \n",
            "E 7\t: loss=142.344, rmse=11.931, r2=0.732; v_loss=138.378, v_rmse=11.763, v_r2=0.746; \n",
            "E 8\t: loss=142.270, rmse=11.928, r2=0.732; v_loss=93.220, v_rmse=9.655, v_r2=0.829; \n",
            "E 9\t: loss=137.097, rmse=11.709, r2=0.742; v_loss=103.263, v_rmse=10.162, v_r2=0.810; \n",
            "E 10\t: loss=135.679, rmse=11.648, r2=0.744; v_loss=89.728, v_rmse=9.472, v_r2=0.835; \n",
            "E 11\t: loss=134.017, rmse=11.577, r2=0.748; v_loss=89.135, v_rmse=9.441, v_r2=0.836; \n",
            "E 12\t: loss=132.486, rmse=11.510, r2=0.750; v_loss=89.608, v_rmse=9.466, v_r2=0.836; \n",
            "E 13\t: loss=126.977, rmse=11.268, r2=0.761; v_loss=81.588, v_rmse=9.033, v_r2=0.850; \n",
            "E 14\t: loss=125.310, rmse=11.194, r2=0.764; v_loss=82.115, v_rmse=9.062, v_r2=0.849; \n",
            "E 15\t: loss=130.130, rmse=11.407, r2=0.755; v_loss=96.874, v_rmse=9.842, v_r2=0.822; \n",
            "E 16\t: loss=125.174, rmse=11.188, r2=0.764; v_loss=80.334, v_rmse=8.963, v_r2=0.853; \n",
            "E 17\t: loss=122.705, rmse=11.077, r2=0.769; v_loss=85.364, v_rmse=9.239, v_r2=0.843; \n",
            "E 18\t: loss=124.436, rmse=11.155, r2=0.766; v_loss=88.133, v_rmse=9.388, v_r2=0.838; \n",
            "E 19\t: loss=123.312, rmse=11.105, r2=0.768; v_loss=77.956, v_rmse=8.829, v_r2=0.857; \n",
            "E 20\t: loss=121.649, rmse=11.029, r2=0.771; v_loss=78.768, v_rmse=8.875, v_r2=0.855; \n",
            "E 21\t: loss=116.888, rmse=10.811, r2=0.780; v_loss=83.624, v_rmse=9.145, v_r2=0.847; \n",
            "E 22\t: loss=121.744, rmse=11.034, r2=0.771; v_loss=87.757, v_rmse=9.368, v_r2=0.839; \n",
            "E 23\t: loss=120.131, rmse=10.960, r2=0.774; v_loss=79.655, v_rmse=8.925, v_r2=0.854; \n",
            "E 24\t: loss=119.763, rmse=10.944, r2=0.774; v_loss=93.058, v_rmse=9.647, v_r2=0.829; \n",
            "E 25\t: loss=117.249, rmse=10.828, r2=0.779; v_loss=80.739, v_rmse=8.985, v_r2=0.852; \n",
            "E 26\t: loss=115.931, rmse=10.767, r2=0.782; v_loss=81.927, v_rmse=9.051, v_r2=0.850; \n",
            "E 27\t: loss=117.320, rmse=10.831, r2=0.779; v_loss=82.736, v_rmse=9.096, v_r2=0.848; \n",
            "E 28\t: loss=115.509, rmse=10.748, r2=0.782; v_loss=79.241, v_rmse=8.902, v_r2=0.855; \n",
            "E 29\t: loss=114.072, rmse=10.680, r2=0.785; v_loss=80.258, v_rmse=8.959, v_r2=0.853; \n",
            "E 30\t: loss=114.544, rmse=10.703, r2=0.784; v_loss=82.411, v_rmse=9.078, v_r2=0.849; \n",
            "E 31\t: loss=114.483, rmse=10.700, r2=0.784; v_loss=90.649, v_rmse=9.521, v_r2=0.834; \n",
            "E 32\t: loss=112.461, rmse=10.605, r2=0.788; v_loss=82.643, v_rmse=9.091, v_r2=0.848; \n",
            "E 33\t: loss=113.886, rmse=10.672, r2=0.785; v_loss=76.658, v_rmse=8.755, v_r2=0.859; \n",
            "E 34\t: loss=113.230, rmse=10.641, r2=0.787; v_loss=76.673, v_rmse=8.756, v_r2=0.859; \n",
            "E 35\t: loss=112.823, rmse=10.622, r2=0.787; v_loss=88.907, v_rmse=9.429, v_r2=0.837; \n",
            "E 36\t: loss=115.889, rmse=10.765, r2=0.782; v_loss=75.144, v_rmse=8.669, v_r2=0.862; \n",
            "E 37\t: loss=113.710, rmse=10.663, r2=0.786; v_loss=77.880, v_rmse=8.825, v_r2=0.857; \n",
            "Finished: 2022-11-03 09:24:51.662954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9cd5f3-c046-4a79-b886-f10f95d396de",
        "id": "9s8vEev3FByt"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.810,RMSE=-11.799\n",
            "Finished: 2022-11-03 09:24:51.765932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Linear RUL"
      ],
      "metadata": {
        "id": "yk8grx_3FByx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.5434197290446054    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 254),\n",
        "('basemodel__epochs', 44),\n",
        "('basemodel__model__activation1', 'relu'),\n",
        "('basemodel__model__activation2', 'selu'),\n",
        "('basemodel__model__activation3', 'elu'),\n",
        "('basemodel__model__dropout1', 0.22005754217690313),\n",
        "('basemodel__model__dropout2', 0.2204345848078386),\n",
        "('basemodel__model__dropout3', 0.6505425209498465),\n",
        "('basemodel__model__layer1', 165),\n",
        "('basemodel__model__layer2', 512),\n",
        "('basemodel__model__layer3', 485),\n",
        "('basemodel__model__learning_rate', 0.0023431044122668105),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.1),\n",
        "('include_settings', True),\n",
        "('poly_degree', 3),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "2TWz2xbfFBy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=3,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=254,\n",
        "                           epochs=44,\n",
        "                           model__activation1='relu',\n",
        "                           model__activation2='selu',\n",
        "                           model__activation3='elu',\n",
        "                           model__dropout1=0.22005754217690313, \n",
        "                           model__dropout2=0.2204345848078386, \n",
        "                           model__dropout3=0.6505425209498465, \n",
        "                           model__layer1=165, \n",
        "                           model__layer2=512, \n",
        "                           model__layer3=485, \n",
        "                           model__learning_rate=0.0023431044122668105,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.1, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822bb4f1-f590-4db9-d493-536751b1b01b",
        "id": "97IkcENqFBy1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=254, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=44, model=<function create_model at 0x00000281C1861DC0>, model__activation1='relu', model__activation2='selu', model__activation3='elu', model__dropout1=0.2200575...model__learning_rate=0.0023431044122668105, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x0000028301B9EBE0>, <tensorflow_addons.metrics.r_square.RSquare object at 0x0000028301B9EE50>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.1, verbose=0),\n",
              "                    include_settings=True, poly_degree=3)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74f93bf-0048-46e4-c520-7b720beba853",
        "id": "GGBuOcsxFBy4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_37 (Dense)            (None, 165)               482625    \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 165)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 512)               84992     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 485)               248805    \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 485)               0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 1)                 486       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 816,908\n",
            "Trainable params: 816,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=7337.337, rmse=85.658, r2=0.104; v_loss=10313.082, v_rmse=101.553, v_r2=-0.500; \n",
            "E 2\t: loss=5084.904, rmse=71.309, r2=0.379; v_loss=4958.545, v_rmse=70.417, v_r2=0.279; \n",
            "E 3\t: loss=4544.997, rmse=67.417, r2=0.445; v_loss=4837.165, v_rmse=69.550, v_r2=0.297; \n",
            "E 4\t: loss=4416.613, rmse=66.458, r2=0.461; v_loss=3276.957, v_rmse=57.245, v_r2=0.523; \n",
            "E 5\t: loss=4301.471, rmse=65.586, r2=0.475; v_loss=3325.531, v_rmse=57.667, v_r2=0.516; \n",
            "E 6\t: loss=4266.282, rmse=65.317, r2=0.479; v_loss=4195.160, v_rmse=64.770, v_r2=0.390; \n",
            "E 7\t: loss=4220.940, rmse=64.969, r2=0.485; v_loss=6046.884, v_rmse=77.762, v_r2=0.121; \n",
            "E 8\t: loss=4328.737, rmse=65.793, r2=0.471; v_loss=5390.454, v_rmse=73.420, v_r2=0.216; \n",
            "E 9\t: loss=4171.823, rmse=64.590, r2=0.491; v_loss=5135.866, v_rmse=71.665, v_r2=0.253; \n",
            "E 10\t: loss=4331.851, rmse=65.817, r2=0.471; v_loss=3448.408, v_rmse=58.723, v_r2=0.499; \n",
            "E 11\t: loss=4205.094, rmse=64.847, r2=0.487; v_loss=5175.097, v_rmse=71.938, v_r2=0.247; \n",
            "E 12\t: loss=4236.908, rmse=65.092, r2=0.483; v_loss=4682.774, v_rmse=68.431, v_r2=0.319; \n",
            "E 13\t: loss=4201.776, rmse=64.821, r2=0.487; v_loss=4560.565, v_rmse=67.532, v_r2=0.337; \n",
            "E 14\t: loss=4246.361, rmse=65.164, r2=0.482; v_loss=3360.446, v_rmse=57.969, v_r2=0.511; \n",
            "Finished: 2022-11-03 09:27:27.033020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3ce2eb-dbba-4257-9f47-3aa76eb45fe3",
        "id": "pNP_HTLyFBy7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=-0.848,RMSE=-74.113\n",
            "Finished: 2022-11-03 09:27:27.482022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Non-Linear RUL\n"
      ],
      "metadata": {
        "id": "UYgtv941FBy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.8120876882657063   \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 127),\n",
        "('basemodel__epochs', 32),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'sigmoid'),\n",
        "('basemodel__model__activation3', 'selu'),\n",
        "('basemodel__model__dropout1', 0.3239516474706593),\n",
        "('basemodel__model__dropout2', 0.4398002541265835),\n",
        "('basemodel__model__dropout3', 0.11820142028402897),\n",
        "('basemodel__model__layer1', 300),\n",
        "('basemodel__model__layer2', 387),\n",
        "('basemodel__model__layer3', 97),\n",
        "('basemodel__model__learning_rate', 0.0007470235779685965),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.27478903553784617),\n",
        "('clip_y', 88),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "WszSSJcHFBy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=88\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=127,\n",
        "                           epochs=32,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='sigmoid',\n",
        "                           model__activation3='selu',\n",
        "                           model__dropout1=0.3239516474706593, \n",
        "                           model__dropout2=0.4398002541265835, \n",
        "                           model__dropout3=0.11820142028402897, \n",
        "                           model__layer1=300, \n",
        "                           model__layer2=387, \n",
        "                           model__layer3=97, \n",
        "                           model__learning_rate=0.0007470235779685965,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.27478903553784617, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cafaff-2296-4177-a4e9-fe6b0a9d482e",
        "id": "5zdwA-JAFBzA"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=127, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=32, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__activation2='sigmoid', model__activation3='selu', model__dropout1=0.323...7470235779685965, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000281D7D86B20>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000281D7D866A0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.27478903553784617, verbose=0),\n",
              "                    clip_y=88, include_settings=True, poly_degree=2)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7d04b8-0f22-4e34-da3c-b9fe38fce2ce",
        "id": "_7BYEM3iFBzC"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_41 (Dense)            (None, 300)               97500     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 300)               0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 387)               116487    \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 387)               0         \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 97)                37636     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 97)                0         \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1)                 98        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 251,721\n",
            "Trainable params: 251,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=684.229, rmse=26.158, r2=-0.007; v_loss=219.813, v_rmse=14.826, v_r2=0.687; \n",
            "E 2\t: loss=225.378, rmse=15.013, r2=0.668; v_loss=168.932, v_rmse=12.997, v_r2=0.759; \n",
            "E 3\t: loss=194.302, rmse=13.939, r2=0.714; v_loss=180.272, v_rmse=13.427, v_r2=0.743; \n",
            "E 4\t: loss=181.024, rmse=13.455, r2=0.734; v_loss=208.858, v_rmse=14.452, v_r2=0.702; \n",
            "E 5\t: loss=179.180, rmse=13.386, r2=0.736; v_loss=146.822, v_rmse=12.117, v_r2=0.791; \n",
            "E 6\t: loss=169.218, rmse=13.008, r2=0.751; v_loss=140.921, v_rmse=11.871, v_r2=0.799; \n",
            "E 7\t: loss=165.793, rmse=12.876, r2=0.756; v_loss=140.216, v_rmse=11.841, v_r2=0.800; \n",
            "E 8\t: loss=162.696, rmse=12.755, r2=0.761; v_loss=137.074, v_rmse=11.708, v_r2=0.805; \n",
            "E 9\t: loss=161.538, rmse=12.710, r2=0.762; v_loss=138.056, v_rmse=11.750, v_r2=0.803; \n",
            "E 10\t: loss=158.169, rmse=12.577, r2=0.767; v_loss=139.414, v_rmse=11.807, v_r2=0.801; \n",
            "E 11\t: loss=156.046, rmse=12.492, r2=0.770; v_loss=139.240, v_rmse=11.800, v_r2=0.802; \n",
            "E 12\t: loss=155.789, rmse=12.482, r2=0.771; v_loss=158.854, v_rmse=12.604, v_r2=0.774; \n",
            "E 13\t: loss=154.940, rmse=12.447, r2=0.772; v_loss=153.515, v_rmse=12.390, v_r2=0.781; \n",
            "E 14\t: loss=152.371, rmse=12.344, r2=0.776; v_loss=139.092, v_rmse=11.794, v_r2=0.802; \n",
            "E 15\t: loss=149.230, rmse=12.216, r2=0.780; v_loss=138.139, v_rmse=11.753, v_r2=0.803; \n",
            "E 16\t: loss=148.288, rmse=12.177, r2=0.782; v_loss=131.464, v_rmse=11.466, v_r2=0.813; \n",
            "E 17\t: loss=150.076, rmse=12.251, r2=0.779; v_loss=134.663, v_rmse=11.604, v_r2=0.808; \n",
            "E 18\t: loss=147.894, rmse=12.161, r2=0.782; v_loss=134.917, v_rmse=11.615, v_r2=0.808; \n",
            "E 19\t: loss=147.146, rmse=12.130, r2=0.783; v_loss=135.562, v_rmse=11.643, v_r2=0.807; \n",
            "E 20\t: loss=146.295, rmse=12.095, r2=0.785; v_loss=132.958, v_rmse=11.531, v_r2=0.811; \n",
            "E 21\t: loss=144.078, rmse=12.003, r2=0.788; v_loss=131.444, v_rmse=11.465, v_r2=0.813; \n",
            "E 22\t: loss=146.788, rmse=12.116, r2=0.784; v_loss=129.507, v_rmse=11.380, v_r2=0.815; \n",
            "E 23\t: loss=145.393, rmse=12.058, r2=0.786; v_loss=129.176, v_rmse=11.366, v_r2=0.816; \n",
            "E 24\t: loss=143.695, rmse=11.987, r2=0.789; v_loss=130.025, v_rmse=11.403, v_r2=0.815; \n",
            "E 25\t: loss=143.261, rmse=11.969, r2=0.789; v_loss=129.806, v_rmse=11.393, v_r2=0.815; \n",
            "E 26\t: loss=142.470, rmse=11.936, r2=0.790; v_loss=142.097, v_rmse=11.920, v_r2=0.798; \n",
            "E 27\t: loss=143.691, rmse=11.987, r2=0.789; v_loss=138.651, v_rmse=11.775, v_r2=0.802; \n",
            "E 28\t: loss=143.421, rmse=11.976, r2=0.789; v_loss=154.355, v_rmse=12.424, v_r2=0.780; \n",
            "E 29\t: loss=141.955, rmse=11.914, r2=0.791; v_loss=127.749, v_rmse=11.303, v_r2=0.818; \n",
            "E 30\t: loss=141.678, rmse=11.903, r2=0.792; v_loss=135.775, v_rmse=11.652, v_r2=0.807; \n",
            "E 31\t: loss=140.011, rmse=11.833, r2=0.794; v_loss=126.791, v_rmse=11.260, v_r2=0.819; \n",
            "E 32\t: loss=139.818, rmse=11.824, r2=0.794; v_loss=123.910, v_rmse=11.131, v_r2=0.823; \n",
            "Finished: 2022-11-03 09:29:17.723282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dfdd9e-3120-4e54-ed23-0cce41bcb168",
        "id": "oLf0y74BFBzE"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.822,RMSE=-12.765\n",
            "Finished: 2022-11-03 09:29:17.819282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP-4 "
      ],
      "metadata": {
        "id": "MfJUDz0oI5zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear RUL"
      ],
      "metadata": {
        "id": "Pb8CuYNOI5zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Score: 0.542517835612285    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 239),\n",
        "('basemodel__epochs', 44),\n",
        "('basemodel__model__activation1', 'relu'),\n",
        "('basemodel__model__activation2', 'sigmoid'),\n",
        "('basemodel__model__activation3', 'selu'),\n",
        "('basemodel__model__activation4', 'selu'),\n",
        "('basemodel__model__dropout1', 0.27228148001908253),\n",
        "('basemodel__model__dropout2', 0.7227228862335966),\n",
        "('basemodel__model__dropout3', 0.8720098451745675),\n",
        "('basemodel__model__dropout4', 0.23520720034744447),\n",
        "('basemodel__model__layer1', 222),\n",
        "('basemodel__model__layer2', 355),\n",
        "('basemodel__model__layer3', 254),\n",
        "('basemodel__model__layer4', 94),\n",
        "('basemodel__model__learning_rate', 0.006241072056393434),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.3776498733118778),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "B5befZCBI5zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=239,\n",
        "                           epochs=44,\n",
        "                           model__activation1='relu',\n",
        "                           model__activation2='sigmoid',\n",
        "                           model__activation3='selu',\n",
        "                           model__activation4='selu',\n",
        "                           model__dropout1=0.27228148001908253, \n",
        "                           model__dropout2=0.7227228862335966, \n",
        "                           model__dropout3=0.8720098451745675, \n",
        "                           model__dropout4=0.23520720034744447, \n",
        "                           model__layer1=222, \n",
        "                           model__layer2=355, \n",
        "                           model__layer3=254, \n",
        "                           model__layer4=94, \n",
        "                           model__learning_rate=0.006241072056393434,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.3776498733118778, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca529aac-b8ea-4d64-8992-3a90fd67c4f3",
        "id": "HjCB0ofWI5zk"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=239, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=44, model=<function create_model at 0x00000281C1861DC0>, model__activation1='relu', model__activation2='sigmoid', model__activation3='selu', model__activation4='s...model__learning_rate=0.006241072056393434, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000282F561E8B0>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F5C6EDF0>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.3776498733118778, verbose=0),\n",
              "                    include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "766fc5ce-7534-4c29-d37a-f6c9beb10919",
        "id": "WBnAz5ncI5zp"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_45 (Dense)            (None, 222)               5550      \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 222)               0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 355)               79165     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 355)               0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 254)               90424     \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 254)               0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 94)                23970     \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 94)                0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 1)                 95        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 199,204\n",
            "Trainable params: 199,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=7590.750, rmse=87.125, r2=0.039; v_loss=5120.567, v_rmse=71.558, v_r2=0.385; \n",
            "E 2\t: loss=5090.758, rmse=71.350, r2=0.355; v_loss=4420.410, v_rmse=66.486, v_r2=0.469; \n",
            "E 3\t: loss=4703.555, rmse=68.582, r2=0.404; v_loss=4919.458, v_rmse=70.139, v_r2=0.409; \n",
            "E 4\t: loss=4498.351, rmse=67.070, r2=0.430; v_loss=4452.657, v_rmse=66.728, v_r2=0.465; \n",
            "E 5\t: loss=4227.794, rmse=65.021, r2=0.465; v_loss=4395.662, v_rmse=66.300, v_r2=0.472; \n",
            "E 6\t: loss=4175.427, rmse=64.618, r2=0.471; v_loss=4163.043, v_rmse=64.522, v_r2=0.500; \n",
            "E 7\t: loss=4149.509, rmse=64.417, r2=0.475; v_loss=4251.161, v_rmse=65.201, v_r2=0.489; \n",
            "E 8\t: loss=4045.889, rmse=63.607, r2=0.488; v_loss=4522.477, v_rmse=67.249, v_r2=0.457; \n",
            "E 9\t: loss=4044.378, rmse=63.595, r2=0.488; v_loss=4115.100, v_rmse=64.149, v_r2=0.506; \n",
            "E 10\t: loss=4035.780, rmse=63.528, r2=0.489; v_loss=4159.560, v_rmse=64.495, v_r2=0.500; \n",
            "E 11\t: loss=4006.944, rmse=63.300, r2=0.493; v_loss=4092.464, v_rmse=63.972, v_r2=0.508; \n",
            "E 12\t: loss=3969.734, rmse=63.006, r2=0.497; v_loss=4160.967, v_rmse=64.506, v_r2=0.500; \n",
            "E 13\t: loss=3943.037, rmse=62.794, r2=0.501; v_loss=4101.720, v_rmse=64.045, v_r2=0.507; \n",
            "E 14\t: loss=3981.955, rmse=63.103, r2=0.496; v_loss=4291.377, v_rmse=65.509, v_r2=0.484; \n",
            "E 15\t: loss=3982.104, rmse=63.104, r2=0.496; v_loss=4072.203, v_rmse=63.814, v_r2=0.511; \n",
            "E 16\t: loss=3951.247, rmse=62.859, r2=0.500; v_loss=4081.606, v_rmse=63.887, v_r2=0.510; \n",
            "E 17\t: loss=3901.239, rmse=62.460, r2=0.506; v_loss=3975.843, v_rmse=63.054, v_r2=0.522; \n",
            "E 18\t: loss=3879.498, rmse=62.286, r2=0.509; v_loss=4276.010, v_rmse=65.391, v_r2=0.486; \n",
            "E 19\t: loss=3923.678, rmse=62.639, r2=0.503; v_loss=4032.503, v_rmse=63.502, v_r2=0.516; \n",
            "E 20\t: loss=3898.431, rmse=62.437, r2=0.506; v_loss=3949.788, v_rmse=62.847, v_r2=0.526; \n",
            "E 21\t: loss=3862.239, rmse=62.147, r2=0.511; v_loss=4073.665, v_rmse=63.825, v_r2=0.511; \n",
            "E 22\t: loss=3868.748, rmse=62.199, r2=0.510; v_loss=4165.514, v_rmse=64.541, v_r2=0.500; \n",
            "E 23\t: loss=3882.396, rmse=62.309, r2=0.508; v_loss=3995.739, v_rmse=63.212, v_r2=0.520; \n",
            "E 24\t: loss=3900.066, rmse=62.451, r2=0.506; v_loss=3950.132, v_rmse=62.850, v_r2=0.525; \n",
            "E 25\t: loss=3889.330, rmse=62.364, r2=0.507; v_loss=4109.456, v_rmse=64.105, v_r2=0.506; \n",
            "E 26\t: loss=3859.559, rmse=62.125, r2=0.511; v_loss=4082.123, v_rmse=63.891, v_r2=0.510; \n",
            "E 27\t: loss=3806.566, rmse=61.697, r2=0.518; v_loss=4046.621, v_rmse=63.613, v_r2=0.514; \n",
            "E 28\t: loss=3827.042, rmse=61.863, r2=0.515; v_loss=4148.110, v_rmse=64.406, v_r2=0.502; \n",
            "E 29\t: loss=3814.542, rmse=61.762, r2=0.517; v_loss=3977.454, v_rmse=63.067, v_r2=0.522; \n",
            "E 30\t: loss=3809.067, rmse=61.718, r2=0.518; v_loss=4107.382, v_rmse=64.089, v_r2=0.507; \n",
            "E 31\t: loss=3805.679, rmse=61.690, r2=0.518; v_loss=3993.746, v_rmse=63.196, v_r2=0.520; \n",
            "E 32\t: loss=3810.063, rmse=61.726, r2=0.518; v_loss=3992.491, v_rmse=63.186, v_r2=0.520; \n",
            "E 33\t: loss=3796.969, rmse=61.620, r2=0.519; v_loss=3947.349, v_rmse=62.828, v_r2=0.526; \n",
            "E 34\t: loss=3792.519, rmse=61.583, r2=0.520; v_loss=3998.659, v_rmse=63.235, v_r2=0.520; \n",
            "E 35\t: loss=3817.901, rmse=61.789, r2=0.517; v_loss=3986.372, v_rmse=63.138, v_r2=0.521; \n",
            "E 36\t: loss=3823.694, rmse=61.836, r2=0.516; v_loss=4101.407, v_rmse=64.042, v_r2=0.507; \n",
            "E 37\t: loss=3783.566, rmse=61.511, r2=0.521; v_loss=4092.397, v_rmse=63.972, v_r2=0.508; \n",
            "E 38\t: loss=3758.429, rmse=61.306, r2=0.524; v_loss=3937.326, v_rmse=62.748, v_r2=0.527; \n",
            "E 39\t: loss=3824.792, rmse=61.845, r2=0.516; v_loss=4088.233, v_rmse=63.939, v_r2=0.509; \n",
            "E 40\t: loss=3815.838, rmse=61.772, r2=0.517; v_loss=4107.626, v_rmse=64.091, v_r2=0.507; \n",
            "E 41\t: loss=3741.680, rmse=61.169, r2=0.526; v_loss=4148.095, v_rmse=64.406, v_r2=0.502; \n",
            "E 42\t: loss=3789.818, rmse=61.561, r2=0.520; v_loss=4242.180, v_rmse=65.132, v_r2=0.490; \n",
            "E 43\t: loss=3782.840, rmse=61.505, r2=0.521; v_loss=3960.554, v_rmse=62.933, v_r2=0.524; \n",
            "E 44\t: loss=3769.645, rmse=61.397, r2=0.523; v_loss=4016.741, v_rmse=63.378, v_r2=0.517; \n",
            "Finished: 2022-11-03 09:30:59.680084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02de0b82-353b-463f-80fe-1e6bfff9177d",
        "id": "E1ATWFZoI5zt"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.294,RMSE=-45.809\n",
            "Finished: 2022-11-03 09:30:59.787085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Linear RUL"
      ],
      "metadata": {
        "id": "qpedH9HzI5zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.7979100870641017    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 125),\n",
        "('basemodel__epochs', 40),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'tanh'),\n",
        "('basemodel__model__activation3', 'relu'),\n",
        "('basemodel__model__activation4', 'selu'),\n",
        "('basemodel__model__dropout1', 0.4239282276871049),\n",
        "('basemodel__model__dropout2', 0.7682276844296427),\n",
        "('basemodel__model__dropout3', 0.37470248079723845),\n",
        "('basemodel__model__dropout4', 0.5586631760668447),\n",
        "('basemodel__model__layer1', 486),\n",
        "('basemodel__model__layer2', 470),\n",
        "('basemodel__model__layer3', 244),\n",
        "('basemodel__model__layer4', 123),\n",
        "('basemodel__model__learning_rate', 0.0006832900387850664),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__validation_split', 0.2233573353320119),\n",
        "('clip_y', 80),\n",
        "('include_settings', True),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "chBMqsKDI5zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=125\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=1,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=125,\n",
        "                           epochs=250,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='tanh',\n",
        "                           model__activation3='relu',\n",
        "                           model__activation4='selu',\n",
        "                           model__dropout1=0.4239282276871049, \n",
        "                           model__dropout2=0.7682276844296427, \n",
        "                           model__dropout3=0.37470248079723845, \n",
        "                           model__dropout4=0.5586631760668447, \n",
        "                           model__layer1=486, \n",
        "                           model__layer2=470, \n",
        "                           model__layer3=244, \n",
        "                           model__layer4=123, \n",
        "                           model__learning_rate=0.0006832900387850664,\n",
        "                           model__optim=RMSprop,\n",
        "                           validation_split=0.2233573353320119, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9daf0c6c-ef16-45ec-9785-fb926fe7df1d",
        "id": "BE54ZRkFI5z3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=125, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=250, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__activation2='tanh', model__activation3='relu', model__activation4='sel...te=0.0006832900387850664, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000283186EFA90>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000283186DD700>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, print_summary=True, validation_split=0.2233573353320119, verbose=0),\n",
              "                    clip_y=125, include_settings=True)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b94664c-bcc3-46be-c1b6-386c88ac2897",
        "id": "1UX0DFssI5z6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_65 (Dense)            (None, 486)               12150     \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 486)               0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 470)               228890    \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 470)               0         \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 244)               114924    \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 244)               0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 123)               30135     \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 123)               0         \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 1)                 124       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 386,223\n",
            "Trainable params: 386,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=1757.041, rmse=41.917, r2=-0.070; v_loss=1179.241, v_rmse=34.340, v_r2=0.301; \n",
            "E 2\t: loss=1064.952, rmse=32.634, r2=0.352; v_loss=721.486, v_rmse=26.860, v_r2=0.573; \n",
            "E 3\t: loss=899.667, rmse=29.994, r2=0.452; v_loss=675.319, v_rmse=25.987, v_r2=0.600; \n",
            "E 4\t: loss=812.976, rmse=28.513, r2=0.505; v_loss=650.245, v_rmse=25.500, v_r2=0.615; \n",
            "E 5\t: loss=759.675, rmse=27.562, r2=0.537; v_loss=506.601, v_rmse=22.508, v_r2=0.700; \n",
            "E 6\t: loss=730.663, rmse=27.031, r2=0.555; v_loss=500.811, v_rmse=22.379, v_r2=0.703; \n",
            "E 7\t: loss=704.103, rmse=26.535, r2=0.571; v_loss=495.841, v_rmse=22.267, v_r2=0.706; \n",
            "E 8\t: loss=693.105, rmse=26.327, r2=0.578; v_loss=504.932, v_rmse=22.471, v_r2=0.701; \n",
            "E 9\t: loss=685.605, rmse=26.184, r2=0.583; v_loss=481.121, v_rmse=21.934, v_r2=0.715; \n",
            "E 10\t: loss=670.239, rmse=25.889, r2=0.592; v_loss=488.767, v_rmse=22.108, v_r2=0.710; \n",
            "E 11\t: loss=668.534, rmse=25.856, r2=0.593; v_loss=506.799, v_rmse=22.512, v_r2=0.700; \n",
            "E 12\t: loss=655.525, rmse=25.603, r2=0.601; v_loss=485.218, v_rmse=22.028, v_r2=0.713; \n",
            "E 13\t: loss=644.766, rmse=25.392, r2=0.607; v_loss=440.408, v_rmse=20.986, v_r2=0.739; \n",
            "E 14\t: loss=644.492, rmse=25.387, r2=0.608; v_loss=432.980, v_rmse=20.808, v_r2=0.744; \n",
            "E 15\t: loss=635.581, rmse=25.211, r2=0.613; v_loss=504.617, v_rmse=22.464, v_r2=0.701; \n",
            "E 16\t: loss=636.987, rmse=25.239, r2=0.612; v_loss=427.427, v_rmse=20.674, v_r2=0.747; \n",
            "E 17\t: loss=631.713, rmse=25.134, r2=0.615; v_loss=462.772, v_rmse=21.512, v_r2=0.726; \n",
            "E 18\t: loss=624.658, rmse=24.993, r2=0.620; v_loss=752.839, v_rmse=27.438, v_r2=0.554; \n",
            "E 19\t: loss=619.486, rmse=24.889, r2=0.623; v_loss=477.495, v_rmse=21.852, v_r2=0.717; \n",
            "E 20\t: loss=615.723, rmse=24.814, r2=0.625; v_loss=415.751, v_rmse=20.390, v_r2=0.754; \n",
            "E 21\t: loss=612.441, rmse=24.748, r2=0.627; v_loss=496.298, v_rmse=22.278, v_r2=0.706; \n",
            "E 22\t: loss=613.462, rmse=24.768, r2=0.626; v_loss=418.444, v_rmse=20.456, v_r2=0.752; \n",
            "E 23\t: loss=609.675, rmse=24.692, r2=0.629; v_loss=428.373, v_rmse=20.697, v_r2=0.746; \n",
            "E 24\t: loss=604.398, rmse=24.585, r2=0.632; v_loss=423.795, v_rmse=20.586, v_r2=0.749; \n",
            "E 25\t: loss=599.713, rmse=24.489, r2=0.635; v_loss=538.697, v_rmse=23.210, v_r2=0.681; \n",
            "E 26\t: loss=598.705, rmse=24.468, r2=0.635; v_loss=478.016, v_rmse=21.864, v_r2=0.717; \n",
            "E 27\t: loss=593.264, rmse=24.357, r2=0.639; v_loss=429.314, v_rmse=20.720, v_r2=0.746; \n",
            "E 28\t: loss=590.261, rmse=24.295, r2=0.641; v_loss=420.013, v_rmse=20.494, v_r2=0.751; \n",
            "E 29\t: loss=592.029, rmse=24.332, r2=0.640; v_loss=454.165, v_rmse=21.311, v_r2=0.731; \n",
            "E 30\t: loss=588.536, rmse=24.260, r2=0.642; v_loss=424.394, v_rmse=20.601, v_r2=0.749; \n",
            "E 31\t: loss=584.740, rmse=24.181, r2=0.644; v_loss=551.344, v_rmse=23.481, v_r2=0.673; \n",
            "E 32\t: loss=579.043, rmse=24.063, r2=0.647; v_loss=435.036, v_rmse=20.858, v_r2=0.742; \n",
            "E 33\t: loss=579.185, rmse=24.066, r2=0.647; v_loss=460.882, v_rmse=21.468, v_r2=0.727; \n",
            "E 34\t: loss=584.537, rmse=24.177, r2=0.644; v_loss=534.445, v_rmse=23.118, v_r2=0.683; \n",
            "E 35\t: loss=576.293, rmse=24.006, r2=0.649; v_loss=395.116, v_rmse=19.878, v_r2=0.766; \n",
            "E 36\t: loss=576.310, rmse=24.006, r2=0.649; v_loss=444.418, v_rmse=21.081, v_r2=0.737; \n",
            "E 37\t: loss=574.969, rmse=23.979, r2=0.650; v_loss=544.571, v_rmse=23.336, v_r2=0.677; \n",
            "E 38\t: loss=569.422, rmse=23.863, r2=0.653; v_loss=407.864, v_rmse=20.196, v_r2=0.758; \n",
            "E 39\t: loss=574.390, rmse=23.966, r2=0.650; v_loss=390.896, v_rmse=19.771, v_r2=0.768; \n",
            "E 40\t: loss=569.284, rmse=23.860, r2=0.653; v_loss=415.084, v_rmse=20.374, v_r2=0.754; \n",
            "E 41\t: loss=568.142, rmse=23.836, r2=0.654; v_loss=387.593, v_rmse=19.687, v_r2=0.770; \n",
            "E 42\t: loss=573.679, rmse=23.952, r2=0.651; v_loss=470.788, v_rmse=21.698, v_r2=0.721; \n",
            "E 43\t: loss=561.560, rmse=23.697, r2=0.658; v_loss=437.796, v_rmse=20.924, v_r2=0.741; \n",
            "E 44\t: loss=564.017, rmse=23.749, r2=0.657; v_loss=402.327, v_rmse=20.058, v_r2=0.762; \n",
            "E 45\t: loss=561.687, rmse=23.700, r2=0.658; v_loss=409.449, v_rmse=20.235, v_r2=0.757; \n",
            "E 46\t: loss=559.943, rmse=23.663, r2=0.659; v_loss=415.380, v_rmse=20.381, v_r2=0.754; \n",
            "E 47\t: loss=557.306, rmse=23.607, r2=0.661; v_loss=426.866, v_rmse=20.661, v_r2=0.747; \n",
            "E 48\t: loss=561.746, rmse=23.701, r2=0.658; v_loss=396.923, v_rmse=19.923, v_r2=0.765; \n",
            "E 49\t: loss=565.186, rmse=23.774, r2=0.656; v_loss=400.437, v_rmse=20.011, v_r2=0.763; \n",
            "E 50\t: loss=553.119, rmse=23.518, r2=0.663; v_loss=406.051, v_rmse=20.151, v_r2=0.759; \n",
            "E 51\t: loss=560.260, rmse=23.670, r2=0.659; v_loss=397.740, v_rmse=19.943, v_r2=0.764; \n",
            "E 52\t: loss=548.258, rmse=23.415, r2=0.666; v_loss=415.669, v_rmse=20.388, v_r2=0.754; \n",
            "E 53\t: loss=551.611, rmse=23.486, r2=0.664; v_loss=396.897, v_rmse=19.922, v_r2=0.765; \n",
            "E 54\t: loss=548.826, rmse=23.427, r2=0.666; v_loss=429.999, v_rmse=20.736, v_r2=0.745; \n",
            "E 55\t: loss=552.575, rmse=23.507, r2=0.664; v_loss=435.001, v_rmse=20.857, v_r2=0.742; \n",
            "E 56\t: loss=550.870, rmse=23.471, r2=0.665; v_loss=488.504, v_rmse=22.102, v_r2=0.711; \n",
            "E 57\t: loss=549.020, rmse=23.431, r2=0.666; v_loss=391.328, v_rmse=19.782, v_r2=0.768; \n",
            "Finished: 2022-11-03 09:45:54.767697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4cd165-53fa-4797-b41e-d75580e258d2",
        "id": "ypPnlb7xI5z_"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.701,RMSE=-23.510\n",
            "Finished: 2022-11-03 09:45:54.879696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Linear RUL"
      ],
      "metadata": {
        "id": "CWlv5EFWI50C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.5298866447847905    \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 370),\n",
        "('basemodel__epochs', 33),\n",
        "('basemodel__model__activation1', 'elu'),\n",
        "('basemodel__model__activation2', 'sigmoid'),\n",
        "('basemodel__model__activation3', 'sigmoid'),\n",
        "('basemodel__model__activation4', 'elu'),\n",
        "('basemodel__model__dropout1', 0.6870110349714994),\n",
        "('basemodel__model__dropout2', 0.5029302361015952),\n",
        "('basemodel__model__dropout3', 0.5605207131815848),\n",
        "('basemodel__model__dropout4', 0.6403930728651405),\n",
        "('basemodel__model__layer1', 428),\n",
        "('basemodel__model__layer2', 483),\n",
        "('basemodel__model__layer3', 286),\n",
        "('basemodel__model__layer4', 94),\n",
        "('basemodel__model__learning_rate', 0.0032976496820942666),\n",
        "('basemodel__model__optim', keras.optimizer_v2.rmsprop.RMSprop),\n",
        "('basemodel__validation_split', 0.12379763238695841),\n",
        "('include_settings', True),\n",
        "('poly_degree', 3),\n",
        "('scaler', StandardScaler())\n",
        "```\n"
      ],
      "metadata": {
        "id": "_hRfD2CAI50D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=-1\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=3,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=370,\n",
        "                           epochs=33,\n",
        "                           model__activation1='elu',\n",
        "                           model__activation2='sigmoid',\n",
        "                           model__activation3='sigmoid',\n",
        "                           model__activation4='elu',\n",
        "                           model__dropout1=0.6870110349714994, \n",
        "                           model__dropout2=0.5029302361015952, \n",
        "                           model__dropout3=0.5605207131815848, \n",
        "                           model__dropout4=0.6403930728651405, \n",
        "                           model__layer1=428, \n",
        "                           model__layer2=483, \n",
        "                           model__layer3=286, \n",
        "                           model__layer4=94, \n",
        "                           model__learning_rate=0.0032976496820942666,\n",
        "                           model__optim=RMSprop,\n",
        "                           validation_split=0.12379763238695841, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2bd578-1c62-41dd-ff05-8932b9467572",
        "id": "zdhee-1GI50E"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=370, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=33, model=<function create_model at 0x00000281C1861DC0>, model__activation1='elu', model__activation2='sigmoid', model__activation3='sigmoid', model__activation4=....0032976496820942666, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x000002831EABEA30>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000282F49C3520>], model__optim=<class 'keras.optimizer_v2.rmsprop.RMSprop'>, print_summary=True, validation_split=0.12379763238695841, verbose=0),\n",
              "                    include_settings=True, poly_degree=3)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce24c5f4-b85e-486a-cddb-d8e723de1fd4",
        "id": "vhJBABSwI50I"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_70 (Dense)            (None, 428)               1251900   \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 428)               0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 483)               207207    \n",
            "                                                                 \n",
            " dropout_51 (Dropout)        (None, 483)               0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 286)               138424    \n",
            "                                                                 \n",
            " dropout_52 (Dropout)        (None, 286)               0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 94)                26978     \n",
            "                                                                 \n",
            " dropout_53 (Dropout)        (None, 94)                0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 1)                 95        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,624,604\n",
            "Trainable params: 1,624,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=9115.433, rmse=95.475, r2=-0.110; v_loss=6711.127, v_rmse=81.921, v_r2=0.038; \n",
            "E 2\t: loss=8485.933, rmse=92.119, r2=-0.034; v_loss=6611.871, v_rmse=81.313, v_r2=0.052; \n",
            "E 3\t: loss=8378.167, rmse=91.532, r2=-0.020; v_loss=6439.999, v_rmse=80.250, v_r2=0.077; \n",
            "E 4\t: loss=8228.167, rmse=90.709, r2=-0.002; v_loss=6747.707, v_rmse=82.144, v_r2=0.033; \n",
            "E 5\t: loss=8146.084, rmse=90.256, r2=0.008; v_loss=6532.639, v_rmse=80.825, v_r2=0.064; \n",
            "E 6\t: loss=8061.339, rmse=89.785, r2=0.018; v_loss=6267.958, v_rmse=79.170, v_r2=0.102; \n",
            "E 7\t: loss=7936.427, rmse=89.087, r2=0.033; v_loss=5997.966, v_rmse=77.447, v_r2=0.140; \n",
            "E 8\t: loss=7804.517, rmse=88.343, r2=0.049; v_loss=5641.326, v_rmse=75.109, v_r2=0.191; \n",
            "E 9\t: loss=7574.775, rmse=87.033, r2=0.077; v_loss=5600.669, v_rmse=74.838, v_r2=0.197; \n",
            "E 10\t: loss=7291.065, rmse=85.388, r2=0.112; v_loss=5180.849, v_rmse=71.978, v_r2=0.257; \n",
            "E 11\t: loss=7026.669, rmse=83.825, r2=0.144; v_loss=6274.371, v_rmse=79.211, v_r2=0.101; \n",
            "E 12\t: loss=6800.055, rmse=82.462, r2=0.172; v_loss=6218.800, v_rmse=78.859, v_r2=0.109; \n",
            "E 13\t: loss=6572.747, rmse=81.072, r2=0.199; v_loss=4303.519, v_rmse=65.601, v_r2=0.383; \n",
            "E 14\t: loss=6390.112, rmse=79.938, r2=0.222; v_loss=6645.939, v_rmse=81.523, v_r2=0.047; \n",
            "E 15\t: loss=6306.624, rmse=79.414, r2=0.232; v_loss=3903.836, v_rmse=62.481, v_r2=0.440; \n",
            "E 16\t: loss=6225.211, rmse=78.900, r2=0.242; v_loss=4127.032, v_rmse=64.242, v_r2=0.408; \n",
            "E 17\t: loss=6161.110, rmse=78.493, r2=0.250; v_loss=4542.172, v_rmse=67.396, v_r2=0.349; \n",
            "E 18\t: loss=6107.031, rmse=78.147, r2=0.256; v_loss=4068.277, v_rmse=63.783, v_r2=0.417; \n",
            "E 19\t: loss=6073.790, rmse=77.935, r2=0.260; v_loss=4419.041, v_rmse=66.476, v_r2=0.367; \n",
            "E 20\t: loss=6065.409, rmse=77.881, r2=0.261; v_loss=7178.410, v_rmse=84.725, v_r2=-0.029; \n",
            "E 21\t: loss=6061.174, rmse=77.854, r2=0.262; v_loss=4887.021, v_rmse=69.907, v_r2=0.300; \n",
            "E 22\t: loss=6054.826, rmse=77.813, r2=0.263; v_loss=5597.316, v_rmse=74.815, v_r2=0.198; \n",
            "E 23\t: loss=6015.870, rmse=77.562, r2=0.267; v_loss=5049.446, v_rmse=71.059, v_r2=0.276; \n",
            "E 24\t: loss=5980.581, rmse=77.334, r2=0.272; v_loss=4937.229, v_rmse=70.265, v_r2=0.292; \n",
            "E 25\t: loss=5995.760, rmse=77.432, r2=0.270; v_loss=6501.765, v_rmse=80.634, v_r2=0.068; \n",
            "E 26\t: loss=5961.422, rmse=77.210, r2=0.274; v_loss=4462.979, v_rmse=66.806, v_r2=0.360; \n",
            "E 27\t: loss=5854.181, rmse=76.513, r2=0.287; v_loss=5667.820, v_rmse=75.285, v_r2=0.188; \n",
            "E 28\t: loss=5853.972, rmse=76.511, r2=0.287; v_loss=5856.607, v_rmse=76.528, v_r2=0.161; \n",
            "E 29\t: loss=5881.845, rmse=76.693, r2=0.284; v_loss=6285.648, v_rmse=79.282, v_r2=0.099; \n",
            "E 30\t: loss=5877.249, rmse=76.663, r2=0.284; v_loss=5887.674, v_rmse=76.731, v_r2=0.156; \n",
            "E 31\t: loss=5897.236, rmse=76.793, r2=0.282; v_loss=4577.605, v_rmse=67.658, v_r2=0.344; \n",
            "E 32\t: loss=5928.589, rmse=76.997, r2=0.278; v_loss=6780.488, v_rmse=82.344, v_r2=0.028; \n",
            "E 33\t: loss=5967.951, rmse=77.253, r2=0.273; v_loss=5624.354, v_rmse=74.996, v_r2=0.194; \n",
            "Finished: 2022-11-03 09:48:21.363574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d187c97c-2bb5-49dd-ae23-b081f62f041b",
        "id": "qnwVx-VII50K"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.117,RMSE=-51.246\n",
            "Finished: 2022-11-03 09:48:21.557574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PolyFeatures + Non-Linear RUL\n"
      ],
      "metadata": {
        "id": "hz9fq_lWI50L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score: 0.7547335898188002  \n",
        "Test: 0.\n",
        "```\n",
        "('basemodel__batch_size', 33),\n",
        "('basemodel__epochs', 35),\n",
        "('basemodel__model__activation1', 'tanh'),\n",
        "('basemodel__model__activation2', 'selu'),\n",
        "('basemodel__model__activation3', 'selu'),\n",
        "('basemodel__model__activation4', 'selu'),\n",
        "('basemodel__model__dropout1', 0.35902813826517177),\n",
        "('basemodel__model__dropout2', 0.10018359566403763),\n",
        "('basemodel__model__dropout3', 0.3129879941596555),\n",
        "('basemodel__model__dropout4', 0.3715931098343138),\n",
        "('basemodel__model__layer1', 24),\n",
        "('basemodel__model__layer2', 209),\n",
        "('basemodel__model__layer3', 95),\n",
        "('basemodel__model__layer4', 512),\n",
        "('basemodel__model__learning_rate', 0.00029766030712635185),\n",
        "('basemodel__model__optim', keras.optimizer_v2.adam.Adam),\n",
        "('basemodel__validation_split', 0.5105596671166837),\n",
        "('clip_y', 127),\n",
        "('include_settings', True),\n",
        "('poly_degree', 2),\n",
        "('scaler', StandardScaler())\n",
        "```"
      ],
      "metadata": {
        "id": "9yBfpDmQI50N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP=127\n",
        "\n",
        "model = MLPWrapperRegressor(\n",
        "        clip_y=CLIP, scaler=StandardScaler(), poly_degree=2,\n",
        "        include_settings=True,\n",
        "        basemodel=\n",
        "            KerasRegressor(model=create_model,\n",
        "                           batch_size=33,\n",
        "                           epochs=35,\n",
        "                           model__activation1='tanh',\n",
        "                           model__activation2='selu',\n",
        "                           model__activation3='selu',\n",
        "                           model__activation4='selu',\n",
        "                           model__dropout1=0.35902813826517177, \n",
        "                           model__dropout2=0.10018359566403763, \n",
        "                           model__dropout3=0.3129879941596555, \n",
        "                           model__dropout4=0.3715931098343138, \n",
        "                           model__layer1=24, \n",
        "                           model__layer2=209, \n",
        "                           model__layer3=95, \n",
        "                           model__layer4=512, \n",
        "                           model__learning_rate=0.00029766030712635185,\n",
        "                           model__optim=Adam,\n",
        "                           validation_split=0.5105596671166837, \n",
        "                           verbose=0, callbacks=[es, printerCallback],\n",
        "                           model__metrics=[RMSE(), R2()],\n",
        "                           model__loss='mse',\n",
        "                           print_summary=True\n",
        "                           )\n",
        "    )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a91feb-9f84-4f7f-c200-ef9f545fc43a",
        "id": "D8qxuavgI50P"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPWrapperRegressor(basemodel=KerasRegressor(batch_size=33, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000281BF12DF40>, <keras.callbacks.LambdaCallback object at 0x00000281BF13F100>], epochs=35, model=<function create_model at 0x00000281C1861DC0>, model__activation1='tanh', model__activation2='selu', model__activation3='selu', model__activation4='selu'...9766030712635185, model__loss='mse', model__metrics=[<keras.metrics.RootMeanSquaredError object at 0x00000283230FC0D0>, <tensorflow_addons.metrics.r_square.RSquare object at 0x00000283186BD700>], model__optim=<class 'keras.optimizer_v2.adam.Adam'>, print_summary=True, validation_split=0.5105596671166837, verbose=0),\n",
              "                    clip_y=127, include_settings=True, poly_degree=2)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_, y_train)\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44725ec-bfaa-4040-d51c-1678e38259b1",
        "id": "TbCjxqHdI50R"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_75 (Dense)            (None, 24)                7800      \n",
            "                                                                 \n",
            " dropout_54 (Dropout)        (None, 24)                0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 209)               5225      \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 209)               0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 95)                19950     \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 95)                0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 512)               49152     \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 82,640\n",
            "Trainable params: 82,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "E 1\t: loss=1932.615, rmse=43.962, r2=-0.125; v_loss=984.121, v_rmse=31.371, v_r2=0.427; \n",
            "E 2\t: loss=813.898, rmse=28.529, r2=0.526; v_loss=667.748, v_rmse=25.841, v_r2=0.611; \n",
            "E 3\t: loss=685.717, rmse=26.186, r2=0.601; v_loss=483.639, v_rmse=21.992, v_r2=0.718; \n",
            "E 4\t: loss=606.543, rmse=24.628, r2=0.647; v_loss=486.127, v_rmse=22.048, v_r2=0.717; \n",
            "E 5\t: loss=571.441, rmse=23.905, r2=0.667; v_loss=499.162, v_rmse=22.342, v_r2=0.709; \n",
            "E 6\t: loss=566.670, rmse=23.805, r2=0.670; v_loss=477.200, v_rmse=21.845, v_r2=0.722; \n",
            "E 7\t: loss=546.347, rmse=23.374, r2=0.682; v_loss=479.243, v_rmse=21.892, v_r2=0.721; \n",
            "E 8\t: loss=539.216, rmse=23.221, r2=0.686; v_loss=479.105, v_rmse=21.888, v_r2=0.721; \n",
            "E 9\t: loss=536.501, rmse=23.162, r2=0.688; v_loss=470.198, v_rmse=21.684, v_r2=0.726; \n",
            "E 10\t: loss=521.407, rmse=22.834, r2=0.696; v_loss=452.002, v_rmse=21.260, v_r2=0.737; \n",
            "E 11\t: loss=518.030, rmse=22.760, r2=0.698; v_loss=443.195, v_rmse=21.052, v_r2=0.742; \n",
            "E 12\t: loss=507.420, rmse=22.526, r2=0.705; v_loss=509.017, v_rmse=22.561, v_r2=0.703; \n",
            "E 13\t: loss=507.030, rmse=22.517, r2=0.705; v_loss=449.030, v_rmse=21.190, v_r2=0.738; \n",
            "E 14\t: loss=496.128, rmse=22.274, r2=0.711; v_loss=455.031, v_rmse=21.331, v_r2=0.735; \n",
            "E 15\t: loss=488.366, rmse=22.099, r2=0.716; v_loss=456.152, v_rmse=21.358, v_r2=0.734; \n",
            "E 16\t: loss=494.997, rmse=22.249, r2=0.712; v_loss=459.689, v_rmse=21.440, v_r2=0.732; \n",
            "E 17\t: loss=487.437, rmse=22.078, r2=0.716; v_loss=504.401, v_rmse=22.459, v_r2=0.706; \n",
            "E 18\t: loss=484.123, rmse=22.003, r2=0.718; v_loss=437.267, v_rmse=20.911, v_r2=0.745; \n",
            "E 19\t: loss=481.980, rmse=21.954, r2=0.719; v_loss=434.818, v_rmse=20.852, v_r2=0.747; \n",
            "E 20\t: loss=483.100, rmse=21.980, r2=0.719; v_loss=442.486, v_rmse=21.035, v_r2=0.742; \n",
            "E 21\t: loss=469.147, rmse=21.660, r2=0.727; v_loss=445.186, v_rmse=21.099, v_r2=0.741; \n",
            "E 22\t: loss=475.628, rmse=21.809, r2=0.723; v_loss=423.890, v_rmse=20.589, v_r2=0.753; \n",
            "E 23\t: loss=471.560, rmse=21.715, r2=0.725; v_loss=474.129, v_rmse=21.775, v_r2=0.724; \n",
            "E 24\t: loss=468.657, rmse=21.648, r2=0.727; v_loss=432.799, v_rmse=20.804, v_r2=0.748; \n",
            "E 25\t: loss=464.385, rmse=21.550, r2=0.730; v_loss=426.048, v_rmse=20.641, v_r2=0.752; \n",
            "E 26\t: loss=468.753, rmse=21.651, r2=0.727; v_loss=483.747, v_rmse=21.994, v_r2=0.718; \n",
            "E 27\t: loss=466.518, rmse=21.599, r2=0.728; v_loss=449.812, v_rmse=21.209, v_r2=0.738; \n",
            "E 28\t: loss=460.809, rmse=21.466, r2=0.732; v_loss=438.012, v_rmse=20.929, v_r2=0.745; \n",
            "E 29\t: loss=464.708, rmse=21.557, r2=0.729; v_loss=462.746, v_rmse=21.512, v_r2=0.730; \n",
            "E 30\t: loss=461.009, rmse=21.471, r2=0.732; v_loss=451.150, v_rmse=21.240, v_r2=0.737; \n",
            "E 31\t: loss=459.407, rmse=21.434, r2=0.733; v_loss=472.460, v_rmse=21.736, v_r2=0.725; \n",
            "E 32\t: loss=453.198, rmse=21.288, r2=0.736; v_loss=417.685, v_rmse=20.437, v_r2=0.757; \n",
            "E 33\t: loss=459.416, rmse=21.434, r2=0.733; v_loss=549.809, v_rmse=23.448, v_r2=0.680; \n",
            "E 34\t: loss=456.433, rmse=21.364, r2=0.734; v_loss=412.321, v_rmse=20.306, v_r2=0.760; \n",
            "E 35\t: loss=453.290, rmse=21.291, r2=0.736; v_loss=422.862, v_rmse=20.564, v_r2=0.754; \n",
            "Finished: 2022-11-03 09:53:49.053977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and formatting test data\n",
        "test_sc = scale_test(X_test,model)\n",
        "\n",
        "# Clipping test labels\n",
        "reclipped_y = y_test.copy()\n",
        "if(CLIP > 0): reclipped_y = reclipped_y[\"RUL\"].clip(upper=CLIP, inplace=False)\n",
        "\n",
        "# Evaluation\n",
        "eval.show_result(reclipped_y, model.basemodel.predict(test_sc))\n",
        "print(\"Finished:\", datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4191e5-366a-45f3-d483-dd788b09d76c",
        "id": "Yz7_bVa-I50T"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2=0.718,RMSE=-23.119\n",
            "Finished: 2022-11-03 09:53:49.168980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCD1WJU8iV4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}